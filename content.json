{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"书单","date":"2020-10-21T07:00:56.225Z","updated":"2020-10-21T05:23:31.270Z","comments":false,"path":"books/index.html","permalink":"http://example.com/books/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-10-21T07:00:56.225Z","updated":"2020-10-21T05:23:31.263Z","comments":true,"path":"links/index.html","permalink":"http://example.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2020-10-21T07:00:56.224Z","updated":"2020-10-21T05:23:31.289Z","comments":false,"path":"repository/index.html","permalink":"http://example.com/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"ElasticStack之Beats学习","slug":"ElasticStack之Beats学习","date":"2020-12-02T08:13:51.000Z","updated":"2020-12-02T09:30:45.863Z","comments":true,"path":"2020/12/02/ElasticStack之Beats学习/","link":"","permalink":"http://example.com/2020/12/02/ElasticStack%E4%B9%8BBeats%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"Beats–轻量型数据采集器官网地址：https://www.elastic.co/cn/products/beats 一、Filebeat–轻量型日志采集器1.工作原理： Harvester 负责读取单个文件的内容 文件被删除或者重命名，将继续读取该文件 Prospector 找到要读取文件的来源 管理harvester 若输入类型为日志类型，则查找路径匹配的所有文件，并为每一个文件分配启动一个harvester 目前支持两种类型：log和stdin filebeat如何保持文件状态（如何判断该读取文件的哪一行） filebeat会保存每个文件的状态并经常刷新到register中（filebeat目录下的register目录下） 该状态记录了harvester读取的最后一行在哪 如果输出无法访问（比如ealsticsearch宕机），filebeat会跟踪最后一行，输出可用时继续下一行发送 2.基本配置讲解123456789101112#创建如下配置文件 xxx.ymlfilebeat.inputs: - type: stdin #控制台输入 log ：日志 enabled: true setup.template.settings: index.number_of_shards: 3 #分片数3 output.console: #输出到控制台 pretty: true enable: true#启动filebeat./filebeat -e -c xxx.yml 123456789101112131415#创建如下配置文件 xxx.ymlfilebeat.inputs: - type: log #日志 enabled: true paths: - /haoke/beats/logs/*.log #自己需要采集的日志路径 tags: [&quot;web&quot;] #添加自定义tag，便于后续的处理 fields: #添加自定义字段 from: haoke-im fields_under_root: true #true为添加到根节点，false为添加到子节点中setup.template.settings: index.number_of_shards: 3 #分片数3 output.console: #输出到控制台 pretty: true enable: true 1234567891011121314#创建如下配置文件 xxx.ymlfilebeat.inputs: - type: log #日志 enabled: true paths: - /haoke/beats/logs/*.log #自己需要采集的日志路径 tags: [&quot;web&quot;] #添加自定义tag，便于后续的处理 fields: #添加自定义字段 from: haoke-im fields_under_root: true #true为添加到根节点，false为添加到子节点中setup.template.settings: index.number_of_shards: 3 #分片数3output.elasticsearch: #指定ES的配置 hosts: [&quot;192.168.1.7:9200&quot;,&quot;192.168.1.7:9201&quot;,&quot;192.168.1.7:9202&quot;] #如果是集群就可以配置多个地址 3.Module前面要想实现日志数据的读取以及处理都是自己手动配置的，其实，在Filebeat中，有大量的Module，可以简化我 们的配置，直接就可以使用 12345678910111213141516171819202122./filebeat modules listEnabled: #开启的moduleDisabled: #未开启的moduleapache2 auditd elasticsearch haproxy icinga iis kafka kibanalogstash mongodb mysqlnginx osquerypostgresqlredis suricata system traefik 以redis module为例示范如何开启redis module以及怎样修改配置 1234567./filebeat modules enable redis #启动./filebeat modules disable redis #禁用Enabled: redisDisabled: #... redis module 配置 123456789101112131415161718cd modules.d/vim redis.yml- module: redis # Main logs log: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. var.paths: [&quot;/data/redis-data/node01/*.log&quot;] #主要修改redis的log地址 # Slow logs, retrieved via the Redis API (SLOWLOG) slowlog: enabled: false # The Redis hosts to connect to. #var.hosts: [&quot;localhost:6379&quot;] #... redis默认情况下，是不会输出日志的，需要进行配置，前面我们使用的容器都没有配置日志输出，下面需要配置一 下。 12345docker create --name redis-node01 -v /data/redis-data/node01:/data -p 6379:6379 redis:5.0.2 --cluster-enabled yes --cluster-config-file nodes-node-01.conf --loglevel debug --logfile nodes-node-01.logdocker create --name redis-node02 -v /data/redis-data/node02:/data -p 6380:6379 redis:5.0.2 --cluster-enabled yes --cluster-config-file nodes-node-02.conf --loglevel debug --logfile nodes-node-02.logdocker create --name redis-node03 -v /data/redis-data/node03:/data -p 6381:6379 redis:5.0.2 --cluster-enabled yes --cluster-config-file nodes-node-03.conf --loglevel debug --logfile nodes-node-03.log loglevel 日志等级分为：debug、verbose、notice、warning 其中，debug 会有大量信息，对开发、测试有用； verbose 等于log4j 中的info，有很多信息，但是不会像debug那样乱； notice 一般信息； warning 只有非常重要/关键的消息被记录。 配置filebeat 123456789101112filebeat.inputs: - type: log #日志 enabled: true paths: - /haoke/beats/logs/*.log #自己需要采集的日志路径setup.template.settings: index.number_of_shards: 3 #分片数3output.elasticsearch: #指定ES的配置 hosts: [&quot;192.168.1.7:9200&quot;,&quot;192.168.1.7:9201&quot;,&quot;192.168.1.7:9202&quot;] #如果是集群就可以配置多个地址filebeat.config.modules: #加载module path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: false 二、Metricbeat–轻量型指标采集器用于从系统和服务中收集指标 Metricbeat有2部分组成 Module 收集的对象，如：mysql、操作系统等 Metricset 收集的指标集合，如：cpu，network，memory等 配置： 1234567891011121314vim metricbeat.ymlmetricbeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: falsesetup.template.settings: index.number_of_shards: 1 #分片数自己设置 index.codec: best_compressionsetup.kibana: output.elasticsearch: hosts: [&quot;192.168.1.7:9200&quot;,&quot;192.168.1.7:9201&quot;,&quot;192.168.1.7:9202&quot;] #自己的地址processors: - add_host_metadata: ~ - add_cloud_metadata: ~ Module使用，直接开启某module然后配置yml文件（如redis.yml ），设置集群的地址等，直接启动就行。","categories":[],"tags":[]},{"title":"elasticsearch入门学习","slug":"elasticsearch入门学习","date":"2020-11-26T13:29:10.000Z","updated":"2020-11-28T11:42:18.632Z","comments":true,"path":"2020/11/26/elasticsearch入门学习/","link":"","permalink":"http://example.com/2020/11/26/elasticsearch%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"ElasticStack之一elasticsearch入门一、介绍：Elasticsearch 基于java，是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。二、 docker安装：12345678#拉取镜像 docker pull elasticsearch:6.5.4 #某个版本#创建容器 docker create --name elasticsearch --net host -e &quot;discovery.type=single-node&quot; -e &quot;network.host=172.16.55.185&quot; elasticsearch:6.5.4 #虚拟机IP地址#启动 docker start elasticsearch #查看日志 docker logs elasticsearch 三、elasticsearch-head 提供界面管理工具四、Restful API123456789PUT http://172.16.55.185:9200/haoke #创建非结构化索引&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: &quot;2&quot;, &quot;number_of_replicas&quot;: &quot;0&quot; &#125; &#125;&#125; 1DELETE http://172.16.55.185:9200/haoke #删除索引 1234567POST http://172.16.55.185:9200/haoke/user/1001 #插入数据&#123; &quot;id&quot;: 1001, &quot;name&quot;: &quot;张三&quot;, &quot;age&quot;: 20, &quot;sex&quot;: &quot;男&quot;&#125; 1234567POST http://172.16.55.185:9200/haoke/user/ #不指定Id插入数据，id自动生成&#123; &quot;id&quot;: 1002, &quot;name&quot;: &quot;张阿三&quot;, &quot;age&quot;: 22, &quot;sex&quot;: &quot;男&quot;&#125; 123456789101112131415在Elasticsearch中，文档数据是不为修改的，但是可以通过覆盖的方式进行更新。PUT http://172.16.55.185:9200/haoke/user/1001&#123; &quot;id&quot;: 1001, &quot;name&quot;: &quot;张三&quot;, &quot;age&quot;: 22, &quot;sex&quot;: &quot;女&quot;&#125;可以局部更新，原因是在内部，依然会查询到这个文档数据，然后进行覆盖操作POST http://172.16.55.185:9200/haoke/user/1001/_update &#123; &quot;doc&quot;: &#123; &quot;age&quot;: 23 &#125;&#125; 1DELETE http://172.16.55.185:9200/haoke/user/1001 #删除数据 123GET http://172.16.55.185:9200/haoke/user/BbPe_WcB9cFOnF3uebvr #根据id查询数据GET http://172.16.55.185:9200/haoke/user/_search #搜索全部数据 (默认返回十条)GET http://172.16.55.185:9200/haoke/user/_search?q=age:20 #查询年龄等于20的用户 12345678910111213141516171819202122232425262728293031323334353637POST http://172.16.55.185:9200/haoke/user/_search #DSL搜索&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;age&quot;: 20 &#125; &#125;&#125;POST http://172.16.55.185:9200/haoke/user/_search &#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gt&quot;: 30 &#125; &#125; &#125;, &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;sex&quot;: &quot;男&quot; &#125; &#125; &#125; &#125;&#125;POST http://172.16.55.185:9200/haoke/user/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;张三 李四&quot; &#125; &#125;&#125; 12345678910111213POST http://172.16.55.185:9200/haoke/user/_search #高亮显示&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;张三 李四&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;name&quot;: &#123;&#125; &#125; &#125;&#125; 12345678910POST http://172.16.55.185:9200/haoke/user/_search #聚合&#123; &quot;aggs&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;&#125; 五、结构化查询123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263POST http://172.16.55.185:9200/itcast/person/_search#term查询：term 主要用于精确匹配哪些值，比如数字，日期，布尔值或 not_analyzed 的字符串(未经分析的文本数据类#型)：&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;age&quot;: 20 &#125; &#125;&#125;#terms查询：terms 跟 term 有点类似，但 terms 允许指定多个匹配条件。 如果某个字段指定了多个值，那么文档需要一#起去做匹配：&#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; &quot;age&quot;: [ 20, 21 ] &#125; &#125;&#125;#range查询：range 过滤允许我们按照指定范围查找一批数据：&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: 20, &quot;lte&quot;: 22 &#125; &#125; &#125;&#125;#exist查询：exists 查询可以用于查找文档中是否包含指定字段或没有某个字段，类似于SQL语句中的 IS_NULL 条件&#123; &quot;query&quot;: &#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;card&quot; &#125; &#125;&#125;#match查询：match 查询是一个标准查询，不管你需要全文本查询还是精确查询基本上都要用到它。#如果你使用 match 查询一个全文本字段，它会在真正查询之前用分析器先分析 match 一下查询字符：&#123; &quot;match&quot;: &#123; &quot;age&quot;: 26 &#125;&#125; &#123;&quot;match&quot;: &#123; &quot;date&quot;: &quot;2014-09-01&quot; &#125;&#125; &#123; &quot;match&quot;: &#123; &quot;public&quot;: true &#125;&#125; &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;full_text&quot; &#125;&#125;#bool查询：bool 查询可以用来合并多个条件查询结果的布尔逻辑，它包含一下操作符：#must :: 多个查询条件的完全匹配,相当于 and 。#must_not :: 多个查询条件的相反匹配，相当于 not 。 #should :: #至少有一个查询条件匹配, 相当于 or 。#这些参数可以分别继承一个查询条件或者一个查询条件的数组：&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;age&quot;: 20 &#125; &#125; &#125; &#125;&#125; 六、分词1234567891011POST http://172.16.55.185:9200/_analyze #分词api&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;hello world&quot;&#125;POST http://172.16.55.185:9200/itcast/_analyze #指定索引分词&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;field&quot;: &quot;hobby&quot;, #指定字段 &quot;text&quot;: &quot;听音乐&quot; &#125; 五种内置分词：Standard、Simple、Whitespace、Stop、Keyword了解 中文分词：ik、jieba、THULAC等，推荐ik分词器 安装ik分词器：将elasticsearch-analysis-ik的zip包解压到/elasticsearch/plugins/ik下即可 如果docker运行先复制进容器，然后进入容器解压，示例： 123456docker cp /tmp/elasticsearch-analysis-ik-6.5.4.zip elasticsearch:/usr/share/elasticsearch/plugins/docker exec -it elasticsearch /bin/bash mkdir /usr/share/elasticsearch/plugins/ik cd /usr/share/elasticsearch/plugins/ik unzip elasticsearch-analysis-ik-6.5.4.zip 七、全文搜索1.倒排索引:由于不是由记录来确定属性值，而是由属性值来确定记录的位置，因而称为倒排索引，简单来说就是由值找位置。全文搜索两个最重要的方面是： 相关性（Relevance） 它是评价查询与其结果间的相关程度，并根据这种相关程度对结果排名的一种能力，这 种计算方式可以是 TF/IDF 方法、地理位置邻近、模糊相似，或其他的某些算法。 分析（Analysis） 它是将文本块转换为有区别的、规范化的 token 的一个过程，目的是为了创建倒排索引以 及查询倒排索引。 2.单词搜索搜索过程说明： 检测字段类型：假设查看字段为”text”字段类型，意味着查询字符串本身也应该被分词 分析查询字符串：然后将查询字符串传入ik分词器，根据输出结果判断底层使用什么查询 查找匹配文档：查找相匹配的文档 为每个文档打分 3.多词搜索：发现一个问题：12345678910111213POST http://172.16.55.185:9200/itcast/person/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &quot;音乐 篮球&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 发现查询结果中包含了“音乐”、“篮球”的数据都已经被搜索到了，但我们想要的是两者都包含了的数据，Elasticsearch为我们提供了这样的逻辑关系 1234567891011121314POST http://172.16.55.185:9200/itcast/person/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &quot;音乐 篮球&quot;, &quot;operator&quot;:&quot;and&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 添加一个operator字段，值and表示两者都包含，or表示至少包含一个，而这是两种极端方式，在hobby有多个关键词时（大于2个）,在Elasticsearch中也支持这样的查询，通过minimum_should_match来指定匹配度，比如70%。 1234567891011121314POST http://172.16.55.185:9200/itcast/person/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &quot;音乐 篮球&quot;, &quot;minimum_should_match&quot;:&quot;70%&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 4.权重：有些时候，我们可能需要对某些词增加权重来影响该条数据的得分示例： 1234567891011121314151617181920212223242526272829303132333435363738POST http://172.16.55.185:9200/itcast/person/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;游泳篮球&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;音乐&quot;, &quot;boost&quot;: 10 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;跑步&quot;, &quot;boost&quot;: 2 &#125; &#125; &#125; ] &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; boost字段表示权重，根据自己需求设置。 5.短语匹配1234567891011121314&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;羽毛球篮球&quot; &#125; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 短语匹配意味着不仅仅是词要匹配，并且词的顺序也要一致 如果觉得这样太过于苛刻，可以增加slop参数，允许跳过N个词进行匹配 123456789101112131415&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;羽毛球足球&quot;, &quot;slop&quot;: 3 &#125; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 八、Elasticsearch集群1.集群节点 master节点 配置文件中node.master属性为true(默认为true)，就有资格被选为master节点。 master节点用于控制整个集群的操作。比如创建或删除索引，管理其它非master节点等。 data节点 配置文件中node.data属性为true(默认为true)，就有资格被设置成data节点。 data节点主要用于执行数据相关的操作。比如文档的CRUD。 客户端节点 配置文件中node.master属性和node.data属性均为false。 该节点不能作为master节点，也不能作为data节点。 可以作为客户端节点，用于响应用户的请求，把请求转发到其他节点 部落节点 当一个节点配置tribe.*的时候，它是一个特殊的客户端，它可以连接多个集群，在所有连接的集群上执 行搜索和其他操作。 2.分片和副本实际上，索引只是一个用来指向一个或多个分片的逻辑命名空间。 一个分片是最小的工作单元，一个lucene实例，它本身就是一个完整的搜索引擎，但是应用程序不会和他直接通信；分片包括主分片和复制分片，复制分片只是主分片的一个副本为了防止故障数据丢失的，同时提供读请求；每个文档都对应一个单独的主分片，所以主分片的数量决定存储数据大小 3.故障问题假如有三个节点，他们的配置都是node.master为true,minimum_master_nodes的大小为1。es-node1,es-node2.es-node3 node2为主节点时，停止node1时，主分片会转移到2，3两个节点，恢复后再转移回去 停止node2时，node3变为主节点，node2的主分片转移到1，3中，但是恢复node2后，并没有转移回来，而是出现脑裂现象，node2与node3都为master节点，所以出现问题。解决问题的办法：minimum_master_nodes的大小官方推荐：(N/2)+1，N为集群中节点数 4.分布式文档1.存储方式当向一个集群保存文档时，文档存储到节点的方式既不是随机的也不是轮询的，而是固定的计算公式 1shard = hash(routing) % number_of_primary_shards 2.文档的写和搜索新建、索引和删除请求都是写操作，写操作只能写在主分片中，而读操作可以在主分片和复制分片中都可以，为了平衡负载 3.全文搜索：全文搜索分为两个阶段：搜索和取回；1搜索：客户端发送请求给某一个节点，这一节点转发请求到每一个分片中，每一分片返回相同的固定长度的结果到开始的节点中，该节点把所以结果排序后得出最终结果。2取回：根据最终结果的信息再向相关分片发送请求获取结果中的文档详细信息，然后返回个客户端。 九、Java客户端1.REST客户端导入依赖测试： 1234567891011121314151617181920public class TestESREST &#123; private static final ObjectMapper MAPPER = new ObjectMapper(); private RestClient restClient; @Before public void init() &#123; RestClientBuilder restClientBuilder = RestClient.builder( new HttpHost(&quot;172.16.55.185&quot;, 9200, &quot;http&quot;), new HttpHost(&quot;172.16.55.185&quot;, 9201, &quot;http&quot;), new HttpHost(&quot;172.16.55.185&quot;, 9202, &quot;http&quot;)); this.restClient = restClientBuilder.build(); &#125; @After public void after() throws IOException &#123; restClient.close(); &#125; //测试代码。。。查询ealsticsearch接口文档&#125; 2.高级REST客户端12345678910111213141516171819public class TestRestHighLevel &#123; private RestHighLevelClient restClient; @Before public void init() &#123; RestClientBuilder restClientBuilder = RestClient.builder( new HttpHost(&quot;172.16.55.185&quot;, 9200, &quot;http&quot;), new HttpHost(&quot;172.16.55.185&quot;, 9201, &quot;http&quot;), new HttpHost(&quot;172.16.55.185&quot;, 9202, &quot;http&quot;)); this.restClient = new RestHighLevelClient(restClientBuilder); &#125; @After public void after() throws IOException &#123; restClient.close(); &#125; //测试代码。。。查询ealsticsearch接口文档&#125; 十、Spring Data Elasticsearch与spring整合导入依赖，编写properties文件，编写启动类。。 编写示例对象： 1234567891011121314@Data @AllArgsConstructor @NoArgsConstructor@Document(indexName = &quot;itcast&quot;, type = &quot;user&quot;, shards = 6, replicas = 1) //shards出现bug，创建后只有5个分片，可以改成自己创建索引，然后createindex设为falsepublic class User&#123; @Id private Long id; @Field(store = true) private String name; @Field private Integer age; @Field private String hobby;&#125; 新增数据 1234567891011121314151617@RunWith(SpringRunner.class) @SpringBootTest public class TestSpringBootES &#123; @Autowired private ElasticsearchTemplate elasticsearchTemplate; //spring最会玩的模版 @Test public void testSave()&#123; User user = new User(); user.setId(1001L); user.setAge(20); user.setName(&quot;张三&quot;); user.setHobby(&quot;足球、篮球、听音乐&quot;); IndexQuery indexQuery = new IndexQueryBuilder().withObject(user).build(); String index = this.elasticsearchTemplate.index(indexQuery); System.out.println(index); &#125; &#125; 批量插入数据 12345678910111213141516@Test public void testBulk() &#123; List list = new ArrayList(); for (int i = 0; i &lt; 5000; i++) &#123; User user = new User(); user.setId(1001L + i); user.setAge(i % 50 + 10); user.setName(&quot;张三&quot; + i); user.setHobby(&quot;足球、篮球、听音乐&quot;); IndexQuery indexQuery = new IndexQueryBuilder().withObject(user).build(); list.add(indexQuery); &#125; Long start = System.currentTimeMillis(); this.elasticsearchTemplate.bulkIndex(list); System.out.println(&quot;用时：&quot; + (System.currentTimeMillis() - start));//用时：4114ms &#125; 更新数据 12345678910@Test public void testUpdate() &#123; IndexRequest indexRequest = new IndexRequest(); indexRequest.source(&quot;age&quot;, &quot;30&quot;); UpdateQuery updateQuery = new UpdateQueryBuilder() .withId(&quot;1001&quot;) .withClass(User.class) .withIndexRequest(indexRequest).build(); this.elasticsearchTemplate.update(updateQuery); &#125; 搜索数据 123456789101112@Test public void testSearch() &#123; PageRequest pageRequest = PageRequest.of(1, 10); //设置分页参数 SearchQuery searchQuery = new NativeSearchQueryBuilder() .withQuery(QueryBuilders.matchQuery(&quot;name&quot;, &quot;张三&quot;)) // match查询 .withPageable(pageRequest) .build(); AggregatedPage&lt;User&gt; users = this.elasticsearchTemplate.queryForPage(searchQuery, User.class); System.out.println(&quot;总页数：&quot; + users.getTotalPages()); //获取总页数 for (User user : users.getContent()) &#123; // 获取搜索到的数据 System.out.println(user); &#125; &#125;","categories":[],"tags":[]},{"title":"关于RocketMQ的一些内容和特性","slug":"关于RocketMQ的一些内容和特性","date":"2020-11-01T07:11:07.000Z","updated":"2020-11-02T02:54:26.602Z","comments":true,"path":"2020/11/01/关于RocketMQ的一些内容和特性/","link":"","permalink":"http://example.com/2020/11/01/%E5%85%B3%E4%BA%8ERocketMQ%E7%9A%84%E4%B8%80%E4%BA%9B%E5%86%85%E5%AE%B9%E5%92%8C%E7%89%B9%E6%80%A7/","excerpt":"","text":"首先是发送消息分同步和异步， Producer的顺序消息：在某些业务中，consumer在消费消息时，是需要按照生产者发送消息的顺序进行消费的。 分布式消息：分布式事务分类有这几种： 基于单个JVM，数据库分库分表了（跨多个数据库）。 基于多JVM，服务拆分了（不跨数据库）。 基于多JVM，服务拆分了 并且数据库分库分表了。 采用半消息方式，发送消息后处理完本地事务再确认 执行流程 发送方向 MQ 服务端发送消息。 MQ Server 将消息持久化成功之后，向发送方 ACK 确认消息已经发送成功，此时消息为半消息。 发送方开始执行本地事务逻辑。 发送方根据本地事务执行结果向 MQ Server 提交二次确认（Commit 或是 Rollback），MQ Server 收到 Commit 状态则将半消息标记为可投递，订阅方最终将收到该消息；MQ Server 收到 Rollback 状态则删除半 消息，订阅方将不会接受该消息。 在断网或者是应用重启的特殊情况下，上述步骤4提交的二次确认最终未到达 MQ Server，经过固定时间后 MQ Server 将对该消息发起消息回查。 发送方收到消息回查后，需要检查对应消息的本地事务执行的最终结果。 发送方根据检查得到的本地事务的最终状态再次提交二次确认，MQ Server 仍按照步骤4对半消息进行操作。 Consumer的Push和Pullpush模式：客户端与服务端建立连接后，当服务端有消息时，将消息推送到客户端。 pull模式：客户端不断的轮询请求服务端，来获取新的消息。 底层都是pull，只不过push被封装到里面，pull需要自己实现 长轮询：发送一个pull等待服务器有消息再拉取，避免了一直pull造成压力 消息模式：集群模式和广播模式 重复消息：肯定存在的，自己解决，要么设置key相同的不处理，要么处理结果相同保持为一条。 RocketMQ存储RocketMQ中的消息数据存储，采用了零拷贝技术（使用 mmap + write 方式），文件系统采用 Linux Ext4 文件系 统进行存储。 RocketMQ消息的存储是由ConsumeQueue和CommitLog配合完成的，CommitLog是真正存储数据的文件， ConsumeQueue是索引文件，存储数据指向到物理文件的配置。 同步刷盘和异步刷盘 同步刷盘 在返回写成功状态时，消息已经被写入磁盘 。 具体流程是：消息写入内存的 PAGECACHE 后，立刻通知刷盘线程刷盘，然后等待刷盘完成，刷盘线程 执行完成后唤醒等待的线程，返回消息写成功的状态 。 异步刷盘 在返回写成功状态时，消息可能只是被写入了内存的 PAGECACHE，写操作的返回快，吞吐量大 当内存里的消息量积累到一定程度时，统一触发写磁盘动作，快速写入。","categories":[],"tags":[]},{"title":"关于RocketMQ发送消息报错","slug":"关于RocketMQ发送消息报错","date":"2020-10-31T05:43:14.000Z","updated":"2020-10-31T05:51:01.809Z","comments":true,"path":"2020/10/31/关于RocketMQ发送消息报错/","link":"","permalink":"http://example.com/2020/10/31/%E5%85%B3%E4%BA%8ERocketMQ%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E6%8A%A5%E9%94%99/","excerpt":"","text":"报错信息：Exception in thread “main” org.apache.rocketmq.remoting.exception.RemotingTooMuchRequestException: sendDefaultImpl call timeout 在虚拟机中rockermq配置完broker启动成功后信息为： 1The broker[itcast, 172.17.0.1:10911] boot success. serializeType&#x3D;JSON and name server is 172.16.185.55:9876 发现broker的ip是172.17.0.1，外部没办法访问所以必须把IP地址改掉，在虚拟机创建一个配置文件../conf/broker.conf 123brokerIP1=172.16.55.185 namesrvAddr=172.16.55.185:9876 brokerName=broker_haoke_im 启动时指定配置文件 -c 1bin&#x2F;mqbroker -c &#x2F;haoke&#x2F;rmq&#x2F;rmqbroker&#x2F;conf&#x2F;broker.conf 会发现成功信息变成如下 1The broker[itcast, 172.16.55.185:10911] boot success. serializeType&#x3D;JSON and name server is 172.16.55.185:9876 通常情况下这样就可以访问了 但是我的还是不行，原因是最容易被忽略的超时问题，把生产者的超时时间设置大一点就可以我这里设置15s 1producer.setSendMsgTimeout(15000);","categories":[],"tags":[]},{"title":"GraphQL学习之开发某服务接口实现","slug":"GraphQL学习之开发某服务接口实现","date":"2020-10-30T00:34:30.000Z","updated":"2020-11-01T01:06:38.776Z","comments":true,"path":"2020/10/30/GraphQL学习之开发某服务接口实现/","link":"","permalink":"http://example.com/2020/10/30/GraphQL%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%BC%80%E5%8F%91%E6%9F%90%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"1.导入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.graphql-java&lt;/groupId&gt; &lt;artifactId&gt;graphql-java&lt;/artifactId&gt; &lt;version&gt;11.0&lt;/version&gt; &lt;/dependency&gt; 2.编写haoke.graphqls文件 12345678910111213141516171819202122232425262728schema &#123; query: HaokeQuery &#125;type HaokeQuery &#123; HouseResources(id:Long):HouseResources &#125;type HouseResources&#123; id:Long! title:String estateId:Long buildingNum:String buildingUnit:String buildingFloorNum:String rent:Int rentMethod:Int paymentMethod:Int houseType:String coveredArea:String useArea:String floor:String orientation:String decoration:Int facilities:String pic:String houseDesc:String contact:String mobile:String time:Int propertyCost:String &#125; 3.编写GraphQLController 1234567891011@RequestMapping(&quot;graphql&quot;) //前端访问地址@Controller public class GraphQLController &#123; @Autowired private GraphQL graphQL; @GetMapping @ResponseBody public Map&lt;String, Object&gt; graphql(@RequestParam(&quot;query&quot;) String query) throws IOException &#123; return this.graphQL.execute(query).toSpecification(); &#125; &#125; 4.编写GraphQLProvider需要与SpringBoot整合，将GraphQL对象加载到Spring容器中 12345678910111213141516171819202122232425262728@Component public class GraphQLProvider &#123; private GraphQL graphQL; @Autowired private HouseResourcesService houseResourcesService; @PostConstruct public void init() throws IOException &#123; File file = ResourceUtils.getFile(&quot;classpath:haoke.graphqls&quot;); GraphQLSchema graphQLSchema = buildSchema(file); this.graphQL = GraphQL.newGraphQL(graphQLSchema).build(); &#125; private GraphQLSchema buildSchema(File file) &#123; TypeDefinitionRegistry typeRegistry = new SchemaParser().parse(file); RuntimeWiring runtimeWiring = buildWiring(); SchemaGenerator schemaGenerator = new SchemaGenerator(); return schemaGenerator.makeExecutableSchema(typeRegistry, runtimeWiring); &#125; private RuntimeWiring buildWiring() &#123; return RuntimeWiring.newRuntimeWiring() .type(&quot;HaokeQuery&quot;, builder -&gt; builder.dataFetcher(&quot;HouseResources&quot;, environment -&gt; &#123; Long id = environment.getArgument(&quot;id&quot;); return this.houseResourcesService.queryById(id); &#125; )).build(); &#125; @Bean public GraphQL graphQL() &#123; return graphQL; &#125; &#125; 以后每当增加查询时，都需要修改该方法，如果查询方法很多的话，那么这个方法将变得非常难以维护，所以需要 进改进。 编写MyDataFetcher接口 1234public interface MyDataFetcher &#123; String fieldName();//查询名称 Object dataFetcher(DataFetchingEnvironment environment);//具体实现查询的逻辑&#125; 编写实现类HouseResourcesDataFetcher 1234567891011121314@Component //加入到Spring容器 public class HouseResourcesDataFetcher implements MyDataFetcher &#123; @Autowired private HouseResourcesService houseResourcesService; @Override public String fieldName() &#123; return &quot;HouseResources&quot;; &#125; @Override public Object dataFetcher(DataFetchingEnvironment environment) &#123; Long id = environment.getArgument(&quot;id&quot;); return this.houseResourcesService.queryById(id); //实现了逻辑 &#125; &#125; 修改GraphQLProvider逻辑 12345678910111213141516171819202122232425262728293031@Component public class GraphQLProvider &#123; private GraphQL graphQL; @Autowired private List&lt;MyDataFetcher&gt; myDataFetchers; //注入容器中所有的MyDataFetcher实现类 @PostConstruct public void init() throws IOException &#123; File file = ResourceUtils.getFile(&quot;classpath:haoke.graphqls&quot;); GraphQLSchema graphQLSchema = buildSchema(file); this.graphQL = GraphQL.newGraphQL(graphQLSchema).build(); &#125; private GraphQLSchema buildSchema(File file) &#123; TypeDefinitionRegistry typeRegistry = new SchemaParser().parse(file); RuntimeWiring runtimeWiring = buildWiring(); SchemaGenerator schemaGenerator = new SchemaGenerator(); return schemaGenerator.makeExecutableSchema(typeRegistry, runtimeWiring); &#125; private RuntimeWiring buildWiring() &#123; return RuntimeWiring.newRuntimeWiring() .type(&quot;HaokeQuery&quot;, builder -&gt; &#123; for (MyDataFetcher myDataFetcher : myDataFetchers) &#123; builder.dataFetcher(myDataFetcher.fieldName(), environment -&gt; myDataFetcher.dataFetcher(environment)); &#125; return builder; &#125;).build(); &#125; @Bean public GraphQL graphQL() &#123; return graphQL; &#125; &#125; 后面如果要加业务逻辑之类的直接实现MyDataFetcher接口，把实现逻辑方法写一下，在Provider中通过myDataFetchers注入容器中所有的实现类，然后在方法中循环实现","categories":[],"tags":[]},{"title":"GraphQL学习之java实现","slug":"GraphQL学习之java实现","date":"2020-10-30T00:33:21.000Z","updated":"2020-10-30T01:40:11.312Z","comments":true,"path":"2020/10/30/GraphQL学习之java实现/","link":"","permalink":"http://example.com/2020/10/30/GraphQL%E5%AD%A6%E4%B9%A0%E4%B9%8Bjava%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"1.导入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.graphql-java&lt;/groupId&gt; &lt;artifactId&gt;graphql-java&lt;/artifactId&gt; &lt;version&gt;11.0&lt;/version&gt; &lt;/dependency&gt; 说明：graphql-java包并没有发布到maven中央仓库，需要配置第三方仓库才能使用。 在setting.xml文件里进行配置 12345678910111213141516171819202122232425262728&lt;profile&gt; &lt;id&gt;bintray&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;bintray&lt;/id&gt; &lt;url&gt;http://dl.bintray.com/andimarek/graphql-java&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;bintray&lt;/id&gt; &lt;url&gt;http://dl.bintray.com/andimarek/graphql-java&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; ………………………………………… &lt;activeProfiles&gt; ……………… &lt;activeProfile&gt;bintray&lt;/activeProfile&gt; &lt;/activeProfiles&gt; 创建user对象： 12345678910111213141516171819202122232425262728293031package cn.itcast.graphql.vo;public class User &#123; private Long id; private String name; private Integer age; public User() &#123; &#125; public User(Long id, String name, Integer age) &#123; this.id = id; this.name = name; this.age = age; &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; &#125; 编写查询User对象实现： 123456789101112#对应的User定义如下 schema &#123; #定义查询 query: UserQuery &#125;type UserQuery &#123; #定义查询的类型 user : User #指定对象以及参数类型 &#125;type User &#123; #定义对象 id:Long! # !表示该属性是非空项 name:String age:Int &#125; Java实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package cn.itcast.graphql.demo; import cn.itcast.graphql.vo.User; import graphql.ExecutionResult; import graphql.GraphQL; import graphql.schema.GraphQLFieldDefinition; import graphql.schema.GraphQLObjectType; import graphql.schema.GraphQLSchema; import graphql.schema.StaticDataFetcher; import static graphql.Scalars.*; import static graphql.schema.GraphQLFieldDefinition.newFieldDefinition; import static graphql.schema.GraphQLObjectType.newObject;public class GraphQLDemo &#123; /** * 定义Schema * &lt;p&gt; * schema &#123; #定义查询 * query: UserQuery * &#125; * * @return */ public static GraphQLSchema createGraphqlSchema(GraphQLFieldDefinition userDefinition) &#123; GraphQLObjectType userQuery = newObject().name(&quot;userQuery&quot;) .field(userDefinition).build(); return GraphQLSchema.newSchema().query(userQuery).build(); &#125; /** * 定义查询的类型 * &lt;p&gt; * type UserQuery &#123; #定义查询的类型 * user : User #指定对象 * &#125; * * @return */ public static GraphQLFieldDefinition createUserDefinition(GraphQLObjectType userType) &#123; return newFieldDefinition() .name(&quot;User&quot;) .type(userType) //静态数据 （和下面设置参数二选一） .dataFetcher(new StaticDataFetcher(new User(1L, &quot;张三&quot;, 20))) // 设置参数 （和上面静态数据二选一） .argument(newArgument().name(&quot;id&quot;).type(GraphQLLong).build()) .dataFetcher(environment -&gt; &#123; // environment是从前端传来的数据 Long id = environment.getArgument(&quot;id&quot;); return new User(id, &quot;张三_&quot;+id, 20 + id.intValue()); &#125;) .build(); &#125; /** * 定义User对象类型 * * type User &#123; #定义对象 * id:Long! # !表示该属性是非空项 * name:String * age:Int * &#125; * * * @return */ public static GraphQLObjectType createUserObjectType()&#123; return newObject() .name(&quot;User&quot;) .field(newFieldDefinition().name(&quot;id&quot;).type(GraphQLLong)) .field(newFieldDefinition().name(&quot;name&quot;).type(GraphQLString)) .field(newFieldDefinition().name(&quot;age&quot;).type(GraphQLInt)) .build(); &#125; //主函数测试 public static void main(String[] args) &#123; //得到userType放入GraphQLFieldDefinition中 GraphQLObjectType userObjectType = createUserObjectType(); //得到userDefinition放入GraphQLSchema中 GraphQLFieldDefinition userDefinition = createUserDefinition(userObjectType); //新建GraphQL对象 GraphQL graphQL = GraphQL.newGraphQL(createGraphqlSchema(userDefinition)).build(); //查询条件 String query = &quot;&#123;User&#123;id,name&#125;&#125;&quot;; //&#123;User(id:1)&#123;id,name&#125;&#125; 设置查询参数的查询条件 //执行 ExecutionResult executionResult = graphQL.execute(query); //打印数据 System.out.println... &#125;&#125; 使用SDL构建schema 推荐使用：SDL方法 创建user.graphqls文件 在resources目录下创建user.graphqls文件: 1234567891011schema &#123; query: UserQuery&#125;type UserQuery &#123; user(id:Long) : User &#125;type User &#123; id:Long! name:String age:Int &#125; 安装graphql插件 实现：因为在配置文件中，所以一定要读取配置文件得到相应的值 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class GraphQLSDLDemo &#123; /** * 读取文件内容 * * @param fileName * @return */ public static String readFileToString(String fileName)&#123; try &#123; return IOUtils.toString(GraphQLSDLDemo.class.getClassLoader().getResourceAsStream(fileName ), &quot;UTF-8&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; //定义Schema public static GraphQLSchema createGraphqlSchema(TypeDefinitionRegistry typeRegistry, RuntimeWiring wiring) &#123; SchemaGenerator schemaGenerator = new SchemaGenerator(); return schemaGenerator.makeExecutableSchema(typeRegistry, wiring); &#125; // 定义类型的注册器 public static TypeDefinitionRegistry createTypeDefinitionRegistry(String fileContent)&#123; SchemaParser schemaParser = new SchemaParser(); return schemaParser.parse(fileContent); &#125; //解决数据查询问题 public static RuntimeWiring createRuntimeWiring() &#123; return RuntimeWiring.newRuntimeWiring() .type(&quot;UserQuery&quot;, typeWiring -&gt; typeWiring .dataFetcher(&quot;user&quot;, environment -&gt; &#123; Long id = environment.getArgument(&quot;id&quot;); return new User(id, &quot;张三_&quot;+id, 20 + id.intValue()); &#125;) ).build(); &#125; public static void main(String[] args) &#123; String fileName = &quot;user.graphqls&quot;; TypeDefinitionRegistry registry=createTypeDefinitionRegistry(readFileToString(fileName)); RuntimeWiring runtimeWiring = createRuntimeWiring(); //新建GraphQl对象--createGraphqlSchema实现方式与第一次不同，两个参数 GraphQL graphQL = GraphQL.newGraphQL(createGraphqlSchema(registry, runtimeWiring)).build(); String query = &quot;&#123;user(id:1)&#123;id,name,age&#125;&#125;&quot;; ExecutionResult executionResult = graphQL.execute(query); System.out.println... &#125;&#125; 两种方式的数据都是静态数据，以后要自己实现查询语句。 第一种用java一步一步的写graphql语句一样，先定义查询，再定义查询类型，再定义对象类型； 第二种写配置文件，读取之后放入定义类型的注册器中，createGraphqlSchema还需要一个参数就是runtimeWiring解决数据查询问题","categories":[],"tags":[]},{"title":"GraphQL入门","slug":"GraphQL入门","date":"2020-10-29T23:57:42.000Z","updated":"2020-10-30T00:29:09.497Z","comments":true,"path":"2020/10/30/GraphQL入门/","link":"","permalink":"http://example.com/2020/10/30/GraphQL%E5%85%A5%E9%97%A8/","excerpt":"","text":"一、GraphQL介绍GraphQL 是由 Facebook 创造的用于描述复杂数据模型的一种查询语言。这里查询语言所指的并不是常规意义上 的类似 sql 语句的查询语言，而是一种用于前后端数据查询方式的规范。 二、GraphQL比Restful的优点解决Restful接口的资源浪费问题，因为当我们想要查询某请求的id和name字段，但是他还有很多字段我们不需要，如果全部拿到而只响应id和name那就是一种资源浪费。 123456789#请求 GET http://127.0.0.1/user/1001#响应： &#123; id : 1001, name : &quot;张三&quot;, age : 20, //不需要 address : &quot;北京市&quot;, //不需要 …… //不需要&#125; Restful接口的请求与响应 还有一种问题是Retful的一次请求不能满足需求，需要有多次请求才能完成，而GraphQL可以一次请求满足 123456789101112131415161718#查询用户信息 GET http://127.0.0.1/user/1001 #响应： &#123; id : 1001, name : &quot;张三&quot;, age : 20, address : &quot;北京市&quot;, …… &#125;#查询用户的身份证信息 GET http://127.0.0.1/card/8888 #响应： &#123; id : 8888, name : &quot;张三&quot;, cardNumber : &quot;999999999999999&quot;, address : &quot;北京市&quot;, …… &#125; Restful接口的多次请求满足响应需求 三、进一步了解GraphQL1.按需索取，避免浪费 2.一次查询多个数据 3.API演进无需划分版本 四、查询规范1.字段：查询和其结果拥有几乎一样的结构 2.参数：语法格式：（参数名：参数值） 3.别名：一次查询多个相同对象，但是值不同，要起别名 别名1：对象(){} 别名2：对象(){} 4.片段：查询对的属相如果相同，可以采用片段的方式进行简化定义 12345678910别名1：对象()&#123; ...片段名&#125;别名2：对象()&#123; ...片段名&#125;fragment 片段名 on Character&#123; 属性 ...&#125; 五、GraphQL的Schema和类型规范Schema定义结构： 1234567891011schema &#123; #定义查询 query: UserQuery &#125;type UserQuery &#123; #定义查询的类型 user(id:ID) : User #指定对象以及参数类型 &#125;type User &#123; #定义对象 id:ID! # !表示该属性是非空项 name:String age:Int &#125; 标量类型： Int ：有符号 32 位整数。 Float ：有符号双精度浮点值。 String ：UTF‐8 字符序列。 Boolean ： true 或者 false 。 ID ：ID 标量类型表示一个唯一标识符，通常用以重新获取对象或者作为缓存中的键。 接口：跟许多类型系统一样，GraphQL 支持接口。一个接口是一个抽象类型，它包含某些字段，而对象类型必须包含这 些字段，才能算实现了这个接口。","categories":[],"tags":[]},{"title":"JVM基本参数命令","slug":"JVM基本参数命令","date":"2020-10-28T02:47:25.000Z","updated":"2020-10-28T03:07:50.904Z","comments":true,"path":"2020/10/28/JVM基本参数命令/","link":"","permalink":"http://example.com/2020/10/28/JVM%E5%9F%BA%E6%9C%AC%E5%8F%82%E6%95%B0%E5%91%BD%E4%BB%A4/","excerpt":"","text":"JVM参数类型分三种：1.标准参数、2.-X参数、3.-XX参数（使用率高） 1234567标准参数常用命令：java -version 查看版本java -showversion 查看版本并退出java -D&lt;名称&gt;&#x3D;&lt;值&gt; 设置系统属性java -help 输出帮助消息java -server 选择server VM 默认就是serverjava -client 选择client VM -server与-client区别：ServerVM初始堆空间大，默认使用并行垃圾回收器，启动慢运行快，ClientVM初始堆空间小，使用串行垃圾回收器，启动更快，运行更慢。还有一点就是现在64位机器只支持server不支持client了。 12345-X参数常用命令：主要是三种模式-Xint：解释模式-Xcomp：编译模式-Xmixed：混合模式 -Xint模式会强制JVM执行所有字节码，运行速度低，-Xcomp使用时，会把所有代码编译成本地代码，-Xmixed是混合模式，是默认的，由JVM自己决定。-Xint是编译比较快而运行是比较慢的-Xcomp是编译比较慢但是运行是比较快的。 1234567-XX参数常用命令：两种方式，boolean类型与非boolean类型boolean：-XX：[+-]&lt;名称&gt; 表示禁止或启动某命令非boolean：-XX：NewRatio&#x3D;1 表示新生代与老年代的比值-Xms：JVM初始堆内存，等价于-XX:InitialHeapSize-XmX：JVm最大堆内存，等价于-XX:MaxHeapSize，-XX:+PrintFlagsFinal 查看jvm运行参数 、运行-XX：+PrintFlagsFinal查看运行参数时，出现=与:=两种形式，其中=表示原始值，而:=表示修改过的值。 12345678910111213jps -l 查看进程idjinfo -flags &lt;进程id&gt; 查看正在运行的jvm所有参数jinfo -flags &lt;参数名&gt; &lt;进程id&gt; 查看某个特定参数jstat 查看堆内存使用情况jstat -class &lt;pid&gt; 查看class加载统计jstat -complier &lt;pid&gt; 查看编译统计jstat -gc &lt;pid&gt; 查看垃圾回收统计jmap 对堆内存进行统计分析jmap -heap &lt;pid&gt; 查看内存使用情况jmap -histo &lt;pid&gt; | more 查看内存中所有对象数量及大小jmap -histo:live &lt;pid&gt; | more 查看内存中活跃对象的数量及大小jmap -dump:format&#x3D;b,file&#x3D;&#x2F;temp&#x2F;dumpdat &lt;pid&gt; 将内存使用情况dump到文件中jhat -port &lt;port&gt; &lt;file&gt; 用jhat将二进制dump文件打开并开启一个端口供访问分析 两个可视化工具，MAT，VisualVM。","categories":[],"tags":[]},{"title":"JVM垃圾回收和垃圾收集器","slug":"JVM垃圾回收和垃圾收集器","date":"2020-10-28T02:47:10.000Z","updated":"2020-10-28T03:07:26.962Z","comments":true,"path":"2020/10/28/JVM垃圾回收和垃圾收集器/","link":"","permalink":"http://example.com/2020/10/28/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%92%8C%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"java与C语音不同，C语音需要手动垃圾回收，而java只需要关心内存申请，不需要关心垃圾回收。 算法：：垃圾回收算法：引用计数法、标记清除法、标记压缩法、复制算法、分代算法等。引用计数法：每个对象都有一个计数器，每当对象被引用时就+1，不被引用就-1，最后都不引用的时候，计数器为0，这时如果进行垃圾回收，此对象会被回收掉。优点：实时性好，不用等内存满了再清理，计数器为0可以直接回收掉；缺点：更新数据会有一点时间开销，浪费cpu资源，最大的问题是无法解决循环引用问题。 标记清除法：从根节点开始标记引用的对象，未被标记的就回收掉，即使两个对象循环引用但是从根节点标记引用的对象没有这两个，所以会被回收掉。解决了循环引用无法回收的问题，这是优点，缺点是效率低下，每次都要遍历所有对象，对其进行标记和清除，并且清理后的内存碎片化严重 标记压缩法：标记清除法的改进，标记相同，清除时，先将存活的对象压缩到内存一端，然后清理掉其他对象，优点是解决了碎片化问题，但是多了一步移动对象的操作，效率也会低下。 复制算法：复制算法的核心就是，将原有的内存空间一分为二，每次只用其中的一块，在垃圾回收时，将正在使用的对象复制到另一个内存空间中，然后将该内存空间清空，交换两个内存的角色，完成垃圾的回收。值得一提的是jvm中的年轻代两个survivor区就是用的这样的算法。优点是垃圾对象多时，效率较高，内存无碎片化。缺点是垃圾对象少时，不适用，比如老年代，再就是一次只能用一半的内存空间内存使用率低。 分代算法：根据回收对象的特点进行选择，在jvm中，年轻代适合使用复制算法，老年代适合使用标记清除或标记压缩算法。 收集器：包括：串行垃圾收集器、并行垃圾收集器、CMS（并发）垃圾收集器、G1垃圾收集器。串行垃圾收集器：是指使用单线程进行垃圾回收，工作时只有一个线程，其他程序都要暂停，这种现象称为STW（Stop-The-World），对于交互性较强的应用而言，这种垃圾收集器是不能够接受的。一般在Javaweb应用中是不会采用该收集器的。因为交互性的应用暂停服务会影响使用效果。 并行垃圾收集器：将单线程改为了多线程进行垃圾回收，这样可以缩短垃圾回收的时间。但是还是要出现STW现象 ①ParNew垃圾收集器：只是单纯的把单线程改为多线程。 ②ParallelGC垃圾收集器：新增了两个和系统吞吐量相关的参数，使得其使用起来更加的灵活和高效。 CMS垃圾收集器：CMS以获取最小停顿时间为目的，这样就可以用在交互性的应用上。因为要获取最小停顿时间，所以对执行过程进行细化，比较复杂，下面是运行过程： 初始化标记(CMS-initial-mark) ,标记root，会导致stw； 并发标记(CMS-concurrent-mark)，与用户线程同时运行； 预清理（CMS-concurrent-preclean），与用户线程同时运行； 重新标记(CMS-remark) ，会导致stw； 并发清除(CMS-concurrent-sweep)，与用户线程同时运行； 调整堆大小，设置CMS在清理之后进行内存压缩，目的是清理内存中的碎片； 并发重置状态等待下次CMS的触发(CMS-concurrent-reset)，与用户线程同时运行； G1垃圾收集器：G1的设计原则就是简化JVM性能调优，开发人员只需要简单的三步即可完成调优：第一步，开启G1垃圾收集器第二步，设置堆的最大内存第三步，设置最大的停顿时间。 G1中提供了三种模式垃圾回收模式，Young GC、Mixed GC 和 Full GC，在不同的条件下被触发。 G1垃圾收集器的设计思想：与其他收集器相比，G1的最大区别是它摒弃了年轻代、老年代的物理划分，采用将堆内存分为若干个区域，这些区域包含了逻辑上的老年区，年轻区，还有一种特殊区域Humongous区，它主要装的是巨型对象。 在G1划分的区域中，年轻代的垃圾收集依然采用暂停所有应用线程的方式，将存活对象拷贝到老年代或者Survivor空间，G1收集器通过将对象从一个区域复制到另外一个区域，完成了清理工作。这就意味着，在正常的处理过程中，G1完成了堆的压缩（至少是部分堆的压缩），这样也就不会有cms内存碎片问题的存在了。 Young GC：主要对Eden区进行GC，在Eden区快耗尽时触发，存活数据移动到Survivor区，如果Survivor不够，则有部分移到年老代。Eden区数据为空时，停止GC。 Remembered Set（已记忆集合）其作用是跟踪指向某个堆内的对象引用。为找到年轻代的根对象采用这种方法，不然将年老代遍历一遍会很浪费时间。 Mixed GC：回收整个年轻代和部分年老代。触发是由参数 -XX:InitiatingHeapOccupancyPercent=n 决定。默认：45%，该参数的意思是：当老年代大小占整个堆大小百分比达到该阀值时触发。 123456789101112‐XX:+PrintGC 输出GC日志‐XX:+PrintGCDetails 输出GC的详细日志‐XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式）‐XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2013‐05‐04T21:53:59.234+0800）‐XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息‐Xloggc:..&#x2F;logs&#x2F;gc.log 日志文件的输出路径测试：‐XX:+UseG1GC ‐XX:MaxGCPauseMillis&#x3D;100 ‐Xmx256m ‐XX:+PrintGCDetails ‐XX:+PrintGCTimeStamps ‐XX:+PrintGCDateStamps ‐XX:+PrintHeapAtGC ‐Xloggc:F:&#x2F;&#x2F;test&#x2F;&#x2F;gc.log 最后介绍一个GC日志查看的网址：GC Easy是一款在线的可视化工具，易用、功能强大，网站：http://gceasy.io/","categories":[],"tags":[]},{"title":"Tomcat8优化入门","slug":"Tomcat8优化入门","date":"2020-10-28T02:46:53.000Z","updated":"2020-10-28T03:11:02.324Z","comments":true,"path":"2020/10/28/Tomcat8优化入门/","link":"","permalink":"http://example.com/2020/10/28/Tomcat8%E4%BC%98%E5%8C%96%E5%85%A5%E9%97%A8/","excerpt":"","text":"1.禁用AJP服务，默认开启占用8009端口 2.设置线程池，maxThreads：最大并发数，默认设置 200，一般建议在 500 ~ 1000；minSpareThreads：Tomcat 初始化时创建的线程数，默认设置 25；prestartminSpareThreads： 在 Tomcat 初始化的时候就初始化 minSpareThreads 的参数值，如果不等于 true，minSpareThreads 的值就没啥效果了 3.nio2运行模式，protocol=”org.apache.coyote.http11.Http11Nio2Protocol” 4.设置jvm垃圾回收器 前三个是对tomcat8自身优化，后面是对jvm参数调整。","categories":[],"tags":[]},{"title":"关于向数据库添加信息时未设置id报错","slug":"关于向数据库添加信息时未设置id报错","date":"2020-10-28T02:46:00.000Z","updated":"2020-10-28T03:13:50.130Z","comments":true,"path":"2020/10/28/关于向数据库添加信息时未设置id报错/","link":"","permalink":"http://example.com/2020/10/28/%E5%85%B3%E4%BA%8E%E5%90%91%E6%95%B0%E6%8D%AE%E5%BA%93%E6%B7%BB%E5%8A%A0%E4%BF%A1%E6%81%AF%E6%97%B6%E6%9C%AA%E8%AE%BE%E7%BD%AEid%E6%8A%A5%E9%94%99/","excerpt":"","text":"错误信息：ids for this class must be manually assigned before calling save(): sample.db.Completedsample 原因分析：从字面上理解的意思是，在save之前，必须手动指定id，其中id的； 解决办法：将主键设为自增，原先需要主键表示的数据，重新设置一个键。此时主键id的 ；之前主键没有设为自增，将id的generator的class设为increment也是同样的效果。","categories":[],"tags":[]},{"title":"记录一下一个小bug 关于Map集合的","slug":"记录一下一个小bug-关于Map集合的","date":"2020-10-28T02:44:55.000Z","updated":"2020-10-28T03:12:51.456Z","comments":true,"path":"2020/10/28/记录一下一个小bug-关于Map集合的/","link":"","permalink":"http://example.com/2020/10/28/%E8%AE%B0%E5%BD%95%E4%B8%80%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%B0%8Fbug-%E5%85%B3%E4%BA%8EMap%E9%9B%86%E5%90%88%E7%9A%84/","excerpt":"","text":"1234567891011121314151617181920212223@data//lombokpublic class ProfileResult&#123;//成员变量//...private Map&lt;String, Object&gt; map;//构造方法...String menus = &quot;&quot;;this.map.put(&quot;menus&quot;,menus);&#125;//-----------------------------@data//lombokpublic class ProfileResult&#123;//成员变量//...private Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();//构造方法...String menus = &quot;&quot;;this.map.put(&quot;menus&quot;,menus);&#125; 这两种写法，第一种会报错；因为map没有指向Map对象，报空指针异常；而第二种就在定义变量时赋值再调用方法时不会报异常。","categories":[],"tags":[]},{"title":"关于拦截器方法抛异常问题","slug":"关于拦截器方法抛异常问题","date":"2020-10-28T02:44:22.000Z","updated":"2020-10-28T03:12:02.082Z","comments":true,"path":"2020/10/28/关于拦截器方法抛异常问题/","link":"","permalink":"http://example.com/2020/10/28/%E5%85%B3%E4%BA%8E%E6%8B%A6%E6%88%AA%E5%99%A8%E6%96%B9%E6%B3%95%E6%8A%9B%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98/","excerpt":"","text":"前些日子写了个SpringMVC的拦截器方法preHandle最后判断否的时候抛出自定义异常，但是前端没有接收到，并报错自定义异常为null CommonException：null 然后查了一下，应该写一个异常处理类加@ControllerAdvice注解并扫描拦截器的包，方法是这样的： 1234567891011121314@ExceptionHandler(value = Exception.class)//异常处理注解 @ResponseBody//响应对象转化为json public Result error(HttpServletRequest request, HttpServletResponse response,Exception e) &#123; e.printStackTrace(); if(e.getClass() == CommonException.class) &#123;//接收到异常并判断是否为CommonException //类型转型 CommonException ce = (CommonException) e; Result result = new Result(ce.getResultCode());//返回result return result; &#125;else&#123; Result result = new Result(ResultCode.SERVER_ERROR); return result; &#125; &#125;","categories":[],"tags":[]},{"title":"Shrio安全框架入门","slug":"Shrio安全框架入门","date":"2020-10-28T02:44:05.000Z","updated":"2020-10-28T03:10:32.924Z","comments":true,"path":"2020/10/28/Shrio安全框架入门/","link":"","permalink":"http://example.com/2020/10/28/Shrio%E5%AE%89%E5%85%A8%E6%A1%86%E6%9E%B6%E5%85%A5%E9%97%A8/","excerpt":"","text":"作为安全框架shiro更加便捷简单，相比与Spring Security来说。 shiro主要使用的几个内部结构： Subject:主体，可以看到主体可以是任何可以与应用交互的“用户”; SecurityManager:相当于SpringMVC中的DispatcherServlet或者Struts2中的FilterDispatcher;是Shiro的心 脏;所有具体的交互都通过SecurityManager进行控制;它管理着所有Subject、且负责进行认证和授权、及会 话、缓存的管理。 Authenticator:认证器，负责主体认证的。 Authrizer:授权器，或者访问控制器，用来决定主体是否有权限进行相应的操作。 Realm:可以有1个或多个Realm，可以认为是安全实体数据源，即用于获取安全实体的;可以是JDBC实现，也可 以是LDAP实现，或者内存实现等等;由用户提供;注意:Shiro不知道你的用户/权限存储在哪及以何种格式存储; 所以我们一般在应用中都需要实现自己的Realm; 应用程序使用shiro过程 应用代码通过Subject来进行认证和授权，Subject委托给SecurityManage ,Realm被注入到SecurityManage中，Realm相当于从数据库获取安全数据。 1基于ini运行模式 12345678910111213141516171819202122232425262728293031323334 #模拟从数据库查询的用户 #数据格式 用户名=密码 zhangsan=123456 lisi=654321 //配置文件//认证 @Testpublic void testLogin() throws Exception&#123; //1.加载ini配置文件创建SecurityManager Factory&lt;SecurityManager&gt; factory = new IniSecurityManagerFactory(&quot;classpath:shiro.ini&quot;); //2.获取securityManagerSecurityManager securityManager = factory.getInstance(); //3.将securityManager绑定到当前运行环境 SecurityUtils.setSecurityManager(securityManager); //4.创建主体(此时的主体还为经过认证)Subject subject = SecurityUtils.getSubject();/*** 模拟登录，和传统等不同的是需要使用主体进行登录*///5.构造主体登录的凭证(即用户名/密码)//第一个参数:登录用户名，第二个参数:登录密码UsernamePasswordToken upToken = new UsernamePasswordToken(&quot;zhangsan&quot;,&quot;123456&quot;); //6.主体登录subject.login(upToken);//7.验证是否登录成功System.out.println(&quot;用户登录成功=&quot;+subject.isAuthenticated());//8.登录成功获取数据//getPrincipal 获取登录成功的安全数据System.out.println(subject.getPrincipal());&#125;//授权//配置文件 [users] #数据格式 用户名=密码,角色1,角色2.. zhangsan=123456,role1,role2 [roles] #数据格式 角色名=权限1，权限2 role1=user:save,user:update//7.用户认证成功之后才可以完成授权工作boolean hasPerm = subject.isPermitted(&quot;user:save&quot;);System.out.println(&quot;用户是否具有save权限=&quot;+hasPerm); 自定义realm 需要继承AuthorizingRealm父类 重写父类中的两个方法， doGetAuthorizationInfo :授权 从principals参数变量中获取已认证用户的信息，然后查询根据得到的id或者用户查询权限 doGetAuthenticationInfo :认证 认证的主要目的，比较用户输入的用户名密码是否和数据库中的一致，首先要把传入的变量强转为UsernamePasswordToken以获取用户名和密码 认证流程： 123451.Subject的login方法登录2.所有的Subject都委托于Security Manager管理3.Security Manager委托给Authenticator认证 它才是真正的身份验证，shiro API的核心身份认证入口，可以自定义实现4.Authenticator委托给AuthenticationStrategy进行Realm身份验证5.把token传入Realm中，获取身份验证信息 授权流程： 12341. 首先调用Subject.isPermitted&#x2F;hasRole接口，其会委托给SecurityManager，而SecurityManager接着会委托 给Authorizer;2. Authorizer是真正的授权者，如果我们调用如isPermitted(“user:view”)，其首先会通过PermissionResolver 把字符串转换成相应的Permission实例;3. 在进行授权之前，其会调用相应的Realm获取Subject相应的角色&#x2F;权限用于匹配传入的角色&#x2F;权限;4. Authorizer会判断Realm的角色&#x2F;权限是否和传入的匹配，如果有多个Realm，会委托给ModularRealmAuthorizer进行循环判断，如果匹配如isPermitted&#x2F;hasRole会返回true，否则返回false表示 授权失败。","categories":[],"tags":[]},{"title":"Shiro与SpringBoot整合","slug":"Shiro与SpringBoot整合","date":"2020-10-28T02:43:54.000Z","updated":"2020-10-28T03:05:19.967Z","comments":true,"path":"2020/10/28/Shiro与SpringBoot整合/","link":"","permalink":"http://example.com/2020/10/28/Shiro%E4%B8%8ESpringBoot%E6%95%B4%E5%90%88/","excerpt":"","text":"导入依赖 修改登录方法 自定义Realm Shiro配置 Shiro过滤器 授权 1基于配置授权 2基于注解授权 导入依赖: 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt; 修改登录方法：shiro需要采集到用户登录数据使用subject的login方法进入realm完成认证工作。 1234567891011 @RequestMapping(value=&quot;/login&quot;)public String login(String username,String password) &#123; try&#123; Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken uptoken = new UsernamePasswordToken(username,password); subject.login(uptoken); return &quot;登录成功&quot;; &#125; catch (Exception e) &#123; return &quot;用户名或密码错误&quot;; &#125;&#125; 自定义Realm：Shiro从Realm获取安全数据(如用户、角色、权限) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class CustomRealm extends AuthorizingRealm &#123; @Override public void setName(String name) &#123; super.setName(&quot;customRealm&quot;); &#125; @Autowired private UserService userService;/*** 构造授权方法 */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) &#123; //1.获取认证的用户数据 User user = (User)principalCollection.getPrimaryPrincipal(); //2.构造认证数据 SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); Set&lt;Role&gt; roles = user.getRoles(); for (Role role : roles) &#123; //添加角色信息 info.addRole(role.getName()); for (Permission permission:role.getPermissions()) &#123; //添加权限信息 info.addStringPermission(permission.getCode()); &#125; &#125; return info; &#125;/*** 认证方法 */ protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException &#123; //1.获取登录的upToken UsernamePasswordToken upToken = (UsernamePasswordToken)authenticationToken; //2.获取输入的用户名密码 String username = upToken.getUsername(); String password = new String(upToken.getPassword()); //3.数据库查询用户 User user = userService.findByName(username); //4.用户存在并且密码匹配存储用户数据 if(user != null &amp;&amp; user.getPassword().equals(password)) &#123; return new SimpleAuthenticationInfo(user,user.getPassword(),this.getName()); &#125;else &#123; //返回null会抛出异常，表明用户不存在或密码不匹配 return null; &#125; &#125;&#125; Shiro的配置：SecurityManager 是 Shiro 架构的心脏，用于协调内部的多个组件完成全部认证授权的过程，使用基于springboot的配置方式完成SecurityManager，Realm的装配。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Configurationpublic class ShiroConfiguration &#123; //配置自定义的Realm @Bean public CustomRealm getRealm() &#123; return new CustomRealm(); &#125; //配置安全管理器 @Bean public SecurityManager securityManager(CustomRealm realm) &#123; //使用默认的安全管理器 DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(realm); //将自定义的realm交给安全管理器统一调度管理 securityManager.setRealm(realm); return securityManager; &#125; //Filter工厂，设置对应的过滤条件和跳转条件 @Bean public ShiroFilterFactoryBean shirFilter(SecurityManager securityManager) &#123; //1.创建shiro过滤器工厂 ShiroFilterFactoryBean filterFactory = new ShiroFilterFactoryBean(); //2.设置安全管理器 filterFactory.setSecurityManager(securityManager); //3.通用配置(配置登录页面，登录成功页面，验证未成功页面) filterFactory.setLoginUrl(&quot;/autherror?code=1&quot;); //设置登录页面 filterFactory.setUnauthorizedUrl(&quot;/autherror?code=2&quot;); //授权失败跳转页面 //4.配置过滤器集合/*** key :访问连接 支持通配符的形式* value:过滤器类型 * shiro常用过滤器* anno :匿名访问(表明此链接所有人可以访问)* authc :认证后访问(表明此链接需登录认证成功之后可以访问)*/ Map&lt;String,String&gt; filterMap = new LinkedHashMap&lt;String,String&gt;(); // 配置不会被拦截的链接 顺序判断 filterMap.put(&quot;/user/home&quot;, &quot;anon&quot;); filterMap.put(&quot;/user/**&quot;, &quot;authc&quot;); //5.设置过滤器 filterFactory.setFilterChainDefinitionMap(filterMap); return filterFactory; &#125; //配置shiro注解支持 @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(SecurityManager securityManager) &#123; AuthorizationAttributeSourceAdvisor advisor = new AuthorizationAttributeSourceAdvisor(); · advisor.setSecurityManager(securityManager); return advisor; &#125; &#125; shiro中的过滤器: Filter 解释 anon 无参，开放权限，可以理解为匿名用户或游客 authc 无参，需要认证 logout 无参，注销，执行后会直接跳转到 shiroFilterFactoryBean.setLoginUrl(); 设置的 url authcBasic 无参，表示 httpBasic 认证 user 无参，表示必须存在用户，当登入操作时不做检查 ssl 无参，表示安全的URL请求，协议为 https perms[user] 参数可写多个，表示需要某个或某些权限才能通过，多个参数时写 perms[“user, admin”]，当有多个参数时必须每个参数都通过才算通过 roles[admin] 参数可写多个，表示是某个或某些角色才能通过，多个参数时写 roles[“admin，user”]， 当有多个参数时必须每个参数都通过才算通过 rest[user] 根据请求的方法，相当于 perms[user:method]，其中 method 为 post，get，delete 等 port[8081] 当请求的URL端口不是8081时，跳转到当前访问主机HOST的8081端口 注意:anon, authc, authcBasic, user 是第一组认证过滤器，perms, port, rest, roles, ssl 是第二组授权过滤 器，要通过授权过滤器，就先要完成登陆认证操作(即先要完成认证才能前去寻找授权) 才能走第二组授权器 (例如访问需要 roles 权限的 url，如果还没有登陆的话，会直接跳转到 shiroFilterFactoryBean.setLoginUrl(); 设置的 url ) 基于配置的授权 123456789//配置请求连接过滤器配置//匿名访问(所有人员可以使用) filterMap.put(&quot;/user/home&quot;, &quot;anon&quot;); //具有指定权限访问filterMap.put(&quot;/user/find&quot;, &quot;perms[user-find]&quot;); //认证之后访问(登录之后可以访问) filterMap.put(&quot;/user/**&quot;, &quot;authc&quot;); //具有指定角色可以访问filterMap.put(&quot;/user/**&quot;, &quot;roles[系统管理员]&quot;); 基于注解的授权 123456789(1)RequiresPermissions //查询@RequiresPermissions(value = &quot;user-find&quot;) public String find() &#123;return &quot;查询用户成功&quot;; &#125;(2)RequiresRoles //查询@RequiresRoles(value = &quot;系统管理员&quot;) public String find() &#123;return &quot;查询用户成功&quot;; &#125;","categories":[],"tags":[]},{"title":"Shiro中的会话管理（怎样创建自己维护的会话）","slug":"Shiro中的会话管理（怎样创建自己维护的会话）","date":"2020-10-28T02:43:40.000Z","updated":"2020-10-28T03:01:13.203Z","comments":true,"path":"2020/10/28/Shiro中的会话管理（怎样创建自己维护的会话）/","link":"","permalink":"http://example.com/2020/10/28/Shiro%E4%B8%AD%E7%9A%84%E4%BC%9A%E8%AF%9D%E7%AE%A1%E7%90%86%EF%BC%88%E6%80%8E%E6%A0%B7%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%BB%B4%E6%8A%A4%E7%9A%84%E4%BC%9A%E8%AF%9D%EF%BC%89/","excerpt":"","text":"shiro提供了三个默认实现： DefaultSessionManager:用于JavaSE环境 ServletContainerSessionManager:用于Web环境，直接使用servlet容器的会话。 DefaultWebSessionManager:用于web环境，自己维护会话(自己维护着会话，直接废弃了Servlet容器的会话管理)。 在web程序中，通过shiro的Subject.login()方法登录成功后，用户的认证信息实际上是保存在HttpSession中的 Shiro结合redis的统一会话管理 构建环境 123456//导入依赖&lt;dependency&gt; &lt;groupId&gt;org.crazycake&lt;/groupId&gt; &lt;artifactId&gt;shiro-redis&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt; 1234//在springboot配置文件中添加redis配置 redis: host: 127.0.0.1 port: 6379 自定义shiro会话管理器 12345678910111213141516171819202122232425/*** 自定义的sessionManager 继承DefaultWebSessionManager*/public class CustomSessionManager extends DefaultWebSessionManager &#123;/*** 头信息中具有sessionid* 请求头:Authorization: sessionid ** 指定sessionId的获取方式*/ protected Serializable getSessionId(ServletRequest request, ServletResponse response) &#123; //获取请求头Authorization中的数据 String id = WebUtils.toHttp(request).getHeader(&quot;Authorization&quot;); if(StringUtils.isEmpty(id)) &#123; //如果没有携带，生成新的sessionId return super.getSessionId(request,response); &#125; else &#123; //返回sessionId; request.setAttribute(ShiroHttpServletRequest.REFERENCED_SESSION_ID_SOURCE,&quot;header&quot;); request.setAttribute(ShiroHttpServletRequest.REFERENCED_SESSION_ID, id); request.setAttribute(ShiroHttpServletRequest.REFERENCED_SESSION_ID_IS_VALID, Boolean.TRUE); &#125; &#125; return id; &#125; 配置Shiro基于redis的会话管理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950511. 配置shiro的RedisManager，通过shiro-redis包提供的RedisManager统一对redis操作 @Value(&quot;$&#123;spring.redis.host&#125;&quot;) private String host;@Value(&quot;$&#123;spring.redis.port&#125;&quot;) private int port;//配置shiro redisManagerpublic RedisManager redisManager() &#123; RedisManager redisManager = new RedisManager(); redisManager.setHost(host); redisManager.setPort(port);return redisManager;&#125;2. Shiro内部有自己的本地缓存机制，为了更加统一方便管理，全部替换redis实现 //配置Shiro的缓存管理器//使用redis实现public RedisCacheManager cacheManager() &#123; RedisCacheManager redisCacheManager = new RedisCacheManager(); redisCacheManager.setRedisManager(redisManager()); return redisCacheManager;&#125;3. 配置SessionDao，使用shiro-redis实现的基于redis的sessionDao /*** RedisSessionDAO shiro sessionDao层的实现 通过redis * 使用的是shiro-redis开源插件*/public RedisSessionDAO redisSessionDAO() &#123; RedisSessionDAO redisSessionDAO = new RedisSessionDAO(); redisSessionDAO.setRedisManager(redisManager()); return redisSessionDAO;&#125;4. 配置会话管理器，指定sessionDao的依赖关系 /*** 3.会话管理器 */public DefaultWebSessionManager sessionManager() &#123; CustomSessionManager sessionManager = new CustomSessionManager(); sessionManager.setSessionDAO(redisSessionDAO()); return sessionManager;&#125;5. 统一交给SecurityManager管理 //配置安全管理器@Beanpublic SecurityManager securityManager(CustomRealm realm) &#123; //使用默认的安全管理器 DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(realm); // 自定义session管理 使用redis securityManager.setSessionManager(sessionManager()); // 自定义缓存实现 使用redis securityManager.setCacheManager(cacheManager()); //将自定义的realm交给安全管理器统一调度管理 securityManager.setRealm(realm); return securityManager;&#125;","categories":[],"tags":[]},{"title":"导入依赖出现循环报错","slug":"导入依赖出现循环报错","date":"2020-10-28T02:43:22.000Z","updated":"2020-10-28T02:58:42.387Z","comments":true,"path":"2020/10/28/导入依赖出现循环报错/","link":"","permalink":"http://example.com/2020/10/28/%E5%AF%BC%E5%85%A5%E4%BE%9D%E8%B5%96%E5%87%BA%E7%8E%B0%E5%BE%AA%E7%8E%AF%E6%8A%A5%E9%94%99/","excerpt":"","text":"Error:java: Annotation processing is not supported for module cycles. Please ensure that all modules 分析：cycle [qrcode-common,qrcode-manager-pojo] ：从这里可以看出，qrcode-common,qrcode-manager-pojo这两个模块有问题，即互相依赖，类似死循环","categories":[],"tags":[]},{"title":"使用umi build出现的Path must be a string恶心解决方法","slug":"使用umi-build出现的Path-must-be-a-string恶心解决方法","date":"2020-10-28T02:42:09.000Z","updated":"2020-10-28T02:58:09.004Z","comments":true,"path":"2020/10/28/使用umi-build出现的Path-must-be-a-string恶心解决方法/","link":"","permalink":"http://example.com/2020/10/28/%E4%BD%BF%E7%94%A8umi-build%E5%87%BA%E7%8E%B0%E7%9A%84Path-must-be-a-string%E6%81%B6%E5%BF%83%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/","excerpt":"","text":"解决方法：按照官网升级umi-plugin-react的版本就完事，很多博客就写了个简单的文字叙述，具体怎么做也没做具体说明，出现这个错误的原因应该是umi插件版本与umi的版本有冲突，现在直接使用命令安装umi的话，系统都会默认给你安装最新版umi3，而umi3已经对原来的插件进行了更新，所以如果你的umi是最新版的，就要参考官方文档最新版的配置方式，不能一味地使用原来的配置，否则会一直报错。技术更新快，几个月前的新技术，在今天可能已经更新几个版本了，所以有问题，记得多看官方说明文档。 改完一定要重新导入依赖才能进行build！！！ 官网：https://umijs.org/docs/upgrade-to-umi-3#%E5%8D%87%E7%BA%A7-umi-plugin-react-%E4%B8%BA-umijspreset-react","categories":[],"tags":[]},{"title":"docker创建启动percona(mysql)闪退问题","slug":"docker创建启动percona-mysql-闪退问题","date":"2020-10-28T02:41:31.000Z","updated":"2020-10-28T02:57:23.057Z","comments":true,"path":"2020/10/28/docker创建启动percona-mysql-闪退问题/","link":"","permalink":"http://example.com/2020/10/28/docker%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8percona-mysql-%E9%97%AA%E9%80%80%E9%97%AE%E9%A2%98/","excerpt":"","text":"#创建容器docker create –name percona -v /data/mysql-data:/var/lib/mysql -p 3306:3306 -eMYSQL_ROOT_PASSWORD=root percona:5.7.23 docker start percona 直接闪退 因为/data目录没有访问权限 开启权限 chomd -R 777 data","categories":[],"tags":[]},{"title":"关于SpringBoot启动某Service实现类报错发现两个实例问题","slug":"关于SpringBoot启动某Service实现类报错发现两个实例问题","date":"2020-10-24T09:08:25.000Z","updated":"2020-10-28T02:50:26.087Z","comments":true,"path":"2020/10/24/关于SpringBoot启动某Service实现类报错发现两个实例问题/","link":"","permalink":"http://example.com/2020/10/24/%E5%85%B3%E4%BA%8ESpringBoot%E5%90%AF%E5%8A%A8%E6%9F%90Service%E5%AE%9E%E7%8E%B0%E7%B1%BB%E6%8A%A5%E9%94%99%E5%8F%91%E7%8E%B0%E4%B8%A4%E4%B8%AA%E5%AE%9E%E4%BE%8B%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题如下 1ServiceImpl required a single bean, but 2 were found; 原因是写了两个实现类继承某父类，而其中一个写了泛型，一个没写所以报错，部分代码如下： 123456789101112131415@Servicepublic class AdServiceImpl extends BaseServiceImpl implements AdService &#123; //分页查询广告 @Override public PageInfo&lt;Ad&gt; queryAdList(Ad ad, Integer page, Integer pageSize) &#123; //逻辑代码 //返回值 &#125;&#125;@Service@Transactionalpublic class HouseResourcesServiceImpl extends BaseServiceImpl&lt;HouseResources&gt; implements HouseResourcesService &#123; //方法代码&#125; 可以看出第一个实现类继承的BaseServiceImpl没有泛型，第二个存在，然后就报错了。改为对应的泛型就好了。","categories":[],"tags":[]},{"title":"关于SpringBoot启动报错DataSource问题","slug":"关于SpringBoot启动报错DataSource问题","date":"2020-10-24T09:07:05.000Z","updated":"2020-10-24T09:54:44.642Z","comments":true,"path":"2020/10/24/关于SpringBoot启动报错DataSource问题/","link":"","permalink":"http://example.com/2020/10/24/%E5%85%B3%E4%BA%8ESpringBoot%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99DataSource%E9%97%AE%E9%A2%98/","excerpt":"","text":"Springboot启动时报错 If you want an embedded database (H2, HSQL or Derby), please put it on the classpath.产生这个错误的原因是springboot的自动配置，如果你没有配置DataSource就会导致这个错误 版本一：解决方法 1@SpringBootApplication(exclude = DataSourceAutoConfiguration.class)//排除自动配置 关于exclude属性和DataSource问题 exclude属性是@SpringBootApplication注解中的@EnableAutoConfiguration属性 排除某个类的自动配置 在本例中，datasource没有配置所以启动时报错 版本二：上述问题又出现了一次这次按版本一没有成功，在网上找了很多答案, 其中的一篇:https://www.cnblogs.com/yourGod/p/9178515.html 按照上述的解决办法:) 版本一这样之后还会报错： 12345Description:Field userRepository in com.wcyq.demo.service.impl.UserServiceImpl required a bean of type &#39;com.wcyq.demo.domain.UserRepository&#39; that could not be found.Action:Consider defining a bean of type &#39;com.wcyq.demo.domain.UserRepository&#39; in your configuration.Process finished with exit code 1 这样dao就找不到了 最后解决办法是application.yml文件配置改变一下 123456789101112131415161718spring:application:name: ihrm-companydatasource:url: jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;ihrm?characterEncoding&#x3D;utf-8username: rootpassword: rootdriverClassName: com.mysql.jdbc.Driver改为下面的 加了druid连接spring:application:name: ihrm-companydatasource:druid:url: jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;ihrm?characterEncoding&#x3D;utf-8username: rootpassword: rootdriverClassName: com.mysql.jdbc.Driver 是yml文件出现了问题，在此记录一下 最后总结一下，导入了和数据库连接的依赖就要考虑你的datasource问题，如果需要就在配置文件中配置好，SpringBoot会自动导入；如果不需要，把exclude写上，或者依赖删掉。","categories":[],"tags":[]},{"title":"关于Spring组件未加人到容器报错问题","slug":"关于Spring组件未加人到容器报错问题","date":"2020-10-21T07:12:27.000Z","updated":"2020-10-21T07:13:28.583Z","comments":true,"path":"2020/10/21/关于Spring组件未加人到容器报错问题/","link":"","permalink":"http://example.com/2020/10/21/%E5%85%B3%E4%BA%8ESpring%E7%BB%84%E4%BB%B6%E6%9C%AA%E5%8A%A0%E4%BA%BA%E5%88%B0%E5%AE%B9%E5%99%A8%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98/","excerpt":"","text":"123问题描述：The injection point has the following annotations:- @org.springframework.beans.factory.annotation.Autowired(required&#x3D;true)Action: 原因是自己在写项目时不小心把Spring的Service组件忘记加注解了，导致没有导入到spring容器中，而另一个类引用了该组件，启动时找不到，所以报错。 以后出现这种错误可以猜测到是Spring的某个@Autowired注解下的组件未导入到容器中。。 在此记录一下","categories":[],"tags":[]},{"title":"第一篇hexo博客","slug":"第一篇hexo博客","date":"2020-10-21T07:06:37.000Z","updated":"2020-10-21T07:10:58.897Z","comments":true,"path":"2020/10/21/第一篇hexo博客/","link":"","permalink":"http://example.com/2020/10/21/%E7%AC%AC%E4%B8%80%E7%AF%87hexo%E5%8D%9A%E5%AE%A2/","excerpt":"","text":"第一篇hexo文章， 以后就在这里写博客。。 加油0.0","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-10-21T05:14:20.539Z","updated":"2020-10-21T05:23:31.288Z","comments":true,"path":"2020/10/21/hello-world/","link":"","permalink":"http://example.com/2020/10/21/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}