{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"书单","date":"2020-10-21T07:00:56.225Z","updated":"2020-10-21T05:23:31.270Z","comments":false,"path":"books/index.html","permalink":"http://example.com/books/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2020-10-21T07:00:56.224Z","updated":"2020-10-21T05:23:31.289Z","comments":false,"path":"repository/index.html","permalink":"http://example.com/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-10-21T07:00:56.225Z","updated":"2020-10-21T05:23:31.263Z","comments":true,"path":"links/index.html","permalink":"http://example.com/links/index.html","excerpt":"","text":""}],"posts":[{"title":"Spring——轻量级控制反转和面向切面编程框架","slug":"Spring——轻量级控制反转和面向切面编程框架","date":"2021-02-15T12:18:25.000Z","updated":"2021-02-20T13:14:41.393Z","comments":true,"path":"2021/02/15/Spring——轻量级控制反转和面向切面编程框架/","link":"","permalink":"http://example.com/2021/02/15/Spring%E2%80%94%E2%80%94%E8%BD%BB%E9%87%8F%E7%BA%A7%E6%8E%A7%E5%88%B6%E5%8F%8D%E8%BD%AC%E5%92%8C%E9%9D%A2%E5%90%91%E5%88%87%E9%9D%A2%E7%BC%96%E7%A8%8B%E6%A1%86%E6%9E%B6/","excerpt":"","text":"1、Spring1.1、简介 Spring：春天———-&gt;给软件行业带来春天 2002年，首次推吹spring框架雏形：interface21 以interface21为基础，经过不断丰富，2004年3月24日，发布了Spring的1.0版 Spring Framework创始人，著名作者。 Rod在悉尼大学不仅获得了计算机学位，同时还获得了音乐学位。更令人吃惊的是在回到软件开发领域之前，他还获得了音乐学的博士学位。 Spring理念：使现有的技术更加容易使用，本身是一个大杂烩，整合了现在所有的技术框架 SSH：Struts2 + Spring + Hibernate SSM：SpringMVC + Spring + Mybatis 官网：https://spring.io/projects/spring-framework#overview 官方下载地址：https://repo.spring.io/release/org/springframework/spring/ GitHub：https://github.com/spring-projects/spring-framework 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.2.11.RELEASE&lt;/version&gt;&lt;/dependency&gt; 1.2、优点 Spring是一个开源的免费的框架 Spring是一个轻量级的、非入侵式的框架 控制反转（IOC）、面向切面编程（AOP） 支持事务处理，对框架整合的支持 总结：Spring是一个轻量级的控制反转和面向切面编程的框架！ 1.3、组成 组成 Spring 框架的每个模块（或组件）都可以单独存在，或者与其他一个或多个模块联合实现。每个模块的功能如下： 核心容器：核心容器提供 Spring 框架的基本功能。核心容器的主要组件是 BeanFactory，它是工厂模式的实现。BeanFactory 使用控制反转（IOC） 模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。 Spring 上下文：Spring 上下文是一个配置文件，向 Spring 框架提供上下文信息。Spring 上下文包括企业服务，例如 JNDI、EJB、电子邮件、国际化、校验和调度功能。 Spring AOP：通过配置管理特性，Spring AOP 模块直接将面向切面的编程功能 , 集成到了 Spring 框架中。所以，可以很容易地使 Spring 框架管理任何支持 AOP的对象。Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖组件，就可以将声明性事务管理集成到应用程序中。 Spring DAO：JDBC DAO 抽象层提供了有意义的异常层次结构，可用该结构来管理异常处理和不同数据库供应商抛出的错误消息。异常层次结构简化了错误处理，并且极大地降低了需要编写的异常代码数量（例如打开和关闭连接）。Spring DAO 的面向 JDBC 的异常遵从通用的 DAO 异常层次结构。 Spring ORM：Spring 框架插入了若干个 ORM 框架，从而提供了 ORM 的对象关系工具，其中包括 JDO、Hibernate 和 iBatis SQL Map。所有这些都遵从 Spring 的通用事务和 DAO 异常层次结构。 Spring Web 模块：Web 上下文模块建立在应用程序上下文模块之上，为基于 Web 的应用程序提供了上下文。所以，Spring 框架支持与 Jakarta Struts 的集成。Web 模块还简化了处理多部分请求以及将请求参数绑定到域对象的工作。 Spring MVC 框架：MVC 框架是一个全功能的构建 Web 应用程序的 MVC 实现。通过策略接口，MVC 框架变成为高度可配置的，MVC 容纳了大量视图技术，其中包括 JSP、Velocity、Tiles、iText 和 POI。 1.4、拓展Spring官网中这样介绍：现代化的Java开发，说白了就是基于Spring的开发 SpringBoot： 一个快速开发的脚手架 基于SpringBoot可以快速开发单个微服务 约定大于配置！ SpringCloud SpringCloud是基于SpringBoot实现的 学习SprinhBoot的前提是要完全掌握Spring和SpringMVC 弊端：Spring发展久了之后违背了原来的使用简单的理念！配置十分繁琐，知道SpringBoot出来后才得以解放。 2、IOC理论指导之前的业务我们采用的方法 1.UserDao接口 2.UserDao实现类 3.UserService接口 4.UserService实现类 用户的需求可能会影响我们原来的代码，我们会根据用户需要去修改源代码！如果代码量十分大，那修改的成本会十分大。 我们是用一个set接口：已经发生了革命性变化 1234private UserDao userDao;public void setUser(UserDao userDao)&#123; this.userDao = userDao;&#125; 之前，程序是主动创建对象！控制权在程序员手上 现在，使用了set注入后，程序不再有主动性，而是变成了被动接受的对象！ 这种思想，从本质上解决了问题，程序员再也不用去管理对象的创建了，系统耦合性大大降低，可以专注在业务实现上。这就是IOC原型！ IOC本质 控制反转IoC(Inversion of Control)，是一种设计思想，DI(依赖注入)是实现IoC的一种方法，也有人认为DI只是IoC的另一种说法。没有IoC的程序中 , 我们使用面向对象编程 , 对象的创建与对象间的依赖关系完全硬编码在程序中，对象的创建由程序自己控制，控制反转后将对象的创建转移给第三方，个人认为所谓控制反转就是：获得依赖对象的方式反转了。 IoC是Spring框架的核心内容，使用多种方式完美的实现了IoC，可以使用XML配置，也可以使用注解，新版本的Spring也可以零配置实现IoC。 采用XML方式配置Bean的时候，Bean的定义信息是和实现分离的，而采用注解的方式可以把两者合为一体，Bean的定义信息直接以注解的形式定义在实现类中，从而达到了零配置的目的。 控制反转是一种通过描述（XML或注解）并通过第三方去生产或获取特定对象的方式。在Spring中实现控制反转的是IoC容器，其实现方法是依赖注入（Dependency Injection,DI）。 3、HelloSpringpojo对象： 123456789101112131415161718public class Hello &#123; private String str; public String getStr() &#123; return str; &#125; public void setStr(String str) &#123; this.str = str; &#125; @Override public String toString() &#123; return &quot;Hello&#123;&quot; + &quot;str=&#x27;&quot; + str + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; spring的xml配置： 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt;&lt;!-- 使用Spring创建对象，在 Spring中这些对象叫做bean 类型 变量名 = new 类型(); Hello hello = new Hello(); bean = 对象 new Hello(); id = 变量名 class = new 的对象 property = 对象中的属性设置值 --&gt; &lt;bean id=&quot;hello&quot; class=&quot;cn.zero.pojo.Hello&quot;&gt; &lt;property name=&quot;str&quot; value=&quot;helloSpring&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 测试用例： 123456789public class MyTest &#123; public static void main(String[] args) &#123; //获取Spring上下文对象 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); //我们的对象都在Spring中管理，要使用，直接取出来就行！ Hello hello = (Hello) context.getBean(&quot;hello&quot;); System.out.println(hello.toString()); &#125;&#125; 这就叫控制反转 控制：谁来控制对象创建，传统程序的对象是由程序本身创建的，使用Spring之后，由Spring来创建。 反转：程序本身不创建对象，而是被动接受对象。 依赖注入：就是利用set方法进行注入，没有set方法spring基本上跑不起来 IOC就是一种编程思想，由主动编程变成被动的接受。 可以通过new ClassPathXmlApplicationContext查看跟踪源码 到了现在为止，我们彻底不需要在程序中修改代码，要实现不同的操作，只需要在xml进行配置，所谓的IOC，就是一句话：对象由Spring来创建、管理、装配。 4、IOC创建对象方式 使用无参构造方法创建对象、默认！ 假设使用有参构造方法。 下标赋值 1234&lt;!--有参构造第一种方式：下标赋值--&gt; &lt;bean id=&quot;user&quot; class=&quot;cn.zero.pojo.User&quot;&gt; &lt;constructor-arg index=&quot;0&quot; value=&quot;whx&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; 参数类型 1234&lt;!--有参构造第二种方式：参数类型--&gt; &lt;bean id=&quot;user&quot; class=&quot;cn.zero.pojo.User&quot;&gt; &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;maizhu&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; 直接通过参数名来设置 1234&lt;!--有参构造第三种方式：参数名来设置--&gt; &lt;bean id=&quot;user&quot; class=&quot;cn.zero.pojo.User&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;maizhu&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; 总结：在spring配置文件加载完的时候，容器中管理的对象已经被初始化了。 5、Spring配置说明5.1、别名12&lt;!--起别名--&gt; &lt;alias name=&quot;user&quot; alias=&quot;user2&quot;/&gt; 5.2、Bean的配置12345678&lt;!-- id：bean的唯一标识符，也就是相当于对象名，引用名 class：bean 对象所对应的全限定名： 包名 + 类型 name：起别名 ，可以有多个值 --&gt; &lt;bean id=&quot;userT&quot; class=&quot;cn.zero.pojo.UserT&quot; name=&quot;u1 u2,u3;u4&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;撒算法&quot;/&gt; &lt;/bean&gt; 5.3、import这个多用于团队合作，他可以将多个配置文件导入合并为一个 applicationContext.xml： 123&lt;import resource=&quot;beans1.xml&quot;/&gt;&lt;import resource=&quot;beans2.xml&quot;/&gt;&lt;import resource=&quot;beans3.xml&quot;/&gt; 6、DI依赖注入6.1、构造器注入前面已经说过了 6.2、set方式注入【重点】 依赖注入：set注入！ 依赖：bean对象的创建依赖于容器 注入：bean对象中的所有属性，由容器来注入！ 【环境测试】 复杂对象 1234567891011public class Address &#123; private String address; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125;&#125; 真实测试对象 12345678910public class Student &#123; private String name; private Address address; private String[] books; private List&lt;String&gt; hobbys; private Map&lt;String,String&gt; card; private Set&lt;String&gt; games; private Properties info; private String wife;&#125; beans.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;bean id=&quot;student&quot; class=&quot;cn.zero.pojo.Student&quot;&gt; &lt;!--第一种：普通值注入，value--&gt; &lt;property name=&quot;name&quot; value=&quot;王鸿翔&quot;/&gt; &lt;!--第二种：bean注入,ref--&gt; &lt;property name=&quot;address&quot; ref=&quot;address&quot;/&gt; &lt;!--数组--&gt; &lt;property name=&quot;books&quot;&gt; &lt;array&gt; &lt;value&gt;基督山伯爵&lt;/value&gt; &lt;value&gt;肖申克的救赎&lt;/value&gt; &lt;value&gt;卡拉马佐夫&lt;/value&gt; &lt;value&gt;围城&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;!--list--&gt; &lt;property name=&quot;hobbys&quot;&gt; &lt;list&gt; &lt;value&gt;看书&lt;/value&gt; &lt;value&gt;吉他&lt;/value&gt; &lt;value&gt;钢琴&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!--map--&gt; &lt;property name=&quot;card&quot;&gt; &lt;map&gt; &lt;entry key=&quot;shenfenzheng&quot; value=&quot;1252463574745&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;!--set--&gt; &lt;property name=&quot;games&quot;&gt; &lt;set&gt; &lt;value&gt;lol&lt;/value&gt; &lt;value&gt;dnf&lt;/value&gt; &lt;value&gt;abc&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;!--null--&gt; &lt;property name=&quot;wife&quot;&gt; &lt;null/&gt; &lt;/property&gt; &lt;!--Properties--&gt; &lt;property name=&quot;info&quot;&gt; &lt;props&gt; &lt;prop key=&quot;学号&quot;&gt;2018000393&lt;/prop&gt; &lt;prop key=&quot;姓名&quot;&gt;王鸿翔&lt;/prop&gt; &lt;prop key=&quot;性别&quot;&gt;男&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; 6.3、拓展方式注入我们可以使用p命名空间和c命名空间进行注入： 使用： 1234567891011&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:c=&quot;http://www.springframework.org/schema/c&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;user&quot; class=&quot;cn.zero.pojo.User&quot; p:name=&quot;whx&quot; p:age=&quot;12&quot;/&gt; &lt;bean id=&quot;user&quot; class=&quot;cn.zero.pojo.User&quot; c:name=&quot;xhw&quot; c:age=&quot;99&quot;/&gt;&lt;/beans&gt; 测试： 12345678public class MyTest2 &#123; @Test public void test01()&#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;userBeans.xml&quot;); User user = context.getBean(&quot;user&quot;, User.class); System.out.println(user.toString()); &#125;&#125; 注意点：p命名和c命名不能直接使用，需要导入约束 12xmlns:p=&quot;http://www.springframework.org/schema/p&quot;xmlns:c=&quot;http://www.springframework.org/schema/c&quot; 6.4、Bean的作用域 单例模式（Spring默认机制） 1234&lt;bean id=&quot;accountService&quot; class=&quot;com.something.DefaultAccountService&quot;/&gt;&lt;!-- the following is equivalent, though redundant (singleton scope is the default) --&gt;&lt;bean id=&quot;accountService&quot; class=&quot;com.something.DefaultAccountService&quot; scope=&quot;singleton&quot;/&gt; 原型模式：每次从容器中get时都会产生新对象！ 1&lt;bean id=&quot;accountService&quot; class=&quot;com.something.DefaultAccountService&quot; scope=&quot;prototype&quot;/&gt; 其余的request、session、application这些只在web开发中使用！ 7、Bean的自动装配 自动装配是Spring满足bean依赖的一种方式 Spring会在上下文中自动寻找，并自动给bean装配属性！ 在Spring中有三种装配方式 在xml中显示配置 在java中显示配置 隐式的自动装配bean【重要！】 7.1、测试1、环境搭建：一个人两个宠物！三个对象 7.2、ByName自动装配12345678&lt;bean id=&quot;cat&quot; class=&quot;cn.zero.pojo.Cat&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;dog&quot; class=&quot;cn.zero.pojo.Dog&quot;&gt;&lt;/bean&gt;&lt;!-- byName：会自动在上下文中查找，和自己对象set方法后面的值对应的beanId！--&gt;&lt;bean id=&quot;people&quot; class=&quot;cn.zero.pojo.People&quot; autowire=&quot;byName&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;五六七&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 7.3、ByType自动装配12345678&lt;bean id=&quot;cat&quot; class=&quot;cn.zero.pojo.Cat&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;dog12314&quot; class=&quot;cn.zero.pojo.Dog&quot;&gt;&lt;/bean&gt;&lt;!-- byType：会自动在上下文中查找，和自己对象的属性类型相对应的bean！属性类型必须唯一，id可以省略或写错，和class对应--&gt;&lt;bean id=&quot;people&quot; class=&quot;cn.zero.pojo.People&quot; autowire=&quot;byType&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;五六七&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 小结： byName需要保证所有bean的Id唯一，并且这个bean的id值需要和自动注入的属性的set方法名后的值一致！ byType需要保证所有bean的class唯一，并且这个bean的类型要和自动注入的属性类型一致！ 7.4、使用注解自动装配jdk1.5就支持注解的使用，Spring2.5支持注解的使用 Spring官网提问关于xml和注解配置哪个更好： The introduction of annotation-based configuration raised the question of whether this approach is “better” than XML. The short answer is “it depends.” The long answer is that each approach has its pros and cons, and, usually, it is up to the developer to decide which strategy suits them better. Due to the way they are defined, annotations provide a lot of context in their declaration, leading to shorter and more concise configuration. However, XML excels at wiring up components without touching their source code or recompiling them. Some developers prefer having the wiring close to the source while others argue that annotated classes are no longer POJOs and, furthermore, that the configuration becomes decentralized and harder to control. 基于注解的配置的引入提出了一个问题，即这种方法是否比XML“更好”。简短的答案是“取决于情况”。长远的答案是每种方法都有其优缺点，通常，由开发人员决定哪种策略更适合他们。由于定义方式的不同，注解在声明中提供了很多上下文，从而使配置更短，更简洁。但是，XML擅长连接组件而不接触其源代码或重新编译它们。一些开发人员更喜欢将布线放置在靠近源的位置，而另一些开发人员则认为带注解的类不再是POJO，而且，配置变得分散并且难以控制。 使用注解须知： 引入约束 配置注解支持 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:annotation-config/&gt;&lt;/beans&gt; @Autowired 直接在属性上使用即可！也可以在set方法上使用 使用Autowired不用编写set方法了，前提是你这个自动装配的属性在IOC（Spring）容器中存在，且符合名字byname！ 科普： 1@Nullable 字段标记了这个注解，说明这个注解可以为Null 123public @interface Autowired &#123; boolean required() default true;&#125; 12345678public class People &#123; //如果定义注解required的值为false，那么该属性可以为Null，和@Nullable作用相同 @Autowired(required = false) private Dog dog; @Autowired private Cat cat; private String name;&#125; 如果@Autowired注解使用环境比较复杂，一个注解不能完成自动装配，可以使用@Qualifier（value = “xxx”）去配合@Autowired使用，指定一个唯一的bean对象注入！ 123456&lt;bean id=&quot;cat123&quot; class=&quot;cn.zero.pojo.Cat&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;cat23&quot; class=&quot;cn.zero.pojo.Cat&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;dog333&quot; class=&quot;cn.zero.pojo.Dog&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;dog123&quot; class=&quot;cn.zero.pojo.Dog&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;people&quot; class=&quot;cn.zero.pojo.People&quot;/&gt; &lt;context:annotation-config&gt;&lt;/context:annotation-config&gt; 123456789public class People &#123; @Autowired @Qualifier(value = &quot;dog123&quot;) private Dog dog; @Autowired @Qualifier(value = &quot;cat123&quot;) private Cat cat;&#125; @Resource 1234567public class People &#123; @Resource private Dog dog; @Resource private Cat cat;&#125; 小结： ​ @Resource和@Autowired的异同点： 相同点： 都是用来自动装配的，都可以放在属性字段上 @Autowired是通过byType方式实现，再通过byName实现 @Resource是通过byName方式实现，如果找不到，再通过byType方式实现！如果两个都找不到就报错！ 不同点： 执行顺序不同，@Autowired先通过byType，@Resource先通过byName 8、使用注解开发在Spirng4之后，在使用注解开发必须要导入aop的包 1.引入约束，导入注解支持，注解扫描 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:annotation-config/&gt; &lt;!--指定要扫描的包，这个包下的注解会生效--&gt; &lt;context:component-scan base-package=&quot;cn.zero.pojo&quot;/&gt;&lt;/beans&gt; 2.@Component注解 12345@Component//相当于&lt;bean id=&quot;user&quot; class=&quot;cn.zero.pojo.User&quot;&gt;public class User &#123; @Value(&quot;哈哈哈&quot;)//相当于&lt;property name=&quot;name&quot; value=&quot;哈哈哈&quot;/&gt; public String name;&#125; 3.衍生的注解： dao【@Repository】 service【@Service】 controller【@Controller】 上述四个注解功能相同，都代表将某个类放入容器中，就是装配Bean 4.自动装配： 1234- @Autowired：自动装配通过类型，名字。 如果@autowired不能唯一自动装配上该属性，则需要通过@Qualifier（value &#x3D; &quot;xxx&quot;）去配合使用。- @Resource：自动装配通过名字，类型。- @Nullable：字段标记了这个注解，说明这个注解可以为Null 5.作用域： 123456@Component@Scope(&quot;singleton&quot;)public class User &#123; @Value(&quot;哈哈哈&quot;) public String name;&#125; 6.小结： xml与注解： xml更加万能，适用于任何场合！维护简单方便！ 注解 不是自己的类使用不了，维护相对复杂！ 最佳实践： xml用来管理Bean 注解只负责属性注入 9、使用Java 的方式配置Spring我们现在要完全不使用Spring的配置，全权交给Java JavaConfig是Spring的一个子项目，在Spring4之后 它变成一个核心功能！ 配置类–相当于xml配置： 12345678910111213//这个也会被Spring容器托管，因为它本身就是一个@Component，@configuration代表这是一个配置类，就和之前的beans.xml作用相同@Configuration@ComponentScan(&quot;cn.zero.pojo&quot;)@Import(ZeroConfig2.class)public class ZeroConfig &#123; //注册一个bean，相当于一个bean标签 //方法名就是id属性的值 //方法返回值对应的对象就是class属性的值 @Bean public User getUser()&#123; return new User(); &#125;&#125; 测试： 12345678public class MyTest &#123; @Test public void test01()&#123; ApplicationContext context = new AnnotationConfigApplicationContext(ZeroConfig.class); User user = context.getBean(&quot;getUser&quot;, User.class); System.out.println(user.name); &#125;&#125; pojo： 12345@Componentpublic class User &#123; @Value(&quot;哈哈哈&quot;) public String name;&#125; 10、代理模式为什么学习代理模式？因为这是SpirngAOP的底层！ 代理模式分类： 静态代理 动态代理 10.1、静态代理角色分析： 抽象角色：一般会使用接口或抽象类 真实角色：被代理的角色 代理角色：代理真实角色，代理真实角色后，我们一般会做一些附属操作 客户：访问代理对象的人 静态代理好处： 可以使真实角色操作更加纯粹，不用去关心一些业务逻辑 公共交给代理角色，实现业务分工 公共业务发生扩展的时候，方便集中管理 缺点：每一个真实角色都需要一个代理对象，开发复杂。 1.抽象角色 123public interface Rent &#123; public void rent();&#125; 2.真实角色 123456public class Host implements Rent&#123; @Override public void rent() &#123; System.out.println(&quot;房东要出租房子&quot;); &#125;&#125; 3.代理角色 123456789101112131415161718192021222324public class Proxy implements Rent&#123; private Host host; public Proxy()&#123; &#125; public Proxy(Host host) &#123; this.host = host; &#125; @Override public void rent() &#123; seeHouse(); host.rent(); sign(); &#125; public void seeHouse()&#123; System.out.println(&quot;sssss&quot;); &#125; public void sign()&#123; System.out.println(&quot;aaaaa&quot;); &#125;&#125; 4.客户端代理角色 1234567public class Client &#123; public static void main(String[] args) &#123; Host host = new Host(); Proxy proxy = new Proxy(host); proxy.rent(); &#125;&#125; AOP的实现机制！–通过代理 10.2、动态代理 动态代理和静态代理角色一样 动态代理的代理类是动态生成的，不是我们直接写好的 动态代理分为两大类：基于接口的动态代理，基于类的动态代理。 基于接口–jdk的动态代理【我们在这里使用】 基于类：cglib java字节码：javassist 需要了解两个类：Proxy：代理，invocationHandler：调用处理 动态代理好处： 可以使真实角色操作更加纯粹，不用去关心一些业务逻辑 公共交给代理角色，实现业务分工 公共业务发生扩展的时候，方便集中管理 一个动态代理类代理的是一个接口，一般就是对应的一类业务 一个动态代理类可以代理多个类，只是要实现统一接口即可 11、AOP方式一：使用Spring的API接口xml配置 12345678910111213&lt;!--注册bean--&gt; &lt;bean id=&quot;userService&quot; class=&quot;cn.zero.service.UserServiceImpl&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;log&quot; class=&quot;cn.zero.log.Log&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;afterLog&quot; class=&quot;cn.zero.log.AfterLog&quot;&gt;&lt;/bean&gt; &lt;!--方式一、原生Spring API接口--&gt; &lt;!--配置aop 需要导入aop约束--&gt; &lt;aop:config&gt; &lt;!--切入点 execution要执行的位置--&gt; &lt;aop:pointcut id=&quot;pointcut&quot; expression=&quot;execution(* cn.zero.service.UserServiceImpl.*(..))&quot;/&gt; &lt;!--执行环绕增强--&gt; &lt;aop:advisor advice-ref=&quot;log&quot; pointcut-ref=&quot;pointcut&quot;/&gt; &lt;aop:advisor advice-ref=&quot;afterLog&quot; pointcut-ref=&quot;pointcut&quot;/&gt; &lt;/aop:config&gt; 执行前API接口 123456public class Log implements MethodBeforeAdvice &#123; @Override public void before(Method method, Object[] objects, Object o) throws Throwable &#123; System.out.println(&quot;1111&quot;+ method.getName()); &#125;&#125; 执行后API接口 123456public class AfterLog implements AfterReturningAdvice &#123; @Override public void afterReturning(Object o, Method method, Object[] objects, Object o1) throws Throwable &#123; System.out.println(&quot;22222&quot;+ method.getName() + o); &#125;&#125; 方式二：自定义实现xml配置 12345678910111213&lt;!--注册bean--&gt; &lt;bean id=&quot;userService&quot; class=&quot;cn.zero.service.UserServiceImpl&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;log&quot; class=&quot;cn.zero.log.Log&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;afterLog&quot; class=&quot;cn.zero.log.AfterLog&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;diy&quot; class=&quot;cn.zero.diy.DiyLog&quot;/&gt;&lt;!--方式二：自定义实现--&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;pointcut&quot; expression=&quot;execution(* cn.zero.service.UserServiceImpl.*(..))&quot;/&gt; &lt;aop:aspect ref=&quot;diy&quot;&gt; &lt;aop:before method=&quot;before&quot; pointcut-ref=&quot;pointcut&quot;/&gt; &lt;aop:after method=&quot;after&quot; pointcut-ref=&quot;pointcut&quot;/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt; 自定义方法 12345678public class DiyLog &#123; public void before()&#123; System.out.println(&quot;====执行方法前=====&quot;); &#125; public void after()&#123; System.out.println(&quot;=====执行方法后=====&quot;); &#125;&#125; 方式三：注解实现xml配置 12345678&lt;!--注册bean--&gt; &lt;bean id=&quot;userService&quot; class=&quot;cn.zero.service.UserServiceImpl&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;log&quot; class=&quot;cn.zero.log.Log&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;afterLog&quot; class=&quot;cn.zero.log.AfterLog&quot;&gt;&lt;/bean&gt;&lt;!--方式三：注解实现--&gt; &lt;bean id=&quot;anno&quot; class=&quot;cn.zero.anno.Annotation&quot;/&gt; &lt;!--代理模式实现：JDK实现（proxy-target-class=&quot;false&quot;）cglib：（proxy-target-class=&quot;true&quot;）--&gt; &lt;aop:aspectj-autoproxy proxy-target-class=&quot;false&quot;/&gt; 带注解的实现类 12345678910111213141516171819@Aspectpublic class Annotation &#123; @Before(&quot;execution(* cn.zero.service.UserServiceImpl.*(..))&quot;) public void before()&#123; System.out.println(&quot;执行方法前&quot;); &#125; @After(&quot;execution(* cn.zero.service.UserServiceImpl.*(..))&quot;) public void after()&#123; System.out.println(&quot;执行方法后&quot;); &#125; @Around(&quot;execution(* cn.zero.service.UserServiceImpl.*(..))&quot;) public void around(ProceedingJoinPoint joinPoint) throws Throwable &#123; System.out.println(&quot;环绕前&quot;); joinPoint.proceed(); Signature signature = joinPoint.getSignature(); System.out.println(signature); System.out.println(&quot;环绕前&quot;); &#125;&#125; 12、整合MybatisMybatis-Spring 13、声明式事务spring中的事务管理 声明式事务：AOP实现 编程式事务：需要在代码中进行事务管理","categories":[],"tags":[]},{"title":"前端组件化框架Vue学习","slug":"前端组件化框架Vue学习","date":"2021-01-20T09:32:46.000Z","updated":"2021-01-20T09:32:46.210Z","comments":true,"path":"2021/01/20/前端组件化框架Vue学习/","link":"","permalink":"http://example.com/2021/01/20/%E5%89%8D%E7%AB%AF%E7%BB%84%E4%BB%B6%E5%8C%96%E6%A1%86%E6%9E%B6Vue%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"前端三大组件之JavaScript学习","slug":"前端三大组件之JavaScript学习","date":"2021-01-20T09:32:15.000Z","updated":"2021-01-20T09:32:15.220Z","comments":true,"path":"2021/01/20/前端三大组件之JavaScript学习/","link":"","permalink":"http://example.com/2021/01/20/%E5%89%8D%E7%AB%AF%E4%B8%89%E5%A4%A7%E7%BB%84%E4%BB%B6%E4%B9%8BJavaScript%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"前端三大组件之Html5+Css3学习","slug":"前端三大组件之Html5-Css3学习","date":"2021-01-20T09:31:48.000Z","updated":"2021-01-20T09:31:49.007Z","comments":true,"path":"2021/01/20/前端三大组件之Html5-Css3学习/","link":"","permalink":"http://example.com/2021/01/20/%E5%89%8D%E7%AB%AF%E4%B8%89%E5%A4%A7%E7%BB%84%E4%BB%B6%E4%B9%8BHtml5-Css3%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Oauth2开发标准学习","slug":"Oauth2开发标准学习","date":"2021-01-11T09:01:04.000Z","updated":"2021-01-11T11:43:14.053Z","comments":true,"path":"2021/01/11/Oauth2开发标准学习/","link":"","permalink":"http://example.com/2021/01/11/Oauth2%E5%BC%80%E5%8F%91%E6%A0%87%E5%87%86%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"Oauth2 解决方案之所以说 Oauth2 是一种解决方案，一个开放标准，而不是一种协议，是因为他是解决第三方访问服务时，不共享其他数据，只拿到应该拿的信息。解决办法是：令牌机制:生成 token 字符串，根据 token 判断是否能访问某些服务，是否具有某些权限。运行流程图:上图涉及四个主体和六个过程:过程:（A）用户打开客户端以后，客户端要求用户给予授权。（B）用户同意给予客户端授权。（C）客户端使用上一步获得的授权，向认证服务器申请令牌。（D）认证服务器对客户端进行认证以后，确认无误，同意发放令牌。（E）客户端使用令牌，向资源服务器申请获取资源。（F）资源服务器确认令牌无误，同意向客户端开放资源。主体:1.客户端(浏览器)2.资源拥有者(用户自己)3.授权服务器(认证服务器)4.资源服务器 授权服务配置向授权服务器进行认证，并返回 token EnableAuthorizationServer 在Confifig包下创建AuthorizationServer 12345@Configuration @EnableAuthorizationServer public class AuthorizationServer extends AuthorizationServerConfigurerAdapter &#123; //略... &#125; 可以用 @EnableAuthorizationServer 注解并继承AuthorizationServerConfifigurerAdapter来配置OAuth2.0 授权服务器。 AuthorizationServerConfifigurerAdapter要求配置以下几个类，这几个类是由Spring创建的独立的配置对象，它们 会被Spring传入AuthorizationServerConfifigurer中进行配置。 123456public class AuthorizationServerConfigurerAdapter implements AuthorizationServerConfigurer &#123; public AuthorizationServerConfigurerAdapter() &#123;&#125; public void configure(AuthorizationServerSecurityConfigurer security) throws Exception &#123;&#125; public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123;&#125; public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123;&#125; &#125; ClientDetailsServiceConfifigurer：用来配置客户端详情服务（ClientDetailsService），客户端详情信息在 这里进行初始化，你能够把客户端详情信息写死在这里或者是通过数据库来存储调取详情信息。 AuthorizationServerEndpointsConfifigurer：用来配置令牌（token）的访问端点和令牌服务(token services)。 AuthorizationServerSecurityConfifigurer：用来配置令牌端点的安全约束。 1.配置客户端详细信息123456789101112@Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; // clients.withClientDetails(clientDetailsService); clients.inMemory()// 使用in‐memory存储 .withClient(&quot;c1&quot;)// client_id .secret(new BCryptPasswordEncoder() .encode(&quot;secret&quot;)) .resourceIds(&quot;res1&quot;) .authorizedGrantTypes(&quot;authorization_code&quot;, &quot;password&quot;,&quot;client_credentials&quot;,&quot;implicit&quot;,&quot;refresh_token&quot;)// 该client允许的授权类型 authorization_code,password,refresh_token,implicit,client_credentials .scopes(&quot;all&quot;)// 允许的授权范围 .autoApprove(false) //加上验证回调地址 .redirectUris(&quot;http://www.baidu.com&quot;); &#125; 2.管理令牌AuthorizationServerTokenServices 接口定义了一些操作使得你可以对令牌进行一些必要的管理，令牌可以被用来 加载身份信息，里面包含了这个令牌的相关权限。除了持久化令牌是委托一个 TokenStore 接口来实现以外，这个类几乎帮你做了 所有的事情。 1).定义tokenconfig 在confifig包下定义TokenConfifig 123456@Configuration public class TokenConfig &#123; @Bean public TokenStore tokenStore() &#123; return new InMemoryTokenStore(); &#125; &#125; 有三种实现： InMemoryTokenStore：被默认采用的，内存中持久化 JdbcTokenStore：基于JDBC的实现， JwtTokenStore：按照jwt生成token 2).定义AuthorizationServerTokenServices 在AuthorizationServer中定义AuthorizationServerTokenServices 12345678910111213@Autowired private TokenStore tokenStore; @Autowired private ClientDetailsService clientDetailsService; @Beanpublic AuthorizationServerTokenServices tokenService() &#123; DefaultTokenServices service=new DefaultTokenServices(); service.setClientDetailsService(clientDetailsService); service.setSupportRefreshToken(true); service.setTokenStore(tokenStore); service.setAccessTokenValiditySeconds(7200); // 令牌默认有效期2小时 service.setRefreshTokenValiditySeconds(259200); // 刷新令牌默认有效期3天 return service; &#125; 3.令牌访问端点AuthorizationServerEndpointsConfifigurer 这个对象的实例可以完成令牌服务以及令牌endpoint配置。 配置授权类型： authenticationManager userDetailsService authorizationCodeServices implicitGrantService tokenGranter 后面两个比较少用，第一个是密码授权时的认证管理器，第二个是查询数据库用到的，第三个是授权码模式 配置授权端点URL： 12345678910111213141516@Autowired private AuthorizationCodeServices authorizationCodeServices; @Autowired private AuthenticationManager authenticationManager; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) &#123; endpoints .authenticationManager(authenticationManager) .authorizationCodeServices(authorizationCodeServices) .tokenServices(tokenService()) .allowedTokenEndpointRequestMethods(HttpMethod.POST); &#125; @Bean public AuthorizationCodeServices authorizationCodeServices() &#123; //设置授权码模式的授权码如何 存取，暂时采用内存方式 return new InMemoryAuthorizationCodeServices(); &#125; 4.令牌端点的安全约束AuthorizationServerSecurityConfifigurer：用来配置令牌端点(Token Endpoint)的安全约束，在 AuthorizationServer中配置如下. 12345@Override public void configure(AuthorizationServerSecurityConfigurer security)&#123; security .tokenKeyAccess(&quot;permitAll()&quot;) .checkTokenAccess(&quot;permitAll()&quot;) .allowFormAuthenticationForClients(); &#125; 授权服务总结：第一步要完成认证，就要知道从哪个客户端传来的请求并认证，因此要进行客户端配置；第二步生成令牌，要配置token相关url，token存在哪里等；第三步对token的端点进行安全约束。","categories":[],"tags":[]},{"title":"Spring Security框架学习","slug":"Spring-Security框架学习","date":"2021-01-07T06:38:11.000Z","updated":"2021-01-10T03:06:41.623Z","comments":true,"path":"2021/01/07/Spring-Security框架学习/","link":"","permalink":"http://example.com/2021/01/07/Spring-Security%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"Spring Security框架学习一、基本概念Spring Security主要两大核心功能：“认证”和“授权” 认证：是指用户是否登录 授权：是判断用户是否有权限进行某操作 Spring Security本质是过滤链，过滤器有很多，主要的三个：方法级，错误，登录 FilterSecurityInterceptor：方法级权限过滤器，位于过滤链最底部 ExceptionTranslationFilter：是个异常过滤器，用来处理在认证授权过程中抛出的异常 UsernamePasswordAuthenticationFilter ：对/login 的 POST 请求做拦截，校验表单中用户名，密码。 过滤器加载过程 两个重要接口： UserDetailService 查询数据库用户密码的过程 PasswodeEncoder 密码加密 二、web权限方案1.设置登录用户名密码1.通过配置文件 123server.port=9111spring.security.user.name=maizhudeaimuspring.security.user.password=123456 2.通过配置类 12345678910111213@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception&#123; BCryptPasswordEncoder bCryptPasswordEncoder = new BCryptPasswordEncoder(); String encode = bCryptPasswordEncoder.encode(&quot;123&quot;); auth.inMemoryAuthentication().withUser(&quot;aimu&quot;).password(encode).roles(&quot;admin&quot;); &#125; @Bean PasswordEncoder password()&#123; return new BCryptPasswordEncoder(); &#125;&#125; 报错：There is no PasswordEncoder mapped for the id “null”，原因是没有把BCryptPasswordEncoder的bean注入容器与之绑定 1234@Bean PasswordEncoder password()&#123; return new BCryptPasswordEncoder(); &#125; 3.自定义编写实体类 上述两种方式不常用，我们实现业务时主要通过数据库查询用户名密码，所以第三种比较重要。 ​ 1.创建配置类，决定使用哪一个userDetailService实现类 12345678910111213@Configurationpublic class SecurityConfigTest extends WebSecurityConfigurerAdapter &#123; @Autowired private UserDetailsService userDetailsService; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception&#123; auth.userDetailsService(userDetailsService).passwordEncoder(password()); &#125; @Bean PasswordEncoder password()&#123; return new BCryptPasswordEncoder(); &#125;&#125; ​ 2.编写实现类，返回User对象，里面含有username和password 12345678910111213141516171819@Service(&quot;userDetailsService&quot;)//这里写上名字，配置类注入才不会报红错误public class MyUserDetailService implements UserDetailsService &#123; @Autowired private UserMapper userMapper; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; //调用UserMapper方法查询数据库 QueryWrapper&lt;cn.aimu.securitydemo1.entity.User&gt; wrapper = new QueryWrapper&lt;&gt;(); wrapper.eq(&quot;username&quot;,username); cn.aimu.securitydemo1.entity.User user = userMapper.selectOne(wrapper); if(user == null)&#123; throw new UsernameNotFoundException(&quot;用户名不存在！！&quot;); &#125; List&lt;GrantedAuthority&gt; auths = AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;role&quot;); return new User(user.getUsername(),new BCryptPasswordEncoder().encode(user.getPassword()),auths); &#125;&#125; 2.自定义登录页面 定义一个login.html页面 12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Spring Security登录&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action=&quot;/user/login&quot; method=&quot;post&quot;&gt; 用户名：&lt;input type=&quot;text&quot; name=&quot;username&quot;/&gt; &lt;br/&gt; 密码：&lt;input type=&quot;text&quot; name=&quot;password&quot;/&gt; &lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;login&quot;/&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 要实现自定义登录页面需要重写另一个configure方法 12345678910111213141516171819202122232425@Configurationpublic class SecurityConfigTest extends WebSecurityConfigurerAdapter &#123; @Autowired private UserDetailsService userDetailsService; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception&#123; auth.userDetailsService(userDetailsService).passwordEncoder(password()); &#125; @Bean PasswordEncoder password()&#123; return new BCryptPasswordEncoder(); &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.formLogin()//自定义自己编写的登录页面 .loginPage(&quot;/login.html&quot;)//登录页面设置 .loginProcessingUrl(&quot;/user/login&quot;)//登录页面访问路径 .defaultSuccessUrl(&quot;/test/index&quot;).permitAll()//登录成功跳转页面 .and().authorizeRequests() .antMatchers(&quot;/&quot;,&quot;/test/hello&quot;,&quot;/user/login&quot;).permitAll()//设置哪些路径可以直接访问，不需要认证 .anyRequest().authenticated() .and().csrf().disable();//关闭csrf防护 &#125;&#125; 3.基于角色或权限进行访问控制 hasAuthority 方法 具有某一个权限 12345//1.配置类设置访问路径具有的权限antMatchers(&quot;/test/index&quot;).hasAuthority(&quot;admins&quot;)//当前用户登录，只有具有admins权限才能访问这个路径//2.在UserDetailService中设置User对象权限List&lt;GrantedAuthority&gt; auths = AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;role&quot;);return new User(user.getUsername(),new BCryptPasswordEncoder().encode(user.getPassword()),auths); 报错There was an unexpected error (type=Forbidden, status=403). 123//改成一样的权限就可以了List&lt;GrantedAuthority&gt; auths = AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;admins&quot;);return new User(user.getUsername(),new BCryptPasswordEncoder().encode(user.getPassword()),auths); hasAnyAuthority 方法 具有多个权限 123456//1.配置类设置访问路径具有的权限.antMatchers(&quot;/test/index&quot;).hasAnyAuthority(&quot;admins,managers&quot;)//多个权限都可以访问时//2.在UserDetailService中设置User对象权限List&lt;GrantedAuthority&gt; auths = AuthorityUtils .commaSeparatedStringToAuthorityList(&quot;admins&quot;);//有其中一个权限就能访问return new User(user.getUsername(),new BCryptPasswordEncoder().encode(user.getPassword()),auths); hasRole 方法 具有某一单一角色 12345//1.配置类设置访问路径具有的角色.antMatchers(&quot;/test/index&quot;).hasRole(&quot;sale&quot;)//经过源码后变成ROLE_sale//2.在UserDetailService中设置User对象角色List&lt;GrantedAuthority&gt; auths = AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;ROLE_sale&quot;);//加上前缀return new User(user.getUsername(),new BCryptPasswordEncoder().encode(user.getPassword()),auths); 需要注意的是跟Role有关的设置名字时不能以ROLE_开头，源码： 12345678private static String hasRole(String role) &#123; Assert.notNull(role, &quot;role cannot be null&quot;); if (role.startsWith(&quot;ROLE_&quot;)) &#123; throw new IllegalArgumentException(&quot;role should not start with &#x27;ROLE_&#x27; since it is automatically inserted. Got &#x27;&quot; + role + &quot;&#x27;&quot;); &#125; else &#123; return &quot;hasRole(&#x27;ROLE_&quot; + role + &quot;&#x27;)&quot;; &#125; &#125; 传进去的role值被改为ROLE_role值所有在service设置角色时要加上前缀 hasAnyRole 方法 具有多个角色中任意一种，配置方式和上面差不多，不在赘述。 4.自定义403页面1.配置类–直接在confige方法写上这样一行代码，表示设置403错误页面 1http.exceptionHandling().accessDeniedPage(&quot;/unauth.html&quot;); 2.再写一个html页面。。 3.然后测试 5.注解使用第一步：启动类（配置类）开启注解@EnableGlobalMethodSecurity(securedEnabled = true) 第二步：在controller的方法上添加下面的注解，设置角色或权限 第三步：在UserDetailService上设置用户所具有的角色或权限 1.@Secured 用户具有某个角色，可以访问方法 2.@PreAuthorize 在方法执行前检验，在里面写前面说到的四个方法 3.@PostAuthorize 在方法执行后检验 4.@PostFilter 在方法执行后过滤返回值，可以过滤掉不需要的返回值 5.@PreFilter 过滤参数中不需要的值 1234567891011@GetMapping(&quot;update&quot;) //@Secured(&#123;&quot;ROLE_sale&quot;,&quot;ROLE_admin&quot;&#125;)//哪些角色可以访问 @PreAuthorize(&quot;hasAnyAuthority(&#x27;admins&#x27;)&quot;)//还有hasAuthorize、hasRole、hasAnyRole @PostFilter(&quot;filterObject.username == &#x27;admin1&#x27;&quot;)//过滤掉返回值中username不是admin1的 public List&lt;User&gt; update() &#123; ArrayList&lt;User&gt; users = new ArrayList&lt;&gt;(); users.add(new User(3,&quot;admin1&quot;,&quot;123&quot;)); users.add(new User(4,&quot;24235&quot;,&quot;123&quot;)); System.out.println(users); return users; &#125; 6.自动登录自动登录其实是数据库中维护一个用户信息和加密串的表，cookies中存放加密串，当第一次登录时通过RememberMeService将加密串分别存入cookies和数据库中，第二次访问时，拿cookies的加密串去数据库查询，如果有匹配的并且相隔时间不超过设置的有效时长，那么就能实现自动登录。 原理图： 1.配置类，注入数据源，配置操作数据库对象 123456789 @Autowired private DataSource dataSource;@Bean public PersistentTokenRepository persistentTokenRepository()&#123; JdbcTokenRepositoryImpl jdbcTokenRepository = new JdbcTokenRepositoryImpl();//操作数据库的对象 jdbcTokenRepository.setDataSource(dataSource); //jdbcTokenRepository.setCreateTableOnStartup(true);//第一次时需要创建表，创建表的sql在源码中有 return jdbcTokenRepository; &#125; 2.配置类中配置自动登录 123.and().rememberMe().tokenRepository(persistentTokenRepository())//自动登录设置操作数据库的方法 .tokenValiditySeconds(60)//设置有效时长，单位秒 .userDetailsService(userDetailsService) 3.登录页面修改 1&lt;input type=&quot;checkbox&quot; name=&quot;remember-me&quot;/&gt;自动登录 name值一定是remember-me，框架中要求的 7.CSRF–跨站请求伪造 在登录页面添加一个隐藏域 123&lt;input type=&quot;hidden&quot;th:if=&quot;$&#123;_csrf&#125;!=null&quot;th:value=&quot;$&#123;_csrf.token&#125;&quot;name=&quot;_csrf&quot;/&gt; 关闭安全配置的类中的 csrf 1// http.csrf().disable(); CSRF防护步骤： 1.第一次请求生成csrfToken保存到session或者Cookie中 2.后面的请求到来时，从请求中提取csrfToken和保存的做比较，判断请求是否合法。主要通过CsrfFilter过滤器完成。 三、微服务权限方案1.根据案例后台用户角色菜单之间的关系构建数据库表：五张表：用户表、角色表、菜单表、用户角色多对多表、角色菜单多对多表。 2.创建微服务工程，导入依赖，导入一些工具类 3.编写security认证授权工具类和处理器 1).DefaultPasswordEncoder–密码加密工具类 12345678910111213141516171819@Componentpublic class DefaultPasswordEncoder implements PasswordEncoder &#123; public DefaultPasswordEncoder() &#123; this(-1); &#125; public DefaultPasswordEncoder(int strength) &#123; &#125; //进行MD5加密 @Override public String encode(CharSequence charSequence) &#123; return MD5.encrypt(charSequence.toString()); &#125; //进行密码比对 @Override public boolean matches(CharSequence charSequence, String encodedPassword) &#123; return encodedPassword.equals(MD5.encrypt(charSequence.toString())); &#125;&#125; 有两个方法，一个是密码加密，用在存入数据库时，密码不能用明文存储，所以加密后存储；一个是密码比对，验证登录用户密码是否正确时，使用这个方法。 2).TokenManager 123456789101112131415161718192021@Componentpublic class TokenManager &#123; //token有效时长 private long tokenEcpiration = 24*60*60*1000; //编码秘钥 private String tokenSignKey = &quot;123456&quot;; //1 使用jwt根据用户名生成token public String createToken(String username) &#123; String token = Jwts.builder().setSubject(username) .setExpiration(new Date(System.currentTimeMillis()+tokenEcpiration)) .signWith(SignatureAlgorithm.HS512, tokenSignKey).compressWith(CompressionCodecs.GZIP).compact(); return token; &#125; //2 根据token字符串得到用户信息 public String getUserInfoFromToken(String token) &#123; String userinfo = Jwts.parser().setSigningKey(tokenSignKey).parseClaimsJws(token).getBody().getSubject(); return userinfo; &#125; //3 删除token public void removeToken(String token) &#123; &#125;&#125; 三个方法：1.生成Token，根据jwt规则；2.从Token中获取信息，都是用JWT工具类操作；3.删除token（未实现） 3).TokenLogoutHandler 123456789101112131415161718192021222324//退出处理器public class TokenLogoutHandler implements LogoutHandler &#123; private TokenManager tokenManager; private RedisTemplate redisTemplate; public TokenLogoutHandler(TokenManager tokenManager,RedisTemplate redisTemplate) &#123; this.tokenManager = tokenManager; this.redisTemplate = redisTemplate; &#125; @Override public void logout(HttpServletRequest request, HttpServletResponse response, Authentication authentication) &#123; //1 从header里面获取token //2 token不为空，移除token，从redis删除token String token = request.getHeader(&quot;token&quot;); if(token != null) &#123; //移除 tokenManager.removeToken(token); //从token获取用户名 String username = tokenManager.getUserInfoFromToken(token); redisTemplate.delete(username); &#125; ResponseUtil.out(response, R.ok()); &#125;&#125; 业务逻辑是退出时将header中的token移除，并且把redis中的token删除 4).UnauthEntryPoint 123456public class UnauthEntryPoint implements AuthenticationEntryPoint &#123; @Override public void commence(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, AuthenticationException e) throws IOException, ServletException &#123; ResponseUtil.out(httpServletResponse, R.error()); &#125;&#125; 未授权就报异常 5).TokenLoginFilter 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class TokenLoginFilter extends UsernamePasswordAuthenticationFilter &#123; private TokenManager tokenManager; private RedisTemplate redisTemplate; private AuthenticationManager authenticationManager; public TokenLoginFilter(AuthenticationManager authenticationManager, TokenManager tokenManager, RedisTemplate redisTemplate) &#123; this.authenticationManager = authenticationManager; this.tokenManager = tokenManager; this.redisTemplate = redisTemplate; this.setPostOnly(false); this.setRequiresAuthenticationRequestMatcher(new AntPathRequestMatcher(&quot;/admin/acl/login&quot;,&quot;POST&quot;)); &#125; //1 获取表单提交用户名和密码 @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException &#123; //获取表单提交数据 try &#123; User user = new ObjectMapper().readValue(request.getInputStream(), User.class); return authenticationManager.authenticate(new UsernamePasswordAuthenticationToken(user.getUsername(),user.getPassword(), new ArrayList&lt;&gt;())); &#125; catch (IOException e) &#123; e.printStackTrace(); throw new RuntimeException(); &#125; &#125; //2 认证成功调用的方法 @Override protected void successfulAuthentication(HttpServletRequest request, HttpServletResponse response, FilterChain chain, Authentication authResult) throws IOException, ServletException &#123; //认证成功，得到认证成功之后用户信息 SecurityUser user = (SecurityUser)authResult.getPrincipal(); //根据用户名生成token String token = tokenManager.createToken(user.getCurrentUserInfo().getUsername()); //把用户名称和用户权限列表放到redis redisTemplate.opsForValue().set(user.getCurrentUserInfo().getUsername(),user.getPermissionValueList()); //返回token ResponseUtil.out(response, R.ok().data(&quot;token&quot;,token)); &#125; //3 认证失败调用的方法 protected void unsuccessfulAuthentication(HttpServletRequest request, HttpServletResponse response, AuthenticationException failed) throws IOException, ServletException &#123; ResponseUtil.out(response, R.error()); &#125;&#125; 认证过滤器：1.认证前获取表单数据；2.认证成功后生成token，存入redis，返回token到header中；3.失败就返回错误 6).TokenAuthFilter 12345678910111213141516171819202122232425262728293031323334353637383940public class TokenAuthFilter extends BasicAuthenticationFilter &#123; private TokenManager tokenManager; private RedisTemplate redisTemplate; public TokenAuthFilter(AuthenticationManager authenticationManager,TokenManager tokenManager,RedisTemplate redisTemplate) &#123; super(authenticationManager); this.tokenManager = tokenManager; this.redisTemplate = redisTemplate; &#125; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; //获取当前认证成功用户权限信息 UsernamePasswordAuthenticationToken authRequest = getAuthentication(request); //判断如果有权限信息，放到权限上下文中 if(authRequest != null) &#123; SecurityContextHolder.getContext().setAuthentication(authRequest); &#125; chain.doFilter(request,response); &#125; private UsernamePasswordAuthenticationToken getAuthentication(HttpServletRequest request) &#123; //从header获取token String token = request.getHeader(&quot;token&quot;); if(token != null) &#123; //从token获取用户名 String username = tokenManager.getUserInfoFromToken(token); //从redis获取对应权限列表 List&lt;String&gt; permissionValueList = (List&lt;String&gt;)redisTemplate.opsForValue().get(username); Collection&lt;GrantedAuthority&gt; authority = new ArrayList&lt;&gt;(); for(String permissionValue : permissionValueList) &#123; SimpleGrantedAuthority auth = new SimpleGrantedAuthority(permissionValue); authority.add(auth); &#125; return new UsernamePasswordAuthenticationToken(username,token,authority); &#125; return null; &#125;&#125; 从header中获取token，在从token中获取用户信息，到redis查询权限列表，返回 7).TokenWebSecurityConfig 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Configuration@EnableWebSecurity@EnableGlobalMethodSecurity(prePostEnabled = true)public class TokenWebSecurityConfig extends WebSecurityConfigurerAdapter &#123; private TokenManager tokenManager; private RedisTemplate redisTemplate; private DefaultPasswordEncoder defaultPasswordEncoder; private UserDetailsService userDetailsService; @Autowired public TokenWebSecurityConfig(UserDetailsService userDetailsService, DefaultPasswordEncoder defaultPasswordEncoder, TokenManager tokenManager, RedisTemplate redisTemplate) &#123; this.userDetailsService = userDetailsService; this.defaultPasswordEncoder = defaultPasswordEncoder; this.tokenManager = tokenManager; this.redisTemplate = redisTemplate; &#125; /** * 配置设置 * @param http * @throws Exception */ //设置退出的地址和token，redis操作地址 @Override protected void configure(HttpSecurity http) throws Exception &#123; http.exceptionHandling() .authenticationEntryPoint(new UnauthEntryPoint())//没有权限访问 .and().csrf().disable() .authorizeRequests() .anyRequest().authenticated() .and().logout().logoutUrl(&quot;/admin/acl/index/logout&quot;)//退出路径 .addLogoutHandler(new TokenLogoutHandler(tokenManager,redisTemplate)).and() .addFilter(new TokenLoginFilter(authenticationManager(), tokenManager, redisTemplate)) .addFilter(new TokenAuthFilter(authenticationManager(), tokenManager, redisTemplate)).httpBasic(); &#125; //调用userDetailsService和密码处理 @Override public void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.userDetailsService(userDetailsService).passwordEncoder(defaultPasswordEncoder); &#125; //不进行认证的路径，可以直接访问 @Override public void configure(WebSecurity web) throws Exception &#123; web.ignoring().antMatchers(&quot;/api/**&quot;); &#125;&#125; 核心配置类：配置自己的过滤器，调用configure进行认证 7).UserDetailsServiceImpl 12345678910111213141516171819202122232425262728@Service(&quot;userDetailsService&quot;)public class UserDetailsServiceImpl implements UserDetailsService &#123; @Autowired private UserService userService; @Autowired private PermissionService permissionService; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; //根据用户名查询数据 User user = userService.selectByUsername(username); //判断 if(user == null) &#123; throw new UsernameNotFoundException(&quot;用户不存在&quot;); &#125; com.atguigu.security.entity.User curUser = new com.atguigu.security.entity.User(); BeanUtils.copyProperties(user,curUser); //根据用户查询用户权限列表 List&lt;String&gt; permissionValueList = permissionService.selectPermissionValueByUserId(user.getId()); SecurityUser securityUser = new SecurityUser(); securityUser.setCurrentUserInfo(curUser); securityUser.setPermissionValueList(permissionValueList); return securityUser; &#125;&#125;","categories":[],"tags":[]},{"title":"Quartz定时任务调度学习","slug":"Quartz定时任务调度学习","date":"2021-01-06T07:32:29.000Z","updated":"2021-01-07T06:31:06.208Z","comments":true,"path":"2021/01/06/Quartz定时任务调度学习/","link":"","permalink":"http://example.com/2021/01/06/Quartz%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"Quartz 任务调度一、概念介绍1.任务类 Job Job 就是你想要实现的任务类，每一个必须实现 org.quartz.job 接口，并且只需实现接口定义的 execute()方法2.触发器 Trigger 用来定时，设置什么时间执行任务3.调度器 Schdule 将 Job 和 Trigger 整合起来负责基于 Trigger 设定的时间来执行 Job4.任务实例JobDetail JobDetail 表示一个具体的可执行的调度程序，Job 是这个可执行程调度程序所要执行的内容，另外 JobDetail 还包含了这个任务调度的方案和策略。 5.原理图： 二、API 介绍 Scheduler - 与调度程序交互的主要API。 Job - 你想要调度器执行的任务组件需要实现的接口 JobDetail - 用于定义作业的实例。 Trigger（即触发器） - 定义执行给定作业的计划的组件。 JobBuilder - 用于定义/构建 JobDetail 实例，用于定义作业的实例。 TriggerBuilder - 用于定义/构建触发器实例。 简单实例： Job实例类 1234567891011121314151617public class HelloJob implements Job &#123; /** * 工作执行方法 * @param jobExecutionContext * @throws JobExecutionException */ @Override public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException &#123; Date date = new Date(); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); String formatDate = simpleDateFormat.format(date); //打印工作。。 System.out.println(&quot;当前正在数据库备份，时间为&quot; + formatDate); &#125;&#125; \u0011测试类 首先说明一下： JobDetail jobDetail = newJob(HelloJob.class) 其实是JobDetail jobDetail = JobBuilder.newJob(HelloJob.class)，这样写也可以通过原因是静态导入的结果，这样可以导入静态方法，变量等。 123import static org.quartz.JobBuilder.newJob;import static org.quartz.SimpleScheduleBuilder.simpleSchedule;import static org.quartz.TriggerBuilder.newTrigger; 123456789101112131415161718192021public class QuartzTest01 &#123; public static void main(String[] args) throws SchedulerException &#123; //1.schedule调度器 Scheduler scheduler = StdSchedulerFactory.getDefaultScheduler(); //2.JobDetail任务实例 JobDetail jobDetail = newJob(HelloJob.class) .withIdentity(&quot;name1&quot;)//参数1：任务的名称（唯一实例）参数2：任务组名称 .usingJobData(&quot;message&quot;,&quot;111&quot;) .build(); //3.trigger触发器 Trigger trigger = newTrigger() .withIdentity(&quot;name1&quot;, &quot;grouo1&quot;)//参数1：触发器的名称（唯一实例）参数2：触发器组名称 .startNow()//立即执行 .withSchedule(simpleSchedule().repeatSecondlyForever(5))//每5s执行一次 .usingJobData(&quot;message&quot;,&quot;2222&quot;) .build(); //绑定触发器和任务实例 scheduler.scheduleJob(jobDetail,trigger); scheduler.start(); &#125;&#125; 三大组件：调度器、任务实例、触发器。由原理图可以看出调度器把任务实例和触发器绑定在一起，一旦触发器条件满足，就触发任务开始工作，对HelloJob进行实例 三、Job 与 JobDetailJob:工作任务调度的接口，任务类需要实现该接口。接口中的 excute()方法类似于 JDK 提供的 TimeTask 中的 run()方法，在里面编写业务逻辑。job 的生命周期:每次调度 job 执行 excute()方法前都会创建一个新的 job 实例，调用完成后会被释放，被回收。 1234@Overridepublic void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException &#123; //... &#125; JobDetail:为 job 实例提供许多可设置的属性，以及 JobDataMap 成员变量属性，它用来存储特定 Job 实例的状态信息 1234JobDetail jobDetail = newJob(HelloJob.class) .withIdentity(&quot;name1&quot;,&quot;group1&quot;)//参数1：任务的名称（唯一实例）必须设置 参数2：任务组名称,不设置默认DEFAULT .usingJobData(&quot;message&quot;,&quot;111&quot;)//JobDataMap参数设置key，value .build();//使用的是Builder构建 四、JobExecutionContext 和 JobDataMapContext 中包含了许多 JobDetail 和 Trigger 的状态信息，它是 Quartz 运行环境以及 Job 本身的明细数据 HelloJob进一步修改。。主要输出Context中的内容 123456789101112131415161718192021222324252627282930public class HelloJob implements Job &#123; private String message; public void setMessage(String message) &#123; this.message = message; &#125; /** * 工作执行方法 * @param jobExecutionContext * @throws JobExecutionException */ @Override public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException &#123; Date date = new Date(); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); String formatDate = simpleDateFormat.format(date); JobKey jobKey = jobExecutionContext.getJobDetail().getKey();//context里的JobDetail任务实例里的内容 System.out.println(jobKey.getName() + jobKey.getGroup() + jobExecutionContext.getJobDetail().getJobClass().getName());//name1 group1 cn.aimu.maizhu.job.HelloJob System.out.println(jobExecutionContext.getJobDetail().getJobClass().getSimpleName());//HelloJob System.out.println(jobExecutionContext.getJobDetail().getJobDataMap().get(&quot;message&quot;));//111 System.out.println(jobExecutionContext.getTrigger().getJobDataMap().get(&quot;message&quot;));//2222 System.out.println(simpleDateFormat.format(jobExecutionContext.getFireTime()));//当前工作时间 System.out.println(simpleDateFormat.format(jobExecutionContext.getNextFireTime()));//下一次工作时间 System.out.println(&quot;参数值为：&quot; + message);//和JobDataMap有关，输出为2222 //打印工作。。 System.out.println(&quot;当前正在数据库备份，时间为&quot; + formatDate); &#125;&#125; JobDataMap 一看就知道实现了 Map 接口，是用来存储数据的，非常方便使用 usingJobData()设置 Key 和 Value 即可。Job 实现类中添加 setter 方法对应 JobDataMap 的键值，初始化时会自动调用这些 setter 方法 上面HelloJob定义的message和setter方法就是Job实例时会初始化调用setter，如果jobdetail和trigger中的key相同，那么trigger会覆盖jobdetail中的，如上message输出2222。 五、有状态 Job 和无状态 Job有状态是指每次运行 Job 之间会保持一些状态，这些状态存储在 JobDataMap 中；而无状态是指每一次调用都会创建一个新的 JobDataMap。加注解@PersistJobDataAfterExecute 我们改一下测试类的JobDetail中的JobDataMap参数为count，0 12345//2.JobDetail任务实例 JobDetail jobDetail = newJob(HelloJob.class) .withIdentity(&quot;name1&quot;)//参数1：任务的名称（唯一实例）参数2：任务组名称 .usingJobData(&quot;count&quot;,0) .build(); 使用setter方法获取count的值，记住trigger中不要设置count的key，不然就会被覆盖，而且trigger没有状态，我们要用的是Job的实例，所有要改JobDetail，而不是Trigger。 123456789101112131415161718192021222324public class HelloJob implements Job &#123; private Integer count; public void setCount(Integer count) &#123; this.count = count; &#125; /** * 工作执行方法 * @param jobExecutionContext * @throws JobExecutionException */ @Override public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException &#123; Date date = new Date(); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); String formatDate = simpleDateFormat.format(date); ++count;//每次加1 System.out.println(&quot;数据：&quot;+count);//输出加1后的数据 jobExecutionContext.getJobDetail().getJobDataMap().put(&quot;count&quot;,count);//存储到JobDataMap中 //打印工作。。 System.out.println(&quot;当前正在数据库备份，时间为&quot; + formatDate); &#125;&#125; 存储到JobDataMap中，本来Job生命周期就是执行完execute方法就释放了，JobDataMap也就没有持久化，输出为1，1，1，1，但是加了注解之后JobDataMap被持久化了，所以输出为1，2，3，4。也可以更改配置文件，这个后面会说到。 六、Trigger主要分四种:SimpleTrigger、CalendarTrigger、DailyTimeTrigger、CronTrigger。其中 SimpleTrigger 和 CronTrigger 最常用 1.SimpleTrigger简单来说，就是隔多长时间重复多少次的触发器 有四种属性:开始时间、结束时间、重复次数、重复时间间隔。 当结束时间和重复次数冲突时，以结束时间为准。也就是结束时间属性优先于重复次数属性。 1234567891011//3.trigger触发器 Trigger trigger = newTrigger() .withIdentity(&quot;name1&quot;, &quot;grouo1&quot;)//参数1：触发器的名称（唯一实例）参数2：触发器组名称 .startAt(startDate)//以某时间开始// .startNow()//立即开始 .endAt(endDate)//以某一时间结束 .withSchedule(SimpleScheduleBuilder.simpleSchedule()//SimpleTrigger .repeatSecondlyForever(5)//5s重复一次 .withRepeatCount(3))//重复4次 .usingJobData(&quot;message&quot; ,&quot;2222&quot;) .build(); 2.CronTrigger最重要的是 Cron表达式，是以日历为基础的，按日历上来进行触发，比如几月几日，或者第几周的周几等等。关于Cron表达式在下一篇文章中。 123456Trigger trigger = newTrigger() .withIdentity(&quot;name1&quot;, &quot;grouo1&quot;)//参数1：触发器的名称（唯一实例）参数2：触发器组名称 .startAt(startDate)//立即执行 .withSchedule(CronScheduleBuilder.cronSchedule(&quot;* * * * * ?&quot;)) .usingJobData(&quot;message&quot; ,&quot;2222&quot;) .build(); 七、Listener监听器监听器就是可以在某个类作用前后进行某些方法的调用，以实现某些功能 1.JobListener1234567891011121314151617181920212223242526public class MyJobListener implements JobListener &#123; @Override public String getName() &#123; String simpleName = this.getClass().getSimpleName(); System.out.println(&quot;监听器名称：&quot; + simpleName); return simpleName; &#125; @Override public void jobToBeExecuted(JobExecutionContext jobExecutionContext) &#123; String name = jobExecutionContext.getJobDetail().getKey().getName(); System.out.println(name+&quot; Schedule在JobDetail将要被执行时调用的方法&quot;); &#125; @Override public void jobExecutionVetoed(JobExecutionContext jobExecutionContext) &#123; String name = jobExecutionContext.getJobDetail().getKey().getName(); System.out.println(name+&quot; Schedule在JobDetail将要被执行,但被否决时调用的方法&quot;); &#125; @Override public void jobWasExecuted(JobExecutionContext jobExecutionContext, JobExecutionException e) &#123; String name = jobExecutionContext.getJobDetail().getKey().getName(); System.out.println(name+&quot; Schedule在JobDetail被执行之后调用的方法&quot;); &#125;&#125; 12345678scheduler.scheduleJob(jobDetail,trigger); //创建一个并注册全局Listener //scheduler.getListenerManager().addJobListener(new MyJobListener(), EverythingMatcher.allJobs()); //创建一个并注册局部Listener scheduler.getListenerManager().addJobListener(new MyJobListener(),KeyMatcher .keyEquals(JobKey.jobKey(&quot;name1&quot;,&quot;group1&quot;)));scheduler.start(); 分为创建全局Listener与局部Listener 2.TriggerListener12345678910111213141516171819202122232425262728293031323334public class MyTriggerListener implements TriggerListener &#123; @Override public String getName() &#123; String simpleName = this.getClass().getSimpleName(); System.out.println(simpleName); return simpleName; &#125; @Override public void triggerFired(Trigger trigger, JobExecutionContext jobExecutionContext) &#123; String name = trigger.getKey().getName(); System.out.println(name + &quot;被触发&quot;); &#125; @Override public boolean vetoJobExecution(Trigger trigger, JobExecutionContext jobExecutionContext) &#123; String name = trigger.getKey().getName(); System.out.println(name + &quot;没有被触发&quot;); return false;//true表示不触发， &#125; @Override public void triggerMisfired(Trigger trigger) &#123; String name = trigger.getKey().getName(); System.out.println(name + &quot;错过触发&quot;); &#125; @Override public void triggerComplete(Trigger trigger, JobExecutionContext jobExecutionContext, Trigger.CompletedExecutionInstruction completedExecutionInstruction) &#123; String name = trigger.getKey().getName(); System.out.println(name + &quot;完成后触发&quot;); &#125;&#125; 123456789scheduler.scheduleJob(jobDetail,trigger); //创建一个并注册全局Listener //scheduler.getListenerManager()//.addTriggerListener(new MyTriggerListener(), EverythingMatcher.allTriggers()); //创建一个并注册局部Listener scheduler.getListenerManager() .addTriggerListener(new MyTriggerListener(),KeyMatcher .keyEquals(TriggerKey.triggerKey(&quot;name1&quot;,&quot;group1&quot;))); scheduler.start(); 3.ScheduleListener123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105public class MySchedulerListener implements SchedulerListener &#123; @Override public void jobScheduled(Trigger trigger) &#123; String name = trigger.getKey().getName(); System.out.println(name+ &quot; 完成部署&quot;); &#125; @Override public void jobUnscheduled(TriggerKey triggerKey) &#123; String name = triggerKey.getName(); System.out.println(name + &quot;完成卸载&quot;); &#125; @Override public void triggerFinalized(Trigger trigger) &#123; System.out.println(&quot;触发器被移除&quot;); &#125; @Override public void triggerPaused(TriggerKey triggerKey) &#123; System.out.println(&quot;暂停&quot;); &#125; @Override public void triggersPaused(String s) &#123; System.out.println(&quot;触发器组&quot;+s+&quot;被暂停&quot;); &#125; @Override public void triggerResumed(TriggerKey triggerKey) &#123; System.out.println(&quot;恢复&quot;); &#125; @Override public void triggersResumed(String s) &#123; System.out.println(&quot;组&quot;+s+&quot;被恢复&quot;); &#125; @Override public void jobAdded(JobDetail jobDetail) &#123; String name = jobDetail.getKey().getName(); System.out.println(name + &quot;job添加&quot;); &#125; @Override public void jobDeleted(JobKey jobKey) &#123; System.out.println(&quot;删除任务&quot;); &#125; @Override public void jobPaused(JobKey jobKey) &#123; System.out.println(&quot;暂停任务&quot;); &#125; @Override public void jobsPaused(String s) &#123; &#125; @Override public void jobResumed(JobKey jobKey) &#123; System.out.println(&quot;恢复任务&quot;); &#125; @Override public void jobsResumed(String s) &#123; &#125; @Override public void schedulerError(String s, SchedulerException e) &#123; System.out.println(&quot;出现错误&quot;); &#125; @Override public void schedulerInStandbyMode() &#123; System.out.println(&quot;处于Standby挂起时调用&quot;); &#125; @Override public void schedulerStarted() &#123; &#125; @Override public void schedulerStarting() &#123; &#125; @Override public void schedulerShutdown() &#123; &#125; @Override public void schedulerShuttingdown() &#123; &#125; @Override public void schedulingDataCleared() &#123; System.out.println(&quot;数据被清除&quot;); &#125;&#125; 123scheduler.scheduleJob(jobDetail,trigger); scheduler.getListenerManager().addSchedulerListener(new MySchedulerListener()); scheduler.start();","categories":[],"tags":[]},{"title":"CronTrigger之Cron表达式学习","slug":"CronTrigger之Cron表达式学习","date":"2021-01-06T07:25:40.000Z","updated":"2021-01-06T07:28:14.162Z","comments":true,"path":"2021/01/06/CronTrigger之Cron表达式学习/","link":"","permalink":"http://example.com/2021/01/06/CronTrigger%E4%B9%8BCron%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"CronTrigger之Cron表达式学习1.Cron表达式：corn从左到右（用空格隔开）：秒 分 小时 月份中的日期 月份 星期中的日期 年份 2.各字段含义 字段 允许值 允许的特殊字符 秒（Seconds） 0～59整数 ，- * / 分（Minutes） 0～59整数 ，- * / 小时（Hours） 0～23整数 ，- * / 日期（DayofMonth） 1～31整数（需要考虑月的天数） ，- * / L W C 月份（Month） 1～12整数或者JAN-DEC ，- * / 星期（DayofWeek） 1～7整数或者SUN-SAT（1=SUN） ，- * / L W C # 年（可选，留空）（Year） 1970～2099 ，- * / 3.注意事项每一个域都使用数字，但还可以出现如下特殊字符，它们的含义是： （1）：表示匹配该域的任意值。假如在Minutes域使用, 即表示每分钟都会触发事件。 （2）?：只能用在DayofMonth和DayofWeek两个域。它也匹配域的任意值，但实际不会。因为DayofMonth和DayofWeek会相互影响。例如想在每月的20日触发调度，不管20日到底是星期几，则只能使用如下写法： 13 13 15 20 * ?, 其中最后一位只能用？，而不能使用，如果使用表示不管星期几都会触发，实际上并不是这样。 （3）-：表示范围。例如在Minutes域使用5-20，表示从5分到20分钟每分钟触发一次 （4）/：表示起始时间开始触发，然后每隔固定时间触发一次。例如在Minutes域使用5/20,则意味着5分钟触发一次，而25，45等分别触发一次. （5）,：表示列出枚举值。例如：在Minutes域使用5,20，则意味着在5和20分每分钟触发一次。 （6）L：表示最后，只能出现在DayofWeek和DayofMonth域。如果在DayofWeek域使用5L,意味着在最后的一个星期四触发。 （7）W:表示有效工作日(周一到周五),只能出现在DayofMonth域，系统将在离指定日期的最近的有效工作日触发事件。例如：在 DayofMonth使用5W，如果5日是星期六，则将在最近的工作日：星期五，即4日触发。如果5日是星期天，则在6日(周一)触发；如果5日在星期一到星期五中的一天，则就在5日触发。另外一点，W的最近寻找不会跨过月份 。 （8）LW:这两个字符可以连用，表示在某个月最后一个工作日，即最后一个星期五。 （9）#:用于确定每个月第几个星期几，只能出现在DayofMonth域。例如在4#2，表示某月的第二个星期三。 4.常用表达式例子 （1）0 0 2 1 * ? * 表示在每月的1日的凌晨2点调整任务 （2）0 15 10 ? * MON-FRI 表示周一到周五每天上午10:15执行作业 （3）0 15 10 ? 6L 2002-2006 表示2002-2006年的每个月的最后一个星期五上午10:15执行作 （4）0 0 10,14,16 * * ? 每天上午10点，下午2点，4点 （5）0 0/30 9-17 * * ? 朝九晚五工作时间内每半小时 （6）0 0 12 ? * WED 表示每个星期三中午12点 （7）0 0 12 * * ? 每天中午12点触发 （8）0 15 10 ? * * 每天上午10:15触发 （9）0 15 10 * * ? 每天上午10:15触发 （10）0 15 10 * * ? * 每天上午10:15触发 （11）0 15 10 * * ? 2005 2005年的每天上午10:15触发 （12）0 * 14 * * ? 在每天下午2点到下午2:59期间的每1分钟触发 （13）0 0/5 14 * * ? 在每天下午2点到下午2:55期间的每5分钟触发 （14）0 0/5 14,18 * * ? 在每天下午2点到2:55期间和下午6点到6:55期间的每5分钟触发 （15）0 0-5 14 * * ? 在每天下午2点到下午2:05期间的每1分钟触发 （16）0 10,44 14 ? 3 WED 每年三月的星期三的下午2:10和2:44触发 （17）0 15 10 ? * MON-FRI 周一至周五的上午10:15触发 （18）0 15 10 15 * ? 每月15日上午10:15触发 （19）0 15 10 L * ? 每月最后一日的上午10:15触发 （20）0 15 10 ? * 6L 每月的最后一个星期五上午10:15触发 （21）0 15 10 ? * 6L 2002-2005 2002年至2005年的每月的最后一个星期五上午10:15触发 （22）0 15 10 ? * 6#3 每月的第三个星期五上午10:15触发","categories":[],"tags":[]},{"title":"Mybatis学习第二天","slug":"Mybatis学习第二天","date":"2020-12-21T09:04:00.000Z","updated":"2020-12-22T05:51:12.478Z","comments":true,"path":"2020/12/21/Mybatis学习第二天/","link":"","permalink":"http://example.com/2020/12/21/Mybatis%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E5%A4%A9/","excerpt":"","text":"Mybatis基本CRUD、参数类型、返回类型、SqlMapConfig配置一、回顾 二、基于代理Dao的CRUD操作1.持久化接口1234567891011121314151617181920212223242526272829303132333435363738394041424344public interface IUserDao &#123; /** * 查询所有User对象 * @return User对象的list集合 */ public List&lt;User&gt; findAll(); /** * 根据 id 查询 * @param userId * @return */ User findById(Integer userId); /** * 保存用户 * @param user * @return 影响数据库记录的行数 */ int saveUser(User user); /** * 更新用户 * @param user * @return 影响数据库记录的行数 */ int updateUser(User user); /** * 删除用户 * @param userId * @return */ int deleteUser(Integer userId); /** * 根据名称模糊查询 * @param username * @return */ List&lt;User&gt; findByName(String username); /** * 查询总记录条数 * @return */ int findTotal();&#125; 2.IUserDao.xml123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;aimu.maizhu.dao.IUserDao&quot;&gt; &lt;select id=&quot;findAll&quot; resultType=&quot;aimu.maizhu.pojo.User&quot;&gt; select * from user &lt;/select&gt; &lt;select id=&quot;findById&quot; resultType=&quot;aimu.maizhu.pojo.User&quot; parameterType=&quot;int&quot;&gt; select * from user where id = #&#123;uid&#125; &lt;/select&gt; &lt;insert id=&quot;saveUser&quot; parameterType=&quot;aimu.maizhu.pojo.User&quot;&gt; &lt;!-- 配置保存时获取插入的 id --&gt; &lt;selectKey keyColumn=&quot;id&quot; keyProperty=&quot;id&quot; resultType=&quot;int&quot;&gt; select last_insert_id(); &lt;/selectKey&gt; insert into user(username,birthday,sex,address) values(#&#123;username&#125;,#&#123;birthday&#125;,#&#123;sex&#125;,#&#123;address&#125;) &lt;/insert&gt; &lt;!-- 更新用户 --&gt; &lt;update id=&quot;updateUser&quot; parameterType=&quot;aimu.maizhu.pojo.User&quot;&gt; update user set username=#&#123;username&#125;,birthday=#&#123;birthday&#125;,sex=#&#123;sex&#125;,address=#&#123;address&#125; where id=#&#123;id&#125; &lt;/update&gt; &lt;delete id=&quot;deleteUser&quot; parameterType=&quot;int&quot;&gt; delete from user where id = #&#123;uid&#125; &lt;/delete&gt; &lt;!-- 根据名称模糊查询 --&gt; &lt;select id=&quot;findByName&quot; resultType=&quot;aimu.maizhu.pojo.User&quot; parameterType=&quot;String&quot;&gt; select * from user where username like #&#123;username&#125; &lt;/select&gt; &lt;!-- 根据名称模糊查询 &lt;select id=&quot;findByName&quot; parameterType=&quot;string&quot; resultType=&quot;com.itheima.domain.User&quot;&gt; select * from user where username like &#x27;%$&#123;value&#125;%&#x27; &lt;/select&gt; --&gt; &lt;!-- 查询总记录条数 --&gt; &lt;select id=&quot;findTotal&quot; resultType=&quot;int&quot;&gt; select count(*) from user; &lt;/select&gt;&lt;/mapper&gt; 3.测试类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990public class MybatisCRUDTest &#123; private InputStream in ; private SqlSessionFactory factory; private SqlSession session; private IUserDao userDao; @Before//在测试方法执行之前执行 public void init()throws Exception &#123; //1.读取配置文件 in = Resources.getResourceAsStream(&quot;SqlMapConfigure.xml&quot;); //2.创建构建者对象 SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder(); //3.创建 SqlSession 工厂对象 factory = builder.build(in); //4.创建 SqlSession 对象 session = factory.openSession(); //5.创建 Dao 的代理对象 userDao = session.getMapper(IUserDao.class); &#125; @Test //查询一个 public void testFindOne() &#123; //6.执行操作 User user = userDao.findById(2); System.out.println(user); &#125; @Test //保存 public void testSave()&#123; User user = new User(); user.setUsername(&quot;modify User property&quot;); user.setAddress(&quot;北京市顺义区&quot;); user.setSex(&quot;男&quot;); user.setBirthday(new Date()); System.out.println(&quot;保存操作之前：&quot;+user); //5.执行保存方法 userDao.saveUser(user); System.out.println(&quot;保存操作之后：&quot;+user); &#125; @Test //更新 public void testUpdateUser()throws Exception&#123; //1.根据 id 查询 User user = userDao.findById(2); //2.更新操作 user.setAddress(&quot;北京市顺义区&quot;); int res = userDao.updateUser(user); System.out.println(res); &#125; @Test //删除 public void testDeleteUser() throws Exception&#123; //6.执行操作 int res = userDao.deleteUser(3); System.out.println(res); &#125; @Test //模糊查询 public void testFindByName()&#123; //5.执行查询一个方法 List&lt;User&gt; users = userDao.findByName(&quot;%王%&quot;); for(User user : users)&#123; System.out.println(user); &#125; &#125; @Test //模糊查询 public void testFindByName()&#123; //5.执行查询一个方法 List&lt;User&gt; users = userDao.findByName(&quot;王&quot;); for(User user : users)&#123; System.out.println(user); &#125; &#125; @Test //查询总数据条数 public void testFindTotal() throws Exception &#123; //6.执行操作 int res = userDao.findTotal(); System.out.println(res); &#125; @After//在测试方法执行完成之后执行 public void destroy() throws Exception&#123; session.commit(); //7.释放资源 session.close(); in.close(); &#125;&#125; 4.#{}与${}区别1234567#&#123;&#125;表示一个占位符号 通过#&#123;&#125;可以实现 preparedStatement 向占位符中设置值，自动进行 java 类型和 jdbc 类型转换，#&#123;&#125;可以有效防止 sql 注入。 #&#123;&#125;可以接收简单类型值或 pojo 属性值。 如果 parameterType 传输单个简单类型值，#&#123;&#125;括号中可以是 value 或其它名称。$&#123;&#125;表示拼接 sql 串 通过$&#123;&#125;可以将 parameterType 传入的内容拼接在 sql 中且不进行 jdbc 类型转换， $&#123;&#125;可以接收简单类型值或 pojo 属性值，如果 parameterType 传输单个简单类型值，$&#123;&#125;括号中只能是 value。 三、Mybatis参数深入parameterType配置参数 我们在上一章节中已经介绍了 SQL 语句传参，使用标签的 parameterType 属性来设定。该属性的取值可以是基本类型，引用类型（例如:String 类型），还可以是实体类类型（POJO 类）。同时也可以使用实体类的包装类，本章节将介绍如何使用实体类的包装类作为参数传递。 注意事项 基 本 类 型 和 String 我 们 可 以 直 接 写 类 型 名 称 ， 也 可 以 使 用 包 名 . 类 名 的 方 式 ， 例 如 ：java.lang.String。 实体类类型，目前我们只能使用全限定类名。 究其原因，是 mybaits 在加载时已经把常用的数据类型注册了别名，从而我们在使用时可以不写包名， 而我们的是实体类并没有注册别名，所以必须写全限定类名。在今天课程的最后一个章节中将讲解如何注册实体类 的别名。 1.编写QueryVo1234@Datapublic class QueryVo implements Serializable &#123; private User user;&#125; 2.编写持久层Dao123456/** * 根据 QueryVo 中的条件查询用户 * @param vo * @return */ List&lt;User&gt; findByVo(QueryVo vo); 3.编写Dao.xml123&lt;select id=&quot;findByVo&quot; parameterType=&quot;aimu.maizhu.pojo.QueryVo&quot; resultType=&quot;aimu.maizhu.pojo.User&quot;&gt; select * from user where username like #&#123;user.username&#125;; &lt;/select&gt; 4.编写测试类1234567891011@Test public void testFindByQueryVo() &#123; QueryVo vo = new QueryVo(); User user = new User(); user.setUsername(&quot;%王%&quot;); vo.setUser(user); List&lt;User&gt; users = userDao.findByVo(vo); for (User u : users) &#123; System.out.println(u); &#125; &#125; 四、Mybatis输出结果封装1.resultType结果类型 需要注意的是，它和 parameterType 一样，如果注册过类型别名的，可以直接使用别名。没有注册过的必须使用全限定类名。例如：我们的实体类此时必须是全限定类名（今天最后一个章节会讲解如何配置实体类的别名） 同时，当是实体类名称是，还有一个要求，实体类中的属性名称必须和查询语句中的列名保持一致，否则无法实现封装。 若不一致，有两种办法解决，要么sql语句起别名，耀目使用resultMap结果类型 2.resultMap结果类型 resultMap 标签可以建立查询的列名和实体类的属性名称不一致时建立对应关系。从而实现封装。 在 select 标签中使用 resultMap 属性指定引用即可。同时 resultMap 可以实现将查询结果映射为复杂类型的 pojo，比如在查询结果映射对象中包括 pojo 和 list 实现一对一查询和一对多查询。 在Dao.xml中定义 1234567891011121314&lt;!-- 建立 User 实体和数据库表的对应关系type 属性：指定实体类的全限定类名id 属性：给定一个唯一标识，是给查询 select 标签引用用的。--&gt;&lt;resultMap type=&quot;com.itheima.domain.User&quot; id=&quot;userMap&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;userId&quot;/&gt; &lt;result column=&quot;username&quot; property=&quot;userName&quot;/&gt; &lt;result column=&quot;sex&quot; property=&quot;userSex&quot;/&gt; &lt;result column=&quot;address&quot; property=&quot;userAddress&quot;/&gt; &lt;result column=&quot;birthday&quot; property=&quot;userBirthday&quot;/&gt;&lt;/resultMap&gt;&lt;!--id 标签：用于指定主键字段result 标签：用于指定非主键字段column 属性：用于指定数据库列名property 属性：用于指定实体类属性名称--&gt; 1234&lt;!-- 配置查询所有操作 --&gt;&lt;select id=&quot;findAll&quot; resultMap=&quot;userMap&quot;&gt; select * from user&lt;/select&gt; resultMap中的id与select中的resultMap相对应 五、SqlMapConfig配置1.SqlMapConfig配置内容和顺序1234567891011121314151617-properties（属性） --property-settings（全局配置参数） --setting-typeAliases（类型别名） --typeAliase --package-typeHandlers（类型处理器）-objectFactory（对象工厂）-plugins（插件）-environments（环境集合属性对象） --environment（环境子属性对象） ---transactionManager（事务管理） ---dataSource（数据源）-mappers（映射器） --mapper --package 2.properties属性properties属性配置一123456&lt;properties&gt; &lt;property name=&quot;jdbc.driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;jdbc.url&quot; value=&quot;jdbc:mysql://localhost:3306/eesy&quot;/&gt; &lt;property name=&quot;jdbc.username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;jdbc.password&quot; value=&quot;1234&quot;/&gt;&lt;/properties&gt; properties属性配置二在classpath下定义db.properties文件 1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/eesyjdbc.username=rootjdbc.password=1234 1&lt;properties url=file:///D:/IdeaProjects/day02_eesy_01mybatisCRUD/src/main/resources/jdbcConfig.properties&quot;&gt;&lt;/properties&gt; 123456&lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt;&lt;/dataSource&gt; 3.typeAliases（类型别名）1234567&lt;typeAliases&gt; &lt;!-- 单个别名定义 --&gt; &lt;typeAlias alias=&quot;user&quot; type=&quot;com.itheima.domain.User&quot;/&gt; &lt;!-- 批量别名定义，扫描整个包下的类，别名为类名（首字母大写或小写都可以） --&gt; &lt;package name=&quot;com.itheima.domain&quot;/&gt; &lt;package name=&quot;其它包&quot;/&gt;&lt;/typeAliases&gt; 4.mappers（映射器）123456&lt;!--使用相对类路径的资源--&gt;&lt;mapper resource=&quot;com/itheima/dao/IUserDao.xml&quot; /&gt;&lt;!--使用mapper接口类路径--&gt;&lt;mapper class=&quot;com.itheima.dao.UserDao&quot;/&gt;&lt;!--注册指定包下的所有 mapper 接口--&gt;&lt;package name=&quot;cn.itcast.mybatis.mapper&quot;/&gt;","categories":[],"tags":[]},{"title":"Mybatis学习第三天","slug":"Mybatis学习第三天","date":"2020-12-21T09:03:54.000Z","updated":"2020-12-25T09:10:43.038Z","comments":true,"path":"2020/12/21/Mybatis学习第三天/","link":"","permalink":"http://example.com/2020/12/21/Mybatis%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E5%A4%A9/","excerpt":"","text":"Mybatis连接池、事务深入、动态Sql、多表查询之一对多、多对多一、Mybatis连接池与事务深入1.连接池技术 在 Mybatis 的 SqlMapConfig.xml 配置文件中，通过来实现 Mybatis 中连接池的配置。 分类 UNPOOLED 不使用连接池的数据源 POOLED 使用连接池的数据源 JNDI 使用 JNDI 实现的数据源 配置123456&lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt;&lt;/dataSource&gt; MyBatis 在初始化时，根据的 type 属性来创建相应类型的的数据源 DataSource 连接的获取分析 真正连接打开的时间点，只是在我们执行SQL语句时，才会进行。其实这样做我们也可以进一步发现，数据库连接是我们最为宝贵的资源，只有在要用到的时候，才去获取并打开连接，当我们用完了就再立即将数据库连接归还到连接池中。 2.事务控制JDBC事务回顾 在 JDBC 中我们可以通过手动方式将事务的提交改为手动方式，通过 setAutoCommit()方法就可以调整。 那么我们的 Mybatis 框架因为是对 JDBC 的封装，所以 Mybatis 框架的事务控制方式，本身也是用 JDBC 的setAutoCommit()方法来设置事务提交方式的。 Mybatis的事务控制1session.commit(); 这是我们的 Connection 的整个变化过程，通过分析我们能够发现之前的 CUD 操作过程中，我们都要手动进行事务的提交，原因是 setAutoCommit()方法，在执行时它的值被设置为 false 了，所以我们在 CUD 操作中，必须通过 sqlSession.commit()方法来执行提交操作。 Mybatis自动事务提交1session = factory.openSession(true); 我们发现，此时事务就设置为自动提交了，同样可以实现CUD操作时记录的保存。虽然这也是一种方式，但就编程而言，设置为自动提交方式为 false 再根据情况决定是否进行提交，这种方式更常用。因为我们可以根据业务情况来决定提交是否进行提交。 二、Mybatis动态Sql1.标签&lt;if&gt;12345678910111213141516&lt;select id=&quot;findByUser&quot; resultMap=&quot;userMap&quot; parameterType=&quot;aimu.maizhu.pojo.User&quot;&gt; select * from user where 1=1 &lt;if test=&quot;userName!=null and userName != &#x27;&#x27; &quot;&gt; and username like #&#123;userName&#125; &lt;/if&gt; &lt;if test=&quot;userAddress != null&quot;&gt; and address like #&#123;userAddress&#125; &lt;/if&gt;&lt;/select&gt;&lt;resultMap id=&quot;userMap&quot; type=&quot;aimu.maizhu.pojo.User&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;userId&quot;/&gt; &lt;result column=&quot;username&quot; property=&quot;userName&quot;/&gt; &lt;result column=&quot;sex&quot; property=&quot;userSex&quot;/&gt; &lt;result column=&quot;address&quot; property=&quot;userAddress&quot;/&gt; &lt;result column=&quot;birthday&quot; property=&quot;userBirthday&quot;/&gt;&lt;/resultMap&gt; 我们根据实体类的不同取值，使用不同的 SQL 语句来进行查询。比如在 id 如果不为空时可以根据 id 查询，如果 username 不同空时还要加入用户名作为条件。这种情况在我们的多条件组合查询中经常会碰到。 2.标签&lt;where&gt;123456789101112&lt;!-- 根据用户信息查询 --&gt; &lt;select id=&quot;findByUser&quot; resultType=&quot;user&quot; parameterType=&quot;user&quot;&gt; &lt;include refid=&quot;defaultSql&quot;&gt;&lt;/include&gt; &lt;where&gt; &lt;if test=&quot;username!=null and username != &#x27;&#x27; &quot;&gt; and username like #&#123;username&#125; &lt;/if&gt; &lt;if test=&quot;address != null&quot;&gt; and address like #&#123;address&#125; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 为了简化上面 where 1=1 的条件拼装，我们可以采用标签来简化开发。 3.标签&lt;foreach&gt;需求 传入多个 id 查询用户信息，用下边两个 sql 实现： SELECT * FROM USERS WHERE username LIKE ‘%张%’ AND (id =10 OR id =89 OR id=16) SELECT * FROM USERS WHERE username LIKE ‘%张%’ AND id IN (10,89,16) 这样我们在进行范围查询时，就要将一个集合中的值，作为参数动态添加进来。这样我们将如何进行参数的传递？ 123456789101112131415161718192021&lt;!-- 查询所有用户在 id 的集合之中 --&gt; &lt;select id=&quot;findInIds&quot; resultType=&quot;user&quot; parameterType=&quot;queryvo&quot;&gt; &lt;!-- select * from user where id in (1,2,3,4,5); --&gt; &lt;include refid=&quot;defaultSql&quot;&gt;&lt;/include&gt; &lt;where&gt; &lt;if test=&quot;ids != null and ids.size() &gt; 0&quot;&gt; &lt;foreach collection=&quot;ids&quot; open=&quot;id in ( &quot; close=&quot;)&quot; item=&quot;uid&quot; separator=&quot;,&quot;&gt; #&#123;uid&#125; &lt;/foreach&gt; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt;&lt;!--SQL 语句：select 字段 from user where id in (?)&lt;foreach&gt;标签用于遍历集合，它的属性：collection:代表要遍历的集合元素，注意编写时不要写#&#123;&#125;open:代表语句的开始部分close:代表结束部分item:代表遍历集合的每个元素，生成的变量名sperator:代表分隔符--&gt; 4.简化编写Sql片段1234&lt;!-- 抽取重复的语句代码片段 --&gt; &lt;sql id=&quot;defaultSql&quot;&gt; select * from user&lt;/sql&gt; 123456789&lt;!-- 配置查询所有操作 --&gt; &lt;select id=&quot;findAll&quot; resultType=&quot;user&quot;&gt; &lt;include refid=&quot;defaultSql&quot;&gt;&lt;/include&gt;&lt;/select&gt;&lt;!--配置根据id查询--&gt;&lt;select id=&quot;findById&quot; resultType=&quot;UsEr&quot; parameterType=&quot;int&quot;&gt; &lt;include refid=&quot;defaultSql&quot;&gt;&lt;/include&gt; where id = #&#123;uid&#125;&lt;/select&gt; 三、Mybatis多表查询之一对多1.一对一查询（多对一）方式一1.定义账户信息实体类123456@Datapublic class Account implements Serializable &#123; private Integer id; private Integer uid; private Double money;&#125; 基本账户信息类–和方式二不同 2.定义账户用户关联类AccountUser12345@Datapublic class AccountUser extends Account implements Serializable &#123; private String username; private String address;&#125; 定义这个类及包含了用户信息也包含了账户信息–实现一对一每个账户都有一个用户 3.持久层接口1234567public interface IAccountDao &#123; /** * 查询所有账户，同时获取账户的名字，地址 * @return AccountUser的集合 */ List&lt;AccountUser&gt; findAll();&#125; 4.AccountDao.xml12345&lt;mapper namespace=&quot;aimu.maizhu.dao.IAccountDao&quot;&gt; &lt;select id=&quot;findAll&quot; resultType=&quot;aimu.maizhu.pojo.AccountUser&quot;&gt; select a.*,u.* from account a, user u where a.uid =u.id; &lt;/select&gt;&lt;/mapper&gt; 5.测试类12345678910111213@Test/*** 测试查询所有账户信息--一对一(多对一)*/public void testFindAll01()&#123; List&lt;AccountUser&gt; accountUsers = accountDao.findAll(); for (AccountUser ac : accountUsers)&#123; System.out.println(ac); System.out.println(ac.getId()); System.out.println(ac.getMoney()); System.out.println(ac.getUid()); &#125;&#125; 方式二1.修改Account类1234567@Datapublic class Account implements Serializable &#123; private Integer id; private Integer uid; private Double money; private User user;&#125; 加入User对象，这样返回Account时就可以携带User信息，这种方式比较常用，和多对一相对应 2.修改接口Dao1234567public interface IAccountDao &#123; /** * 查询所有账户，同时获取账户的名字，地址 * @return AccountUser的集合 */ List&lt;Account&gt; findAll();&#125; 集合的泛型修改为Account 3.重新定义Dao.xml12345678910111213141516&lt;mapper namespace=&quot;aimu.maizhu.dao.IAccountDao&quot;&gt; &lt;resultMap id=&quot;AccountMap&quot; type=&quot;aimu.maizhu.pojo.Account&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;uid&quot; property=&quot;uid&quot;/&gt; &lt;result column=&quot;money&quot; property=&quot;money&quot;/&gt; &lt;association property=&quot;user&quot; javaType=&quot;aimu.maizhu.pojo.User&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;userId&quot;/&gt; &lt;result column=&quot;username&quot; property=&quot;userName&quot;/&gt; &lt;result column=&quot;sex&quot; property=&quot;userSex&quot;/&gt; &lt;result column=&quot;address&quot; property=&quot;userAddress&quot;/&gt; &lt;result column=&quot;birthday&quot; property=&quot;userBirthday&quot;/&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id=&quot;findAll&quot; resultMap=&quot;AccountMap&quot;&gt; select a.*,u.* from account a, user u where a.uid =u.id; &lt;/select&gt; resultMap中的association标签表示引入的实体类（从表的方面来说），表示Account中包含User实体属性，javaType表示实体类的类型，配置完别忘了select标签的resultMap的设置 4.修改测试类12345678910@Test /** * 测试查询所有账户信息--一对一(多对一) */ public void testFindAll02()&#123; List&lt;Account&gt; accounts = accountDao.findAll(); for (Account ac : accounts)&#123; System.out.println(ac); &#125; &#125; 以上是对Mybatis一对一（多对一）查询的基本配置的两种方式，下面介绍一对多查询。 2.一对多查询1.编写sql语句1select u.*,a.id as aid ,a.uid,a.money from user u left outer join account a on u.id =a.uid 2.User类加入List&lt;Account&gt;12345678910@Datapublic class User implements Serializable &#123; private Integer userId; private String userName; private Date userBirthday; private String userSex; private String userAddress; private List&lt;Account&gt; accounts;&#125; 加入了List集合进去，表示一个User可能对应多个Account 3.持久层加入方法12345/** * 查询所有用户，同时获取出每个用户下的所有账户信息 * @return */ List&lt;User&gt; findAll(); 4.xml配置1234567891011121314151617&lt;mapper namespace=&quot;aimu.maizhu.dao.IUserDao&quot;&gt; &lt;resultMap id=&quot;userMap&quot; type=&quot;aimu.maizhu.pojo.User&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;userId&quot;/&gt; &lt;result column=&quot;username&quot; property=&quot;userName&quot;/&gt; &lt;result column=&quot;sex&quot; property=&quot;userSex&quot;/&gt; &lt;result column=&quot;address&quot; property=&quot;userAddress&quot;/&gt; &lt;result column=&quot;birthday&quot; property=&quot;userBirthday&quot;/&gt; &lt;collection property=&quot;accounts&quot; ofType=&quot;aimu.maizhu.pojo.Account&quot;&gt; &lt;id column=&quot;aid&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;uid&quot; property=&quot;uid&quot;/&gt; &lt;result column=&quot;money&quot; property=&quot;money&quot;/&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=&quot;findAll&quot; resultMap=&quot;userMap&quot;&gt; select u.*,a.id as aid ,a.uid,a.money from user u left outer join account a on u.id =a.uid &lt;/select&gt;&lt;/mapper&gt; 一对多的配置中collection 是用于建立一对多中集合属性的对应关系 ofType 用于指定集合元素的数据类型 5.测试方法12345678910@Test public void testFindAll() &#123; //6.执行操作 List&lt;User&gt; users = userDao.findAll(); for (User user : users) &#123; System.out.println(&quot;-------每个用户的内容---------&quot;); System.out.println(user); System.out.println(user.getAccounts()); &#125; &#125; 一对多完成，接下来是多对多 四、Mybatis多表查询之多对多 通过前面的学习，我们使用 Mybatis 实现一对多关系的维护。多对多关系其实我们看成是双向的一对多关系。 1.从Role到User多对多1.sql语句 1234567891011121314SELECTr.*,u.id uid,u.username username,u.birthday birthday,u.sex sex,u.address addressFROM ROLE rINNER JOIN USER_ROLE urON ( r.id = ur.rid)INNER JOINUSER uON (ur.uid = u.id); 2.Role实体类 12345678@Datapublic class Role implements Serializable &#123; private Integer roleId; private String roleName; private String roleDesc; //多对多的关系映射：一个角色可以赋予多个用户 private List&lt;User&gt; users;&#125; 3.Role持久层接口 1234567891011/*** @author 黑马程序员* @Company http://www.ithiema.com*/public interface IRoleDao &#123; /** * 查询所有角色 * @return */ List&lt;Role&gt; findAll();&#125; 4.编写xml文件 123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapperPUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;com.itheima.dao.IRoleDao&quot;&gt; &lt;!--定义 role 表的 ResultMap--&gt; &lt;resultMap id=&quot;roleMap&quot; type=&quot;role&quot;&gt; &lt;id property=&quot;roleId&quot; column=&quot;rid&quot;&gt;&lt;/id&gt; &lt;result property=&quot;roleName&quot; column=&quot;role_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;roleDesc&quot; column=&quot;role_desc&quot;&gt;&lt;/result&gt; &lt;collection property=&quot;users&quot; ofType=&quot;user&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;&gt;&lt;/id&gt; &lt;result column=&quot;username&quot; property=&quot;username&quot;&gt;&lt;/result&gt; &lt;result column=&quot;address&quot; property=&quot;address&quot;&gt;&lt;/result&gt; &lt;result column=&quot;sex&quot; property=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result column=&quot;birthday&quot; property=&quot;birthday&quot;&gt;&lt;/result&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;!--查询所有--&gt; &lt;select id=&quot;findAll&quot; resultMap=&quot;roleMap&quot;&gt;select u.*,r.id as rid,r.role_name,r.role_desc from role r left outer join user_role ur on r.id = ur.ridleft outer join user u on u.id = ur.uid &lt;/select&gt;&lt;/mapper&gt; 实际上操作相当于一对多关系，xml设置的resultMap和一对多的一样 5.测试。。略 2.从User到Role多对多 从 User 出发，我们也可以发现一个用户可以具有多个角色，这样用户到角色的关系也还是一对多关系。这样 我们就可以认为 User 与 Role 的多对多关系，可以被拆解成两个一对多关系来实现。 Mybatis延迟加载、缓存、注解开发一、延迟加载 延迟加载： ​ 就是在需要用到数据时才进行加载，不需要用到数据时就不加载数据。延迟加载也称懒加载. 好处： ​ 先从单表查询，需要时再从关联表去关联查询，大大提高数据库性能，因为查询单表要比关联查询多张表速度要快。 坏处： ​ 因为只有当需要用到数据时，才会进行数据库查询，这样在大批量数据查询时，因为查询工作也要消耗时间，所以可能造成用户等待时间变长，造成用户体验下降。 association、collection 具备延迟加载功能。 1.使用association实现延迟加载12345678&lt;resultMap type=&quot;account&quot; id=&quot;accountMap&quot;&gt; &lt;id column=&quot;aid&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;uid&quot; property=&quot;uid&quot;/&gt; &lt;result column=&quot;money&quot; property=&quot;money&quot;/&gt; &lt;!-- 它是用于指定从表方的引用实体属性的 --&gt; &lt;association property=&quot;user&quot; javaType=&quot;user&quot;select=&quot;com.itheima.dao.IUserDao.findById&quot;column=&quot;uid&quot;&gt; &lt;/association&gt;&lt;/resultMap&gt; select： 填写我们要调用的 select 映射的 id column ： 填写我们要传递给 select 映射的参数 1234&lt;settings&gt; &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt; &lt;setting name=&quot;aggressiveLazyLoading&quot; value=&quot;false&quot;/&gt;&lt;/settings&gt; 开启延迟加载 配置SqlMapConfig 2.使用collection实现延迟加载1234567891011121314&lt;resultMap type=&quot;user&quot; id=&quot;userMap&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;&gt;&lt;/id&gt; &lt;result column=&quot;username&quot; property=&quot;username&quot;/&gt; &lt;result column=&quot;address&quot; property=&quot;address&quot;/&gt; &lt;result column=&quot;sex&quot; property=&quot;sex&quot;/&gt; &lt;result column=&quot;birthday&quot; property=&quot;birthday&quot;/&gt; &lt;!-- collection 是用于建立一对多中集合属性的对应关系 ofType 用于指定集合元素的数据类型 select 是用于指定查询账户的唯一标识（账户的 dao 全限定类名加上方法名称） column 是用于指定使用哪个字段的值作为条件查询 --&gt; &lt;collection property=&quot;accounts&quot; ofType=&quot;account&quot; select=&quot;com.itheima.dao.IAccountDao.findByUid&quot; column=&quot;id&quot;&gt; &lt;/collection&gt;&lt;/resultMap&gt; select 属性： 用于指定查询 account 列表的 sql 语句，所以填写的是该 sql 映射的 id column 属性： 用于指定 select 属性的 sql 语句的参数来源，上面的参数来自于 user 的 id 列，所以就写成 id 这一 个字段名了 二、缓存1.一级缓存测试： 123456789@Testpublic void testFindById() &#123; //6.执行操作 User user = userDao.findById(41); System.out.println(&quot;第一次查询的用户：&quot;+user); User user2 = userDao.findById(41); System.out.println(&quot;第二次查询用户：&quot;+user2); System.out.println(user == user2);&#125; 发现打印结果相同，说明存在一级缓存 一级缓存是 SqlSession 范围的缓存，当调用 SqlSession 的修改，添加，删除，commit()，close()等方法时，就会清空一级缓存。 12345678910111213@Testpublic void testFirstLevelCache()&#123; User user1 = userDao.findById(41); System.out.println(user1); // sqlSession.close(); //再次获取 SqlSession 对象 // sqlSession = factory.openSession(); sqlSession.clearCache();//此方法也可以清空缓存 userDao = sqlSession.getMapper(IUserDao.class); User user2 = userDao.findById(41); System.out.println(user2); System.out.println(user1 == user2);&#125; 清除缓存的测试，两种方法，一是把sqqlSession直接关闭重开，二是使用清空缓存方法。 1234567891011121314@Testpublic void testClearlCache()&#123; //1.根据 id 查询用户 User user1 = userDao.findById(41); System.out.println(user1); //2.更新用户信息 user1.setUsername(&quot;update user clear cache&quot;); user1.setAddress(&quot;北京市海淀区&quot;); userDao.updateUser(user1); //3.再次查询 id 为 41 的用户 User user2 = userDao.findById(41); System.out.println(user2); System.out.println(user1 == user2);&#125; 当执行sqlSession.close()后，再次获取sqlSession并查询id=41的User对象时，又重新执行了sql 语句，从数据库进行了查询操作。更新方法提交事务，使缓存清空了。 2.二级缓存 二级缓存是 mapper 映射级别的缓存，多个 SqlSession 去操作同一个 Mapper 映射的 sql 语句，多个 SqlSession 可以共用二级缓存，二级缓存是跨 SqlSession 的。 1.开启二级缓存–SqlMapConfig配置 12345&lt;settings&gt;&lt;!-- 开启二级缓存的支持 --&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt;因为 cacheEnabled 的取值默认就为 true，所以这一步可以省略不配置。为 true 代表开启二级缓存；为false 代表不开启二级缓存。 2.配置dao.xml 1234&lt;mapper namespace&#x3D;&quot;com.itheima.dao.IUserDao&quot;&gt; &lt;!-- 开启二级缓存的支持 --&gt; &lt;cache&gt;&lt;&#x2F;cache&gt;&lt;&#x2F;mapper&gt; 3.配置statement上的useCache属性 12345&lt;!-- 根据 id 查询 --&gt; &lt;select id=&quot;findById&quot; resultType=&quot;user&quot; parameterType=&quot;int&quot; useCache=&quot;true&quot;&gt; select * from user where id = #&#123;uid&#125;&lt;/select&gt; 将 UserDao.xml 映射文件中的select标签中设置 useCache=”true”代表当前这个 statement 要使用二级缓存，如果不使用二级缓存可以设置为 false。 注意：针对每次查询都需要最新的数据 sql，要设置成 useCache=false，禁用二级缓存。 4.二级缓存测试 1234567891011121314@Testpublic void testFirstLevelCache()&#123; SqlSession sqlSession1 = factory.openSession(); IUserDao dao1 = sqlSession1.getMapper(IUserDao.class); User user1 = dao1.findById(41); System.out.println(user1); sqlSession1.close();//一级缓存消失 SqlSession sqlSession2 = factory.openSession(); IUserDao dao2 = sqlSession2.getMapper(IUserDao.class); User user2 = dao2.findById(41); System.out.println(user2); sqlSession2.close(); System.out.println(user1 == user2);&#125; 经过上面的测试，我们发现执行了两次查询，并且在执行第一次查询后，我们关闭了一级缓存，再去执行第二 次查询时，我们发现并没有对数据库发出 sql 语句，所以此时的数据就只能是来自于我们所说的二级缓存。 当我们在使用二级缓存时，所缓存的类一定要实现 java.io.Serializable 接口，这种就可以使用序列化 方式来保存对象。 三、注解开发 1234567891011@Insert:实现新增@Update:实现更新@Delete:实现删除@Select:实现查询@Result:实现结果集封装@Results:可以与@Result 一起使用，封装多个结果集@ResultMap:实现引用@Results 定义的封装@One:实现一对一结果集封装@Many:实现一对多结果集封装@SelectProvider: 实现动态 SQL 映射@CacheNamespace:实现注解二级缓存的使用 注解开启二级缓存 12345&lt;!-- 配置二级缓存 --&gt; &lt;settings&gt; &lt;!-- 开启二级缓存的支持 --&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt; 1234@CacheNamespace(blocking=true)//mybatis 基于注解方式实现配置二级缓存public interface IUserDao &#123; //..&#125;","categories":[],"tags":[]},{"title":"Mybatis学习之自定义框架","slug":"Mybatis学习之自定义框架","date":"2020-12-20T00:49:23.000Z","updated":"2020-12-21T08:10:38.104Z","comments":true,"path":"2020/12/20/Mybatis学习之自定义框架/","link":"","permalink":"http://example.com/2020/12/20/Mybatis%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A1%86%E6%9E%B6/","excerpt":"","text":"Mybatis学习第一天Mybatis框架是一款解决数据持久化问题的框架，类似的有Hibernate、Spring Data。 一、概述123456789mybatis 是一个优秀的基于 java 的持久层框架，它内部封装了 jdbc，使开发者只需要关注 sql 语句本身，而不需要花费精力去处理加载驱动、创建连接、创建 statement 等繁杂的过程。mybatis 通过 xml 或注解的方式将要执行的各种 statement 配置起来，并通过 java 对象和 statement 中sql 的动态参数进行映射生成最终执行的 sql 语句，最后由 mybatis 框架执行 sql 并将结果映射为 java 对象并返回。采用 ORM 思想解决了实体和数据库映射的问题，对 jdbc 进行了封装，屏蔽了 jdbc api 底层访问细节，使我们不用与 jdbc api 打交道，就可以完成对数据库的持久化操作。为了我们能够更好掌握框架运行的内部过程，并且有更好的体验，下面我们将从自定义 Mybatis 框架开始来学习框架。此时我们将会体验框架从无到有的过程体验，也能够很好的综合前面阶段所学的基础。 JDBC分析： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class JdbcTest &#123; public static void main(String[] args) &#123; Connection connection = null; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; //加载数据库驱动 Class.forName(&quot;com.mysql.jdbc.Driver&quot;); //通过驱动管理类获取数据库链接 connection = DriverManager .getConnection(&quot;jdbc:mysql://localhost:3306/mybatis?characterEncoding=utf-8&amp;useSSL=false&quot;, &quot;root&quot;, &quot;rootroot&quot;); //定义 sql 语句 ?表示占位符 String sql = &quot;select * from user where username = ?&quot;; //获取预处理 statement preparedStatement = connection.prepareStatement(sql); //设置参数，第一个参数为 sql 语句中参数的序号（从 1 开始），第二个参数为设置的参数值 preparedStatement.setString(1, &quot;王五&quot;); //向数据库发出 sql 执行查询，查询出结果集 resultSet = preparedStatement.executeQuery(); //遍历查询结果集 while (resultSet.next()) &#123; System.out.println(resultSet.getString(&quot;id&quot;) + &quot; &quot; + resultSet.getString(&quot;username&quot;)); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; //释放资源 if (resultSet != null) &#123; try &#123; resultSet.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (preparedStatement != null) &#123; try &#123; preparedStatement.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (connection != null) &#123; try &#123; connection.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 1、数据库链接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库链接池可解决此问题。 2、Sql 语句在代码中硬编码，造成代码不易维护，实际应用 sql 变化的可能较大，sql 变动需要改变 java代码。 3、使用 preparedStatement 向占有位符号传参数存在硬编码，因为 sql 语句的 where 条件不一定，可能多也可能少，修改 sql 还要修改代码，系统不易维护。 4、对结果集解析存在硬编码（查询列名），sql 变化导致解析代码变化，系统不易维护，如果能将数据库记录封装成 pojo 对象解析比较方便。 二、Mybatis快速入门1.创建maven工程2.导入依赖1234567891011121314151617181920212223242526272829&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.16&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3.编写User实体类123456789@Datapublic class User implements Serializable &#123; private Integer id; private String username; private Date birthday; private String sex; private String address;&#125; 4.编写持久层接口IUserDao12345678public interface IUserDao &#123; /** * 查询所有User对象 * @return User对象的list集合 */ public List&lt;User&gt; findAll();&#125; 5.编写持久层接口的映射文件IUserDao.xml123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;aimu.maizhu.dao.IUserDao&quot;&gt; &lt;select id=&quot;findAll&quot; resultType=&quot;aimu.maizhu.pojo.User&quot;&gt; select * from user &lt;/select&gt;&lt;/mapper&gt; 要求： 创建位置：必须和持久层接口在相同的包中。 名称：必须以持久层接口名称命名文件名，扩展名是.xml 6.编写SqlMapConfigure1234567891011121314151617181920212223242526&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;!--配置Mybatis环境--&gt; &lt;environments default=&quot;mysql&quot;&gt; &lt;!--配置Mysql环境--&gt; &lt;environment id=&quot;mysql&quot;&gt; &lt;!--配置事务的类型--&gt; &lt;transactionManager type=&quot;JDBC&quot;&gt;&lt;/transactionManager&gt; &lt;!--配置连接数据库的信息：用的是数据源(连接池)--&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis?characterEncoding=utf8&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;rootroot&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 告知 mybatis 映射配置的位置 --&gt; &lt;mappers&gt; &lt;mapper resource=&quot;aimu/maizhu/dao/IUserDao.xml&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 7.测试类12345678910111213141516171819202122public class MybatisTest &#123; public static void main(String[] args) throws IOException &#123; //1.读取配置文件 InputStream in = Resources.getResourceAsStream(&quot;SqlMapConfigure.xml&quot;); //2.创建 SqlSessionFactory 的构建者对象 SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder(); //3.使用构建者创建工厂对象 SqlSessionFactory SqlSessionFactory factory = builder.build(in); //4.使用 SqlSessionFactory 生产 SqlSession 对象 SqlSession session = factory.openSession(); //5.使用 SqlSession 创建 dao 接口的代理对象 IUserDao userDao = session.getMapper(IUserDao.class); //6.使用代理对象执行查询所有方法 List&lt;User&gt; userList = userDao.findAll(); for(User user : userList) &#123; System.out.println(user); &#125; //7.释放资源 session.close(); in.close(); &#125;&#125; 在使用基于注解的 Mybatis 配置时，请移除 xml 的映射配置（IUserDao.xml） 在IUserDao方法上添加注解并修改SqlMapConfigure.xml的mapper信息 三、自定义Mybatis框架 1.创建工程，引入坐标1234567891011121314151617181920212223242526272829303132&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;dom4j&lt;/groupId&gt; &lt;artifactId&gt;dom4j&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.16&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jaxen&lt;/groupId&gt; &lt;artifactId&gt;jaxen&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 这里说明一点就行dom4j的版本问题，在2.x版本中的root.selectNodes方法中，返回的是List是不需要的,1.6.1版本的返回List没有泛型，可以使用List，然后使用element类的方法是我们需要的。 2.引入工具类(1)解析xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187public class XMLConfigBuilder &#123; /** * 解析主配置文件，把里面的内容填充到 DefaultSqlSession 所需要的地方 * 使用的技术： * dom4j+xpath * * @param session */ public static void loadConfiguration(DefaultSqlSession session, InputStream config) &#123; try &#123; //定义封装连接信息的配置对象（mybatis 的配置对象） Configuration cfg = new Configuration(); //1.获取 SAXReader 对象 SAXReader reader = new SAXReader(); //2.根据字节输入流获取 Document 对象 Document document = reader.read(config); //3.获取根节点 Element root = document.getRootElement(); //4.使用 xpath 中选择指定节点的方式，获取所有 property 节点 List&lt;Element&gt; propertyElements = root.selectNodes(&quot;//property&quot;); //5.遍历节点 for (Element propertyElement : propertyElements) &#123; //判断节点是连接数据库的哪部分信息 //取出 name 属性的值 String name = propertyElement.attributeValue(&quot;name&quot;); if (&quot;driver&quot;.equals(name)) &#123; //表示驱动 //获取 property 标签 value 属性的值 String driver = propertyElement.attributeValue(&quot;value&quot;); cfg.setDriver(driver); &#125; if (&quot;url&quot;.equals(name)) &#123; //表示连接字符串 //获取 property 标签 value 属性的值 String url = propertyElement.attributeValue(&quot;value&quot;); cfg.setUrl(url); &#125; if (&quot;username&quot;.equals(name)) &#123; //表示用户名 //获取 property 标签 value 属性的值 String username = propertyElement.attributeValue(&quot;value&quot;); cfg.setUsername(username); &#125; if (&quot;password&quot;.equals(name)) &#123; //表示密码 //获取 property 标签 value 属性的值 String password = propertyElement.attributeValue(&quot;value&quot;); cfg.setPassword(password); &#125; &#125; //取出 mappers 中的所有 mapper 标签，判断他们使用了 resource 还是 class 属性 List&lt;Element&gt; mapperElements = root.selectNodes(&quot;//mappers/mapper&quot;); //遍历集合 for (Element mapperElement : mapperElements) &#123; //判断 mapperElement 使用的是哪个属性 Attribute attribute = mapperElement.attribute(&quot;resource&quot;); if (attribute != null) &#123; System.out.println(&quot;使用的是 XML&quot;); //表示有 resource 属性，用的是 XML //取出属性的值 String mapperPath = attribute.getValue();// 获 取 属 性 的 值&quot;com/itheima/dao/IUserDao.xml&quot; //把映射配置文件的内容获取出来，封装成一个 map Map&lt;String, Mapper&gt; mappers = loadMapperConfiguration(mapperPath); //给 configuration 中的 mappers 赋值 cfg.setMappers(mappers); &#125; else &#123; System.out.println(&quot;使用的是注解&quot;); //表示没有 resource 属性，用的是注解 //获取 class 属性的值 String daoClassPath = mapperElement.attributeValue(&quot;class&quot;); //根据 daoClassPath 获取封装的必要信息 Map&lt;String, Mapper&gt; mappers = loadMapperAnnotation(daoClassPath); //给 configuration 中的 mappers 赋值 cfg.setMappers(mappers); &#125; &#125; //把配置对象传递给 DefaultSqlSession session.setCfg(cfg); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; finally &#123; try &#123; config.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 根据传入的参数，解析 XML，并且封装到 Map 中 * @param mapperPath 映射配置文件的位置 * @return map 中包含了获取的唯一标识（key 是由 dao 的全限定类名和方法名组成） * 以及执行所需的必要信息（value 是一个 Mapper 对象，里面存放的是执行的 SQL 语句和 要封装的实体类全限定类名） */ private static Map&lt;String,Mapper&gt; loadMapperConfiguration(String mapperPath)throws IOException &#123; InputStream in = null; try&#123; //定义返回值对象 Map&lt;String,Mapper&gt; mappers = new HashMap&lt;String,Mapper&gt;(); //1.根据路径获取字节输入流 in = Resources.getResourceAsStream(mapperPath); //2.根据字节输入流获取 Document 对象 SAXReader reader = new SAXReader(); Document document = reader.read(in); //3.获取根节点 Element root = document.getRootElement(); //4.获取根节点的 namespace 属性取值 String namespace = root.attributeValue(&quot;namespace&quot;);//是组成 map 中 key 部分 //5.获取所有的 select 节点 List&lt;Element&gt; selectElements = root.selectNodes(&quot;//select&quot;); //6.遍历 select 节点集合 for(Element selectElement : selectElements)&#123; //取出 id 属性的值 组成 map 中 key 的部分 String id = selectElement.attributeValue(&quot;id&quot;); //取出 resultType 属性的值 组成 map 中 value 的部分 String resultType = selectElement.attributeValue(&quot;resultType&quot;); //取出文本内容 组成 map 中 value 的部分 String queryString = selectElement.getText(); //创建 Key String key = namespace+&quot;.&quot;+id; //创建 Value Mapper mapper = new Mapper(); mapper.setQueryString(queryString); mapper.setResultType(resultType); //把 key 和 value 存入 mappers 中 mappers.put(key,mapper); &#125; return mappers; &#125;catch(Exception e)&#123; throw new RuntimeException(e); &#125;finally&#123; in.close(); &#125; &#125; /** * 根据传入的参数，得到 dao 中所有被 select 注解标注的方法。 * 根据方法名称和类名，以及方法上注解 value 属性的值，组成 Mapper 的必要信息 * @param daoClassPath * @return */ private static Map&lt;String,Mapper&gt; loadMapperAnnotation(String daoClassPath)throws Exception&#123; //定义返回值对象 Map&lt;String,Mapper&gt; mappers = new HashMap&lt;String, Mapper&gt;(); //1.得到 dao 接口的字节码对象 Class daoClass = Class.forName(daoClassPath); //2.得到 dao 接口中的方法数组 Method[] methods = daoClass.getMethods(); //3.遍历 Method 数组 for(Method method : methods)&#123; //取出每一个方法，判断是否有 select 注解 boolean isAnnotated = method.isAnnotationPresent(Select.class); if(isAnnotated)&#123; //创建 Mapper 对象 Mapper mapper = new Mapper(); //取出注解的 value 属性值 Select selectAnno = method.getAnnotation(Select.class); String queryString = selectAnno.value(); mapper.setQueryString(queryString); //获取当前方法的返回值，还要求必须带有泛型信息 Type type = method.getGenericReturnType();//List&lt;User&gt; //判断 type 是不是参数化的类型 if(type instanceof ParameterizedType)&#123; //强转 ParameterizedType ptype = (ParameterizedType)type; //得到参数化类型中的实际类型参数 Type[] types = ptype.getActualTypeArguments(); //取出第一个 Class domainClass = (Class)types[0]; //获取 domainClass 的类名 String resultType = domainClass.getName(); //给 Mapper 赋值 mapper.setResultType(resultType); &#125; //组装 key 的信息 //获取方法的名称 String methodName = method.getName(); String className = method.getDeclaringClass().getName(); String key = className+&quot;.&quot;+methodName; //给 map 赋值 mappers.put(key,mapper); &#125; &#125; return mappers; &#125;&#125; （2）执行sql语句 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Executor &#123; public &lt;E&gt; List&lt;E&gt; selectList(Mapper mapper, Connection conn) &#123; PreparedStatement pstm = null; ResultSet rs = null; try &#123; //1.取出 mapper 中的数据 String queryString = mapper.getQueryString();//select * from user String resultType = mapper.getResultType();//com.itheima.domain.User Class domainClass = Class.forName(resultType);//User.class //2.获取 PreparedStatement 对象 pstm = conn.prepareStatement(queryString); //3.执行 SQL 语句，获取结果集 rs = pstm.executeQuery(); //4.封装结果集 List&lt;E&gt; list = new ArrayList&lt;E&gt;();//定义返回值 while (rs.next()) &#123; //实例化要封装的实体类对象 E obj = (E) domainClass.newInstance();//User 对象 //取出结果集的元信息：ResultSetMetaData ResultSetMetaData rsmd = rs.getMetaData(); //取出总列数 int columnCount = rsmd.getColumnCount(); //遍历总列数 for (int i = 1; i &lt;= columnCount; i++) &#123; //获取每列的名称，列名的序号是从 1 开始的 String columnName = rsmd.getColumnName(i); //根据得到列名，获取每列的值 Object columnValue = rs.getObject(columnName); //给 obj 赋值：使用 Java 内省机制（借助 PropertyDescriptor 实现属性的封装） PropertyDescriptor pd = new PropertyDescriptor(columnName, domainClass);//要求：实体类的属性和数据库表的列名保持一种 //获取它的写入方法 Method writeMethod = pd.getWriteMethod();//setUsername(Stringusername); //把获取的列的值，给对象赋值 writeMethod.invoke(obj, columnValue); &#125; //把赋好值的对象加入到集合中 list.add(obj); &#125; return list; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; finally &#123; release(pstm, rs); &#125; &#125; private void release(PreparedStatement pstm, ResultSet rs) &#123; if (rs != null) &#123; try &#123; rs.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; if (pstm != null) &#123; try &#123; pstm.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; （3）数据源的工具类 12345678910111213141516171819public class DataSourcesUtil &#123; /** * 获取连接 * * @param cfg * @return */ public static Connection getConnection(Configuration cfg) &#123; try &#123; Class.forName(cfg.getDriver()); Connection conn = DriverManager.getConnection(cfg.getUrl(), cfg.getUsername(), cfg.getPassword()); return conn; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 3.SqlMapConfig配置1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;configuration&gt; &lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql:///mybatis?characterEncoding=utf8&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;rootroot&quot;&gt;&lt;/property&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=&quot;aimu/maizhu/dao/IUserDao.xml&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 4.编写读取配置文件类12345678910111213public class Resources &#123; /** * 用于加载 xml 文件，并且得到一个流对象 * @param xmlPath * @return * 在实际开发中读取配置文件: * 第一：使用类加载器。但是有要求：a 文件不能过大。 b 文件必须在类路径下(classpath) * 第二：使用 ServletContext 的 getRealPath() */ public static InputStream getResourceAsStream(String xmlPath) &#123; return Resources.class.getClassLoader().getResourceAsStream(xmlPath); &#125;&#125; 5.编写Mapper类12345@Datapublic class Mapper &#123; private String queryString;//sql private String resultType;//结果类型的全限定类名&#125; 6.编写Configuration配置类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Configuration &#123; private String username; //用户名 private String password;//密码 private String url;//地址 private String driver;//驱动 //map 集合 Map&lt;唯一标识，Mapper&gt; 用于保存映射文件中的 sql 标识及 sql 语句 private Map&lt;String,Mapper&gt; mappers = new HashMap&lt;&gt;(); public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public String getDriver() &#123; return driver; &#125; public void setDriver(String driver) &#123; this.driver = driver; &#125; public Map&lt;String, Mapper&gt; getMappers() &#123; return mappers; &#125; public void setMappers(Map&lt;String, Mapper&gt; mappers) &#123; this.mappers.putAll(mappers); &#125;&#125; 这里说明一下mappers的set方法 this.mappers.putAll(mappers);不可以用this.mappers=mappers，因为如果有多个mappers会覆盖掉前面的mappers，用putAll方法会将所有的mappers放到map里。 7.编写User实体类12345678@Datapublic class User implements Serializable &#123; private int id; private String username;// 用户姓名 private String sex;// 性别 private Date birthday;// 生日 private String address;// 地址&#125; 8.持久层接口与xml1234567public interface IUserDao &#123; /** * 查询所有用户 * @return */ List&lt;User&gt; findAll();&#125; 1234567&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;mapper namespace=&quot;aimu.maizhu.dao.IUserDao&quot;&gt; &lt;!-- 配置查询所有操作 --&gt; &lt;select id=&quot;findAll&quot; resultType=&quot;aimu.maizhu.pojo.User&quot;&gt; select * from user &lt;/select&gt;&lt;/mapper&gt; 9.编写构建者类1234567891011121314public class SqlSessionFactoryBuilder &#123; /** * 根据传入的流，实现对 SqlSessionFactory 的创建 * * @param in 它就是 SqlMapConfig.xml 的配置以及里面包含的 IUserDao.xml 的配置 * @returnSqlSessionFactory */ public SqlSessionFactory build(InputStream in) &#123; DefaultSqlSessionFactory factory = new DefaultSqlSessionFactory();//给 factory 中 config 赋值 factory.setConfig(in); return factory; &#125;&#125; 10.编写SqlSessionFactory接口与实现类1234567public interface SqlSessionFactory &#123; /** * 创建一个新的 SqlSession 对象 * @return */ SqlSession openSession();&#125; 12345678910111213public class DefaultSqlSessionFactory implements SqlSessionFactory &#123; private InputStream config = null; public void setConfig(InputStream config) &#123; this.config = config; &#125; @Override public SqlSession openSession() &#123; DefaultSqlSession session = new DefaultSqlSession();//调用工具类解析 xml 文件 XMLConfigBuilder.loadConfiguration(session, config); return session; &#125;&#125; 11.编写SqlSession接口与实现类123456789101112public interface SqlSession &#123; /** * 创建 Dao 接口的代理对象 * @param daoClass * @return */ &lt;T&gt; T getMapper(Class&lt;T&gt; daoClass); /** * 释放资源 */ void close();&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class DefaultSqlSession implements SqlSession &#123; //核心配置对象 private Configuration cfg; public void setCfg(Configuration cfg) &#123; this.cfg = cfg; &#125; //连接对象 private Connection conn; //调用 DataSourceUtils 工具类获取连接 public Connection getConn() &#123; try &#123; conn = DataSourcesUtil.getConnection(cfg); return conn; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * 动态代理： * 涉及的类：Proxy * 使用的方法：newProxyInstance * 方法的参数： * ClassLoader：和被代理对象使用相同的类加载器,通常都是固定的 * Class[]：代理对象和被代理对象要求有相同的行为。（具有相同的方法） * InvocationHandler：如何代理。需要我们自己提供的增强部分的代码 */ @Override public &lt;T&gt; T getMapper(Class&lt;T&gt; daoClass) &#123; conn = getConn(); System.out.println(conn); T daoProxy = (T) Proxy.newProxyInstance(daoClass.getClassLoader(), new Class[]&#123;daoClass&#125;, new MapperProxyFactory(cfg.getMappers(), conn)); return daoProxy; &#125;//释放资源 @Override public void close() &#123; try &#123; System.out.println(conn); conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; //查询所有方法 public &lt;E&gt; List&lt;E&gt; selectList(String statement) &#123; Mapper mapper = cfg.getMappers().get(statement); return new Executor().selectList(mapper, conn); &#125;&#125; 12.编写用于创建Dao接口代理对象的类1234567891011121314151617181920212223242526272829303132333435public class MapperProxyFactory implements InvocationHandler &#123; private Map&lt;String, Mapper&gt; mappers; private Connection conn; public MapperProxyFactory(Map&lt;String, Mapper&gt; mappers, Connection conn) &#123; this.mappers = mappers; this.conn = conn; &#125; /** * 对当前正在执行的方法进行增强 * 取出当前执行的方法名称 * 取出当前执行的方法所在类 * 拼接成 key * 去 Map 中获取 Value（Mapper) * 使用工具类 Executor 的 selectList 方法 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //1.取出方法名 String methodName = method.getName(); //2.取出方法所在类名 String className = method.getDeclaringClass().getName(); //3.拼接成 Key String key = className + &quot;.&quot; + methodName; //4.使用 key 取出 mapper Mapper mapper = mappers.get(key); if (mapper == null) &#123; throw new IllegalArgumentException(&quot;传入的参数有误，无法获取执行的必要条件&quot;); &#125; //5.创建 Executor 对象 Executor executor = new Executor(); return executor.selectList(mapper, conn); &#125;&#125; 13.修改为注解配置方式(1)Select注解 12345@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface Select &#123;String value();&#125; (2)IUserDao接口 12345678public interface IUserDao &#123;/*** 查询所有用户* @return*/@Select(&quot;select * from user&quot;)List&lt;User&gt; findAll();&#125; (3)SqlMapConfig配置 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;configuration&gt; &lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql:///mybatis?characterEncoding=utf8&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;rootroot&quot;&gt;&lt;/property&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;!--配置注解类地址--&gt; &lt;mapper class=&quot;aimu.maizhu.dao.IUserDao&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt;","categories":[],"tags":[]},{"title":"Mysql集群搭建入门学习","slug":"Mysql集群搭建入门学习","date":"2020-12-07T04:36:06.000Z","updated":"2020-12-07T14:06:28.716Z","comments":true,"path":"2020/12/07/Mysql集群搭建入门学习/","link":"","permalink":"http://example.com/2020/12/07/Mysql%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"Mysql集群搭建一、系统框架存在的问题我们的系统框架中，DBserver我们只使用了单节点服务，而面对高并发、海量数据的时候显然存在很严重的问题，所以我们要实现Mysql集群的搭建 二、Mysql集群搭建方案1.读写分离数据库要求一般“读多写少”，所以一个思路就是：一个主库负责写，成为写库；其他数据库负责读，成为读库。 实现的要求： 读库写库数据必须一致 读数据必须到读库 写数据必须到写库 图中的框架就是读写分离框架，该框架存在2个问题 应用程序需要连接多个数据库，增加开发难度 可以通过中间件来解决（Mycat） 如果在程序内部实现，可以使用Spring的AOP技术 主从之间的同步是异步实现的，所以是弱一致性 可能会出现数据写入写库中，读取时读不到数据，或者数据丢失等情况 采用PXC集群解决（强一致性，不分主从） 2.中间件上面框架的问题之一可以通过中间件来解决。 从框架中可以看出 应用程序只需连接到中间件即可 应用程序无需区分读写操作，对中间件读写操作即可 中间件区分读写操作，读操作到从节点，写操作到主节点 该框架也存在问题，中间件的性能成为了瓶颈，可以改造成下面的架构： 这样中间件性能就不会成为瓶颈，但是应用程序又要连接多个中间件，开发难度还是增加了 3.负载均衡为了解决以上问题，我们可以通过haproxy代理解决，由代理完成负载均衡功能。 至此，读写分离的高可用框架搭建成功。 4.PXC集群架构在前面的中间件只解决了主从分离的开发复杂问题，而并不能解决弱一致性问题，而PXC就是为了解决弱一致性问题的架构，它可以保证数据在任何一个节点写入的同时可以同步到其他节点，无延迟。 5.混合框架在前面的PXC架构中，虽然可以实现了事务的强一致性，但是它是通过牺牲了性能换来的一致性，如果在某些业务场景下，如果没有强一致性的需求，那么使用PXC就不合适了。所以，在我们的系统架构中，需要将这两种方式综合起来，这样才是一个较为完善的架构。 三、Mysql主从复制架构搭建1.主从复制原理：（master称为主，slave称为从） master将数据改变记录到二进制日志文件中，即配置文件log-bin指定的文件 slave将master的二进制日志文件拷贝到它的中继日志中 slave重做中继日志中的事件，将改变反映到自己的数据库中（数据重演） 主从配置需要注意的地方 主DBserver与从DBserver版本一致 主DBserver与从DBserver数据一致 主DBserver与从DBserver的server_id必须唯一 2.主库配置文件my.conf123456#开启主从复制，主库的配置 log-bin = mysql-bin #指定主库serverid server-id=1 #指定同步的数据库，如果不指定则同步全部数据库 binlog-do-db=my_test 3.主库创建同步用户1234#授权用户slave01使用123456密码登录mysql grant replication slave on *.* to &#39;slave01&#39;@&#39;127.0.0.1&#39; identified by &#39;123456&#39;; #刷新配置 flush privileges; 4.从库配置my.conf 12#指定serverid，只要不重复即可，从库也只有这一个配置，其他都在SQL语句中操作 server-id=2 sql 1234567891011CHANGE MASTER TO master_host&#x3D;&#39;127.0.0.1&#39;, #主库地址 master_user&#x3D;&#39;slave01&#39;, master_password&#x3D;&#39;123456&#39;, master_port&#x3D;3306, master_log_file&#x3D;&#39;mysql-bin.000006&#39;, #根据show master status查看 master_log_pos&#x3D;1120; #根据show master status查看 #启动slave同步 START SLAVE; #查看同步状态 SHOW SLAVE STATUS; 5.搭建主库12345678910111213141516171819202122232425262728#创建目录 mkdir /data/mysql/master01cd /data/mysql/master01 mkdir conf data chmod 777 * -R#创建配置文件 cd /data/mysql/master01/conf vim my.cnf#输入如下内容 [mysqld] log-bin=mysql-bin #开启二进制日志 server-id=1 #服务id，不可重复#创建容器 docker create --name percona-master01 -v /data/mysql/master01/data:/var/lib/mysql -v /data/mysql/master01/conf:/etc/my.cnf.d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=root percona:5.7.23#启动 docker start percona-master01 &amp;&amp; docker logs -f percona-master01#创建同步账户以及授权 create user &#x27;itcast&#x27;@&#x27;%&#x27; identified by &#x27;itcast&#x27;; grant replication slave on *.* to &#x27;itcast&#x27;@&#x27;%&#x27;; flush privileges;#查看master状态 show master status; 6.搭建从库1234567891011121314151617181920212223242526272829303132333435363738#创建目录 mkdir /data/mysql//slave01cd /data/mysql//slave01mkdir conf data chmod 777 * -R#创建配置文件 cd /data/mysql//slave01/conf vim my.cnf#输入如下内容 [mysqld] server-id=2 #服务id，不可重复#创建容器 docker create --name percona-/slave01 -v /data/mysql//slave01/data:/var/lib/mysql -v /data/mysql//slave01/conf:/etc/my.cnf.d -p 3307:3306 -e MYSQL_ROOT_PASSWORD=root percona:5.7.23#启动 docker start percona-slave01 &amp;&amp; docker logs -f percona-slave01#设置master相关信息 CHANGE MASTER TO master_host=&#x27;10.211.55.7&#x27;, master_user=&#x27;itcast&#x27;, master_password=&#x27;itcast&#x27;, master_port=3306, master_log_file=&#x27;mysql-bin.000002&#x27;, master_log_pos=648; #启动同步 start slave; #查看master状态 show slave status;#看到以下俩个参数为yes表示创建成功Slave_IO_Running: Yes Slave_SQL_Running: Yes 7.主从复制模式在MySQL中提供了有3种模式，基于SQL语句的复制(statement-based replication, SBR)，基于行的复制(row-based replication, RBR)，混合模式复制(mixed\u0002-based replication, MBR)。对应的，binlog的格式也有三种：STATEMENT，ROW，MIXED。 STATEMENT模式（SBR） 每一条会修改数据的sql语句会记录到binlog中。 优点是并不需要记录每一条sql语句和每一行的数据变化，减少了binlog日志量，节约IO，提高性能。 缺点是在某些情况下会导致master-slave中的数据不一致(如sleep()函数， last_insert_id()，以及user-defifined functions(udf)等会出现问题)（最简单的例子是date时间函数，创建出来的时间会不同） ROW模式（RBR） 不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了，修改成什么样了。而且不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题。缺点是会产生大量的日志，尤其是alter table的时候会让日志暴涨。 MIXED模式（MBR） 以上两种模式的混合使用，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog，MySQL会根据执行的SQL语句选择日志保存方式。 建议使用MIXED模式。 12345678#修改主库的配置 binlog_format=MIXED#重启 docker restart percona-master01 &amp;&amp; docker logs -f percona-master01#查看二进制日志相关的配置项 show global variables like &#x27;binlog%&#x27;;#发现binlog_format已经被改为MIXED模式了 四、Mycat中间件1.读写分离（一个主从库）server.xml 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mycat:server SYSTEM &quot;server.dtd&quot;&gt; &lt;mycat:server xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;system&gt; &lt;property name=&quot;nonePasswordLogin&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;useHandshakeV10&quot;&gt;1&lt;/property&gt; &lt;property name=&quot;useSqlStat&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;useGlobleTableCheck&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;sequnceHandlerType&quot;&gt;2&lt;/property&gt; &lt;property name=&quot;subqueryRelationshipCheck&quot;&gt;false&lt;/property&gt; &lt;property name=&quot;processorBufferPoolType&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;handleDistributedTransactions&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;useOffHeapForMerge&quot;&gt;1&lt;/property&gt; &lt;property name=&quot;memoryPageSize&quot;&gt;64k&lt;/property&gt; &lt;property name=&quot;spillsFileBufferSize&quot;&gt;1k&lt;/property&gt; &lt;property name=&quot;useStreamOutput&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;systemReserveMemorySize&quot;&gt;384m&lt;/property&gt; &lt;property name=&quot;useZKSwitch&quot;&gt;false&lt;/property&gt; &lt;/system&gt; &lt;!--这里是设置的itcast用户和虚拟逻辑库--&gt; &lt;user name=&quot;itcast&quot; defaultAccount=&quot;true&quot;&gt; &lt;property name=&quot;password&quot;&gt;itcast123&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;itcast&lt;/property&gt; &lt;/user&gt; &lt;/mycat:server&gt; schema.xml 123456789101112131415161718&lt;?xml version=&quot;1.0&quot;?&gt; &lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt; &lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;!--配置数据表--&gt; &lt;schema name=&quot;itcast&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot;&gt; &lt;table name=&quot;tb_ad&quot; dataNode=&quot;dn1&quot; rule=&quot;mod-long&quot; /&gt; &lt;/schema&gt; &lt;!--配置分片关系--&gt; &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;cluster1&quot; database=&quot;itcast&quot; /&gt; &lt;!--配置连接信息--&gt; &lt;dataHost name=&quot;cluster1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;3&quot; writeType=&quot;1&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=&quot;W1&quot; url=&quot;192.168.1.18:3306&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;readHost host=&quot;W1R1&quot; url=&quot;192.168.1.18:3307&quot; user=&quot;root&quot; password=&quot;root&quot; /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt; &lt;/mycat:schema&gt; balance属性说明： 负载均衡类型，目前的取值有3 种： balance=”0”, 不开启读写分离机制，所有读操作都发送到当前可用的writeHost 上。 balance=”1”，全部的readHost 与stand by writeHost 参与select 语句的负载均衡，简单的说，当双 主双从模式(M1-&gt;S1，M2-&gt;S2，并且M1 与M2 互为主备)，正常情况下，M2,S1,S2 都参与select 语句的负载均衡。 balance=”2”，所有读操作都随机的在writeHost、readhost 上分发。 balance=”3”，所有读请求随机的分发到wiriterHost 对应的readhost 执行，writerHost 不负担读压 力，注意balance=3 只在1.4 及其以后版本有，1.3 没有。 rule.xml 123&lt;function name=&quot;mod-long&quot; class=&quot;io.mycat.route.function.PartitionByMod&quot;&gt; &lt;property name=&quot;count&quot;&gt;1&lt;/property&gt; &lt;/function&gt; 连接数据库测试。。。 2.数据分片（多个主从库集群）主从库创建步骤略。。按主从复制的操作就行，只不过多创建几个集群。 有一点要注意：master01与master02的server_id可以一样，但在一个主从库中不能相同，比如master01与slave01中的server_id 配置schema.xml 12345678910111213141516171819202122232425262728&lt;?xml version=&quot;1.0&quot;?&gt; &lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt; &lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;!--配置数据表--&gt; &lt;schema name=&quot;itcast&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot;&gt; &lt;table name=&quot;tb_ad&quot; dataNode=&quot;dn1,dn2&quot; rule=&quot;mod-long&quot; /&gt; &lt;/schema&gt; &lt;!--配置分片关系--&gt; &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;cluster1&quot; database=&quot;itcast&quot; /&gt; &lt;dataNode name=&quot;dn2&quot; dataHost=&quot;cluster2&quot; database=&quot;itcast&quot; /&gt; &lt;!--配置连接信息--&gt; &lt;dataHost name=&quot;cluster1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;3&quot; writeType=&quot;1&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=&quot;W1&quot; url=&quot;192.168.1.18:3306&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;readHost host=&quot;W1R1&quot; url=&quot;192.168.1.18:3307&quot; user=&quot;root&quot; password=&quot;root&quot; /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt; &lt;dataHost name=&quot;cluster2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;3&quot; writeType=&quot;1&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=&quot;W2&quot; url=&quot;192.168.1.18:3316&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;readHost host=&quot;W2R1&quot; url=&quot;192.168.1.18:3317&quot; user=&quot;root&quot; password=&quot;root&quot; /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt; &lt;/mycat:schema&gt; rule.xml 123&lt;function name=&quot;mod-long&quot; class=&quot;io.mycat.route.function.PartitionByMod&quot;&gt; &lt;property name=&quot;count&quot;&gt;2&lt;/property&gt; &lt;!--cluster有两个所以设置为2--&gt;&lt;/function&gt; 连接测试。。。 3.Mycat集群Mycat02配置修改 123456789cp mycat mycat2 -R vim wrapper.conf #设置jmx端口 wrapper.java.additional.7=-Dcom.sun.management.jmxremote.port=1985 vim server.xml #设置服务端口以及管理端口 &lt;property name=&quot;serverPort&quot;&gt;8067&lt;/property&gt; &lt;property name=&quot;managerPort&quot;&gt;9067&lt;/property&gt; 五、负载均衡1.简介–自己百度在前面架构中，虽然对mycat做了集群，保障了mycat的可靠性，但是，应用程序需要连接到多个mycat，显然不是很友好的，也就是说缺少负载均衡的组件，接下来我们来了解下HAProxy。 官网：http://www.haproxy.org/ 2.部署安装123456#拉取镜像 docker pull haproxy:1.9.3 #创建目录，用于存放配置文件 mkdir /haoke/haproxy #创建容器 docker create --name haproxy --net host -v /haoke/haproxy:/usr/local/etc/haproxy haproxy:1.9.3 配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041#创建文件 vim /haoke/haproxy/haproxy.cfg#输入如下内容global log 127.0.0.1 local2 maxconn 4000 daemon defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 listen admin_stats bind 0.0.0.0:4001 mode http stats uri /dbs stats realm Global\\ statistics stats auth admin:admin123 listen proxy-mysql bind 0.0.0.0:4002 mode tcp balance roundrobin option tcplog #代理mycat服务 server mycat_1 192.168.1.18:8066 check port 8066 maxconn 2000 server mycat_2 192.168.1.18:8067 check port 8067 maxconn 2000 启动haproxy连接测试。。。通过web界面进行测试：http://192.168.1.18:4001/dbs 六、PXC集群1.简介Percona XtraDB Cluster（简称PXC）是针对MySQL用户的高可用性和扩展性解决方案，基于Percona Server 。其包括了Write Set REPlication补丁，使用Galera 2.0库，这是一个针对事务性应用程序的同步多主机复制插件。 Percona Server是MySQL的改进版本，使用 XtraDB 存储引擎，在功能和性能上较 MySQL 有着很显著的提升，如提升了在高负载情况下的 InnoDB 的性能，为 DBA 提供了一些非常有用的性能诊断工具，另外有更多的参数和命令来控制服务器行为。 官网：https://www.percona.com/software/mysql-database/percona-xtradb-cluster 2.架构 3.部署安装1234567891011121314151617181920212223#创建数据卷（存储路径：/var/lib/docker/volumes）docker volume create v1docker volume create v2docker volume create v3#拉取镜像docker pull percona/percona-xtradb-cluster:5.7#重命名 docker tag percona/percona-xtradb-cluster:5.7 pxc#创建网络 docker network create --subnet=172.30.0.0/24 pxc-network #创建容器#第一节点 docker create -p 13306:3306 -v v1:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -e CLUSTER_NAME=pxc --name=pxc_node1 --net=pxc-network --ip=172.30.0.2 pxc #第二节点（增加了CLUSTER_JOIN参数） docker create -p 13307:3306 -v v2:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -e CLUSTER_NAME=pxc --name=pxc_node2 -e CLUSTER_JOIN=pxc_node1 --net=pxc-network -- ip=172.30.0.3 pxc #第三节点（增加了CLUSTER_JOIN参数） docker create -p 13308:3306 -v v3:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -e CLUSTER_NAME=pxc --name=pxc_node3 -e CLUSTER_JOIN=pxc_node1 --net=pxc-network -- ip=172.30.0.4 pxc #查看集群节点 show status like &#x27;wsrep_cluster%&#x27;; 4.集群说明 尽可能的控制PXC的规模，节点越多，数据同步越慢 PXC集群的硬件设备性能要一致，否则配置低的节点会拖慢速度 只支持innoDB引擎，不支持其它存储引擎 5.PXC集群与Replication的区别 PXC集群方案所有节点都是可读可写的，Replication从节点不能写入，因为主从同步是单向的，无法从slave节点向master点同步。 PXC同步机制是同步进行的，这也是它能保证数据强一致性的根本原因，Replication同步机制是异步进行的，它如果从节点停止同步，依然可以向主节点插入数据，正确返回，造成数据主从数据的不一致性。 PXC是用牺牲性能保证数据的一致性，Replication在性能上是高于PXC的。所以两者用途也不一致。PXC是用于重要信息的存储，例如：订单、用户信息等。Replication用于一般信息的存储，能够容忍数据丢失，例如：购物车，用户行为日志等。 七、综合应用1.架构 说明： HAProxy作为负载均衡器 部署了2个Mycat节点作为数据库中间件 部署了2个PXC集群节点，作为2个Mycat分片，每个PXC集群中有2个节点，作为数据的同步存储 部署了1个主从复制集群 房源数据保存到PXC分片中，其余数据保存到主从架构中 2.部署PXC集群123456789101112131415161718192021222324252627#创建数据卷（存储路径：/var/lib/docker/volumes）docker volume create haoke-v1docker volume create haoke-v2docker volume create haoke-v3docker volume create haoke-v4#拉取镜像 docker pull percona/percona-xtradb-cluster:5.7 #创建网络 docker network create --subnet=172.30.0.0/24 pxc-network#创建容器#集群1，第一节点 docker create -p 13306:3306 -v haoke-v1:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -e CLUSTER_NAME=pxc --name=pxc_node1 --net=pxc-network --ip=172.30.0.2 pxc #第二节点（增加了CLUSTER_JOIN参数） docker create -p 13307:3306 -v haoke-v2:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -e CLUSTER_NAME=pxc --name=pxc_node2 -e CLUSTER_JOIN=pxc_node1 --net=pxc-network -- ip=172.30.0.3 pxc #集群2 #第一节点 docker create -p 13308:3306 -v haoke-v3:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -e CLUSTER_NAME=pxc --name=pxc_node3 --net=pxc-network --ip=172.30.0.4 pxc #第二节点（增加了CLUSTER_JOIN参数） docker create -p 13309:3306 -v haoke-v4:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -e CLUSTER_NAME=pxc --name=pxc_node4 -e CLUSTER_JOIN=pxc_node3 --net=pxc-network -- ip=172.30.0.5 pxc #启动 docker start pxc_node1 &amp;&amp; docker logs -f pxc_node1 docker start pxc_node2 &amp;&amp; docker logs -f pxc_node2 docker start pxc_node3 &amp;&amp; docker logs -f pxc_node3 docker start pxc_node4 &amp;&amp; docker logs -f pxc_node4 3.部署主从复制集群master 12345678910111213141516171819202122#创建目录 mkdir /data/mysql/haoke/master01 -p cd /data/mysql/haoke/master01 mkdir conf data chmod 777 * -R #创建配置文件 cd /data/mysql/haoke/master01/conf vim my.cnf #输入如下内容 [mysqld] log-bin=mysql-bin #开启二进制日志 server-id=1 #服务id，不可重复 #创建容器 docker create --name percona-haoke-master01 -v /data/mysql/haoke/master01/data:/var/lib/mysql -v /data/mysql/haoke/master01/conf:/etc/my.cnf.d -p 23306:3306 -e MYSQL_ROOT_PASSWORD=root percona:5.7.23 #启动 docker start percona-haoke-master01 &amp;&amp; docker logs -f percona-haoke-master01 #创建同步账户以及授权 create user &#x27;itcast&#x27;@&#x27;%&#x27; identified by &#x27;itcast&#x27;; grant replication slave on *.* to &#x27;itcast&#x27;@&#x27;%&#x27;; flush privileges; #查看master状态 show master status; slave 1234567891011121314151617181920212223242526#创建目录 mkdir /data/mysql/haoke/slave01 -p cd /data/mysql/haoke/slave01 mkdir conf data chmod 777 * -R #创建配置文件 cd /data/mysql/haoke/slave01/conf vim my.cnf #输入如下内容 [mysqld] server-id=2 #服务id，不可重复 #创建容器 docker create --name percona-haoke-slave01 -v /data/mysql/haoke/slave01/data:/var/lib/mysql -v /data/mysql/haoke/slave01/conf:/etc/my.cnf.d -p 23307:3306 -e MYSQL_ROOT_PASSWORD=root percona:5.7.23 #启动 docker start percona-haoke-slave01 &amp;&amp; docker logs -f percona-haoke-slave01 #设置master相关信息 CHANGE MASTER TO master_host=&#x27;192.168.1.18&#x27;, master_user=&#x27;itcast&#x27;, master_password=&#x27;itcast&#x27;, master_port=23306, master_log_file=&#x27;mysql-bin.000002&#x27;, master_log_pos=648; #启动同步 start slave;#查看master状态 show slave status; 4.部署Mycat配置节点一节点二，其中schema.xml的3个cluster配置注意一下。 server.xml 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mycat:server SYSTEM &quot;server.dtd&quot;&gt; &lt;mycat:server xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;system&gt; &lt;property name=&quot;nonePasswordLogin&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;useHandshakeV10&quot;&gt;1&lt;/property&gt; &lt;property name=&quot;useSqlStat&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;useGlobleTableCheck&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;sequnceHandlerType&quot;&gt;2&lt;/property&gt; &lt;property name=&quot;subqueryRelationshipCheck&quot;&gt;false&lt;/property&gt; &lt;property name=&quot;processorBufferPoolType&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;handleDistributedTransactions&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;useOffHeapForMerge&quot;&gt;1&lt;/property&gt; &lt;property name=&quot;memoryPageSize&quot;&gt;64k&lt;/property&gt; &lt;property name=&quot;spillsFileBufferSize&quot;&gt;1k&lt;/property&gt; &lt;property name=&quot;useStreamOutput&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;systemReserveMemorySize&quot;&gt;384m&lt;/property&gt; &lt;property name=&quot;useZKSwitch&quot;&gt;false&lt;/property&gt; &lt;/system&gt; &lt;!--这里是设置的itcast用户和虚拟逻辑库--&gt; &lt;user name=&quot;itcast&quot; defaultAccount=&quot;true&quot;&gt; &lt;property name=&quot;password&quot;&gt;itcast123&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;itcast&lt;/property&gt; &lt;/user&gt; &lt;/mycat:server&gt; schema.xml 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version=&quot;1.0&quot;?&gt; &lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt; &lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;!--配置数据表--&gt; &lt;schema name=&quot;haoke&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot;&gt; &lt;table name=&quot;tb_house_resources&quot; dataNode=&quot;dn1,dn2&quot; rule=&quot;mod-long&quot; /&gt; &lt;table name=&quot;tb_ad&quot; dataNode=&quot;dn3&quot;/&gt; &lt;/schema&gt; &lt;!--配置分片关系--&gt; &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;cluster1&quot; database=&quot;haoke&quot; /&gt; &lt;dataNode name=&quot;dn2&quot; dataHost=&quot;cluster2&quot; database=&quot;haoke&quot; /&gt; &lt;dataNode name=&quot;dn3&quot; dataHost=&quot;cluster3&quot; database=&quot;haoke&quot; /&gt; &lt;!--配置连接信息--&gt; &lt;dataHost name=&quot;cluster1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;2&quot; writeType=&quot;1&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=&quot;W1&quot; url=&quot;192.168.1.18:13306&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;readHost host=&quot;W1R1&quot; url=&quot;192.168.1.18:13307&quot; user=&quot;root&quot; password=&quot;root&quot; /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt; &lt;dataHost name=&quot;cluster2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;2&quot; writeType=&quot;1&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=&quot;W2&quot; url=&quot;192.168.1.18:13308&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;readHost host=&quot;W2R1&quot; url=&quot;192.168.1.18:13309&quot; user=&quot;root&quot; password=&quot;root&quot; /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt; &lt;dataHost name=&quot;cluster3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;3&quot; writeType=&quot;1&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=&quot;W3&quot; url=&quot;192.168.1.18:23306&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;readHost host=&quot;W3R1&quot; url=&quot;192.168.1.18:23307&quot; user=&quot;root&quot; password=&quot;root&quot; /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt; &lt;/mycat:schema&gt; 注意到cluster1与2的balance值为2，表示所有读操作都随机的在writeHost、readhost 上分发。体现了PXC集群方式，而cluster3的balance值为3，体现了读写分离集群方式 rule.xml 123&lt;function name=&quot;mod-long&quot; class=&quot;io.mycat.route.function.PartitionByMod&quot;&gt; &lt;property name=&quot;count&quot;&gt;2&lt;/property&gt; &lt;/function&gt; 设置端口并启动 12345678vim wrapper.conf #设置jmx端口 wrapper.java.additional.7=-Dcom.sun.management.jmxremote.port=11985 vim server.xml #设置服务端口以及管理端口 &lt;property name=&quot;serverPort&quot;&gt;18067&lt;/property&gt; &lt;property name=&quot;managerPort&quot;&gt;19067&lt;/property&gt; ./startup_nowrap.sh &amp;&amp; tail -f ../logs/mycat.log 节点二 123456789cp mycat-node1/ mycat-node2 -R vim wrapper.conf #设置jmx端口 wrapper.java.additional.7=-Dcom.sun.management.jmxremote.port=11986 vim server.xml #设置服务端口以及管理端口 &lt;property name=&quot;serverPort&quot;&gt;18068&lt;/property&gt; &lt;property name=&quot;managerPort&quot;&gt;19068&lt;/property&gt; ./startup_nowrap.sh &amp;&amp; tail -f ../logs/mycat.log 5.部署Haproxy配置haproxy.cfg成功进入界面并看到mycat两个节点都成功连接 。。。 至此Mysql集群的搭建算是基本完成。","categories":[],"tags":[]},{"title":"ElasticStack之kibana与Logstash入门","slug":"ElasticStack之kibana与Logstash入门","date":"2020-12-03T09:11:26.000Z","updated":"2020-12-03T09:59:36.662Z","comments":true,"path":"2020/12/03/ElasticStack之kibana与Logstash入门/","link":"","permalink":"http://example.com/2020/12/03/ElasticStack%E4%B9%8Bkibana%E4%B8%8ELogstash%E5%85%A5%E9%97%A8/","excerpt":"","text":"Kibana与Logstash入门学习搭建一、Kibana 官网：https://www.elastic.co/cn/products/kibana Kibana 是一款开源的数据分析和可视化平台，它是 Elastic Stack 成员之一，设计用于和 Elasticsearch 协作。您可以 使用 Kibana 对 Elasticsearch 索引中的数据进行搜索、查看、交互操作。您可以很方便的利用图表、表格及地图对 数据进行多元化的分析和呈现。 1.安装部署 两种方式： 本地安装 12345678#解压安装包tar -xvf kibana-6.5.4-linux-x86_64.tar.gz#修改配置文件vim config/kibana.ymlserver.host: &quot;192.168.1.7&quot; #对外暴露服务的地址 elasticsearch.url: &quot;http://192.168.1.7:9200&quot; #配置Elasticsearch docker安装 123456789#拉取镜像 docker pull kibana:6.5.4#创建配置文件 vim kibana.yml server.host: &quot;192.168.1.7&quot; #配置服务地址elasticsearch.url: &quot;http://192.168.1.7:9200&quot; #配置elasticsearch#创建容器 docker create --name kibana --net host -v /haoke/beats/kibana- docker/kibana.yml:/usr/share/kibana/config/kibana.yml kibana:6.5.4 #配置文件映射 Metricbeat仪表盘 1234#修改metricbeat配置 setup.kibana: host: &quot;192.168.1.7:5601&quot; #安装仪表盘到Kibana ./metricbeat setup --dashboards Filebeat仪表盘 123456#修改配置文件 添加kibana配置setup.kibana: host: &quot;192.168.1.7:5601&quot; #安装仪表盘到kibana ./filebeat -c haoke-redis.yml setup 二、Logstash 集中、转换、存储数据，主要用来处理数据 是一种管道的概念：分三步，输入、过滤、输出。 三、整合一个Filebeat输出日志的小项目–简单搭建ElasticStack Nginx启动–产生log文件–filebeat读取–发送到logstash–过滤后发送到elasticsearch 配置filebeat 123456789101112#vim haoke-nginx.yml filebeat.inputs: - type: log enabled: true paths: - /var/log/nginx/access.log tags: [&quot;log&quot;] fields: from: nginx fields_under_root: falseoutput.logstash: #输出到logstash而不输出到elasticsearch hosts: [&quot;192.168.1.7:5044&quot;] 配置logstash 1.修改nginx配置文件 1234567vim /etc/nginx/nginx.conflog_format main &#x27;$remote_addr - $remote_user [$time_local] &#x27; &#x27;&quot;$request&quot; $status $body_bytes_sent &#x27; &#x27;&quot;$http_referer&quot; &quot;$http_user_agent&quot;&#x27;;access_log /var/log/nginx/access.log main; 2.编写nginx-patterns文件 1NGINX_ACCESS %&#123;IPORHOST:remote_addr&#125; - %&#123;USERNAME:remote_user&#125; \\[% &#123;HTTPDATE:time_local&#125;\\] \\&quot;%&#123;DATA:request&#125;\\&quot; %&#123;INT:status&#125; %&#123;NUMBER:bytes_sent&#125; \\&quot;% &#123;DATA:http_referer&#125;\\&quot; \\&quot;%&#123;DATA:http_user_agent&#125;\\&quot; 3.修改haoke-pipeline.conf文件 12345678910111213141516171819input &#123; beats &#123; port &#x3D;&gt; &quot;5044&quot; &#125; &#125;filter &#123; grok &#123; patterns_dir &#x3D;&gt; &quot;&#x2F;haoke&#x2F;logstash-6.5.4&#x2F;nginx-patterns&quot; match &#x3D;&gt; &#123; &quot;message&quot; &#x3D;&gt; &quot;%&#123;NGINX_ACCESS&#125;&quot;&#125; remove_tag &#x3D;&gt; [ &quot;_grokparsefailure&quot; ] add_tag &#x3D;&gt; [ &quot;nginx_access&quot; ] &#125; &#125;output &#123; elasticsearch &#123; hosts &#x3D;&gt; [ &quot;192.168.1.7:9200&quot;,&quot;192.168.1.7:9201&quot;,&quot;192.168.1.7:9202&quot; ] &#125; &#125; 测试。。。成功！","categories":[],"tags":[]},{"title":"ElasticStack之Beats学习","slug":"ElasticStack之Beats学习","date":"2020-12-02T08:13:51.000Z","updated":"2020-12-02T09:30:45.863Z","comments":true,"path":"2020/12/02/ElasticStack之Beats学习/","link":"","permalink":"http://example.com/2020/12/02/ElasticStack%E4%B9%8BBeats%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"Beats–轻量型数据采集器官网地址：https://www.elastic.co/cn/products/beats 一、Filebeat–轻量型日志采集器1.工作原理： Harvester 负责读取单个文件的内容 文件被删除或者重命名，将继续读取该文件 Prospector 找到要读取文件的来源 管理harvester 若输入类型为日志类型，则查找路径匹配的所有文件，并为每一个文件分配启动一个harvester 目前支持两种类型：log和stdin filebeat如何保持文件状态（如何判断该读取文件的哪一行） filebeat会保存每个文件的状态并经常刷新到register中（filebeat目录下的register目录下） 该状态记录了harvester读取的最后一行在哪 如果输出无法访问（比如ealsticsearch宕机），filebeat会跟踪最后一行，输出可用时继续下一行发送 2.基本配置讲解123456789101112#创建如下配置文件 xxx.ymlfilebeat.inputs: - type: stdin #控制台输入 log ：日志 enabled: true setup.template.settings: index.number_of_shards: 3 #分片数3 output.console: #输出到控制台 pretty: true enable: true#启动filebeat./filebeat -e -c xxx.yml 123456789101112131415#创建如下配置文件 xxx.ymlfilebeat.inputs: - type: log #日志 enabled: true paths: - /haoke/beats/logs/*.log #自己需要采集的日志路径 tags: [&quot;web&quot;] #添加自定义tag，便于后续的处理 fields: #添加自定义字段 from: haoke-im fields_under_root: true #true为添加到根节点，false为添加到子节点中setup.template.settings: index.number_of_shards: 3 #分片数3 output.console: #输出到控制台 pretty: true enable: true 1234567891011121314#创建如下配置文件 xxx.ymlfilebeat.inputs: - type: log #日志 enabled: true paths: - /haoke/beats/logs/*.log #自己需要采集的日志路径 tags: [&quot;web&quot;] #添加自定义tag，便于后续的处理 fields: #添加自定义字段 from: haoke-im fields_under_root: true #true为添加到根节点，false为添加到子节点中setup.template.settings: index.number_of_shards: 3 #分片数3output.elasticsearch: #指定ES的配置 hosts: [&quot;192.168.1.7:9200&quot;,&quot;192.168.1.7:9201&quot;,&quot;192.168.1.7:9202&quot;] #如果是集群就可以配置多个地址 3.Module前面要想实现日志数据的读取以及处理都是自己手动配置的，其实，在Filebeat中，有大量的Module，可以简化我 们的配置，直接就可以使用 12345678910111213141516171819202122./filebeat modules listEnabled: #开启的moduleDisabled: #未开启的moduleapache2 auditd elasticsearch haproxy icinga iis kafka kibanalogstash mongodb mysqlnginx osquerypostgresqlredis suricata system traefik 以redis module为例示范如何开启redis module以及怎样修改配置 1234567./filebeat modules enable redis #启动./filebeat modules disable redis #禁用Enabled: redisDisabled: #... redis module 配置 123456789101112131415161718cd modules.d/vim redis.yml- module: redis # Main logs log: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. var.paths: [&quot;/data/redis-data/node01/*.log&quot;] #主要修改redis的log地址 # Slow logs, retrieved via the Redis API (SLOWLOG) slowlog: enabled: false # The Redis hosts to connect to. #var.hosts: [&quot;localhost:6379&quot;] #... redis默认情况下，是不会输出日志的，需要进行配置，前面我们使用的容器都没有配置日志输出，下面需要配置一 下。 12345docker create --name redis-node01 -v /data/redis-data/node01:/data -p 6379:6379 redis:5.0.2 --cluster-enabled yes --cluster-config-file nodes-node-01.conf --loglevel debug --logfile nodes-node-01.logdocker create --name redis-node02 -v /data/redis-data/node02:/data -p 6380:6379 redis:5.0.2 --cluster-enabled yes --cluster-config-file nodes-node-02.conf --loglevel debug --logfile nodes-node-02.logdocker create --name redis-node03 -v /data/redis-data/node03:/data -p 6381:6379 redis:5.0.2 --cluster-enabled yes --cluster-config-file nodes-node-03.conf --loglevel debug --logfile nodes-node-03.log loglevel 日志等级分为：debug、verbose、notice、warning 其中，debug 会有大量信息，对开发、测试有用； verbose 等于log4j 中的info，有很多信息，但是不会像debug那样乱； notice 一般信息； warning 只有非常重要/关键的消息被记录。 配置filebeat 123456789101112filebeat.inputs: - type: log #日志 enabled: true paths: - /haoke/beats/logs/*.log #自己需要采集的日志路径setup.template.settings: index.number_of_shards: 3 #分片数3output.elasticsearch: #指定ES的配置 hosts: [&quot;192.168.1.7:9200&quot;,&quot;192.168.1.7:9201&quot;,&quot;192.168.1.7:9202&quot;] #如果是集群就可以配置多个地址filebeat.config.modules: #加载module path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: false 二、Metricbeat–轻量型指标采集器用于从系统和服务中收集指标 Metricbeat有2部分组成 Module 收集的对象，如：mysql、操作系统等 Metricset 收集的指标集合，如：cpu，network，memory等 配置： 1234567891011121314vim metricbeat.ymlmetricbeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: falsesetup.template.settings: index.number_of_shards: 1 #分片数自己设置 index.codec: best_compressionsetup.kibana: output.elasticsearch: hosts: [&quot;192.168.1.7:9200&quot;,&quot;192.168.1.7:9201&quot;,&quot;192.168.1.7:9202&quot;] #自己的地址processors: - add_host_metadata: ~ - add_cloud_metadata: ~ Module使用，直接开启某module然后配置yml文件（如redis.yml ），设置集群的地址等，直接启动就行。","categories":[],"tags":[]},{"title":"elasticsearch入门学习","slug":"elasticsearch入门学习","date":"2020-11-26T13:29:10.000Z","updated":"2020-11-28T11:42:18.632Z","comments":true,"path":"2020/11/26/elasticsearch入门学习/","link":"","permalink":"http://example.com/2020/11/26/elasticsearch%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"ElasticStack之一elasticsearch入门一、介绍：Elasticsearch 基于java，是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。二、 docker安装：12345678#拉取镜像 docker pull elasticsearch:6.5.4 #某个版本#创建容器 docker create --name elasticsearch --net host -e &quot;discovery.type=single-node&quot; -e &quot;network.host=172.16.55.185&quot; elasticsearch:6.5.4 #虚拟机IP地址#启动 docker start elasticsearch #查看日志 docker logs elasticsearch 三、elasticsearch-head 提供界面管理工具四、Restful API123456789PUT http://172.16.55.185:9200/haoke #创建非结构化索引&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: &quot;2&quot;, &quot;number_of_replicas&quot;: &quot;0&quot; &#125; &#125;&#125; 1DELETE http://172.16.55.185:9200/haoke #删除索引 1234567POST http://172.16.55.185:9200/haoke/user/1001 #插入数据&#123; &quot;id&quot;: 1001, &quot;name&quot;: &quot;张三&quot;, &quot;age&quot;: 20, &quot;sex&quot;: &quot;男&quot;&#125; 1234567POST http://172.16.55.185:9200/haoke/user/ #不指定Id插入数据，id自动生成&#123; &quot;id&quot;: 1002, &quot;name&quot;: &quot;张阿三&quot;, &quot;age&quot;: 22, &quot;sex&quot;: &quot;男&quot;&#125; 123456789101112131415在Elasticsearch中，文档数据是不为修改的，但是可以通过覆盖的方式进行更新。PUT http://172.16.55.185:9200/haoke/user/1001&#123; &quot;id&quot;: 1001, &quot;name&quot;: &quot;张三&quot;, &quot;age&quot;: 22, &quot;sex&quot;: &quot;女&quot;&#125;可以局部更新，原因是在内部，依然会查询到这个文档数据，然后进行覆盖操作POST http://172.16.55.185:9200/haoke/user/1001/_update &#123; &quot;doc&quot;: &#123; &quot;age&quot;: 23 &#125;&#125; 1DELETE http://172.16.55.185:9200/haoke/user/1001 #删除数据 123GET http://172.16.55.185:9200/haoke/user/BbPe_WcB9cFOnF3uebvr #根据id查询数据GET http://172.16.55.185:9200/haoke/user/_search #搜索全部数据 (默认返回十条)GET http://172.16.55.185:9200/haoke/user/_search?q=age:20 #查询年龄等于20的用户 12345678910111213141516171819202122232425262728293031323334353637POST http://172.16.55.185:9200/haoke/user/_search #DSL搜索&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;age&quot;: 20 &#125; &#125;&#125;POST http://172.16.55.185:9200/haoke/user/_search &#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gt&quot;: 30 &#125; &#125; &#125;, &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;sex&quot;: &quot;男&quot; &#125; &#125; &#125; &#125;&#125;POST http://172.16.55.185:9200/haoke/user/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;张三 李四&quot; &#125; &#125;&#125; 12345678910111213POST http://172.16.55.185:9200/haoke/user/_search #高亮显示&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;张三 李四&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;name&quot;: &#123;&#125; &#125; &#125;&#125; 12345678910POST http://172.16.55.185:9200/haoke/user/_search #聚合&#123; &quot;aggs&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;&#125; 五、结构化查询123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263POST http://172.16.55.185:9200/itcast/person/_search#term查询：term 主要用于精确匹配哪些值，比如数字，日期，布尔值或 not_analyzed 的字符串(未经分析的文本数据类#型)：&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;age&quot;: 20 &#125; &#125;&#125;#terms查询：terms 跟 term 有点类似，但 terms 允许指定多个匹配条件。 如果某个字段指定了多个值，那么文档需要一#起去做匹配：&#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; &quot;age&quot;: [ 20, 21 ] &#125; &#125;&#125;#range查询：range 过滤允许我们按照指定范围查找一批数据：&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: 20, &quot;lte&quot;: 22 &#125; &#125; &#125;&#125;#exist查询：exists 查询可以用于查找文档中是否包含指定字段或没有某个字段，类似于SQL语句中的 IS_NULL 条件&#123; &quot;query&quot;: &#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;card&quot; &#125; &#125;&#125;#match查询：match 查询是一个标准查询，不管你需要全文本查询还是精确查询基本上都要用到它。#如果你使用 match 查询一个全文本字段，它会在真正查询之前用分析器先分析 match 一下查询字符：&#123; &quot;match&quot;: &#123; &quot;age&quot;: 26 &#125;&#125; &#123;&quot;match&quot;: &#123; &quot;date&quot;: &quot;2014-09-01&quot; &#125;&#125; &#123; &quot;match&quot;: &#123; &quot;public&quot;: true &#125;&#125; &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;full_text&quot; &#125;&#125;#bool查询：bool 查询可以用来合并多个条件查询结果的布尔逻辑，它包含一下操作符：#must :: 多个查询条件的完全匹配,相当于 and 。#must_not :: 多个查询条件的相反匹配，相当于 not 。 #should :: #至少有一个查询条件匹配, 相当于 or 。#这些参数可以分别继承一个查询条件或者一个查询条件的数组：&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;age&quot;: 20 &#125; &#125; &#125; &#125;&#125; 六、分词1234567891011POST http://172.16.55.185:9200/_analyze #分词api&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;hello world&quot;&#125;POST http://172.16.55.185:9200/itcast/_analyze #指定索引分词&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;field&quot;: &quot;hobby&quot;, #指定字段 &quot;text&quot;: &quot;听音乐&quot; &#125; 五种内置分词：Standard、Simple、Whitespace、Stop、Keyword了解 中文分词：ik、jieba、THULAC等，推荐ik分词器 安装ik分词器：将elasticsearch-analysis-ik的zip包解压到/elasticsearch/plugins/ik下即可 如果docker运行先复制进容器，然后进入容器解压，示例： 123456docker cp /tmp/elasticsearch-analysis-ik-6.5.4.zip elasticsearch:/usr/share/elasticsearch/plugins/docker exec -it elasticsearch /bin/bash mkdir /usr/share/elasticsearch/plugins/ik cd /usr/share/elasticsearch/plugins/ik unzip elasticsearch-analysis-ik-6.5.4.zip 七、全文搜索1.倒排索引:由于不是由记录来确定属性值，而是由属性值来确定记录的位置，因而称为倒排索引，简单来说就是由值找位置。全文搜索两个最重要的方面是： 相关性（Relevance） 它是评价查询与其结果间的相关程度，并根据这种相关程度对结果排名的一种能力，这 种计算方式可以是 TF/IDF 方法、地理位置邻近、模糊相似，或其他的某些算法。 分析（Analysis） 它是将文本块转换为有区别的、规范化的 token 的一个过程，目的是为了创建倒排索引以 及查询倒排索引。 2.单词搜索搜索过程说明： 检测字段类型：假设查看字段为”text”字段类型，意味着查询字符串本身也应该被分词 分析查询字符串：然后将查询字符串传入ik分词器，根据输出结果判断底层使用什么查询 查找匹配文档：查找相匹配的文档 为每个文档打分 3.多词搜索：发现一个问题：12345678910111213POST http://172.16.55.185:9200/itcast/person/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &quot;音乐 篮球&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 发现查询结果中包含了“音乐”、“篮球”的数据都已经被搜索到了，但我们想要的是两者都包含了的数据，Elasticsearch为我们提供了这样的逻辑关系 1234567891011121314POST http://172.16.55.185:9200/itcast/person/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &quot;音乐 篮球&quot;, &quot;operator&quot;:&quot;and&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 添加一个operator字段，值and表示两者都包含，or表示至少包含一个，而这是两种极端方式，在hobby有多个关键词时（大于2个）,在Elasticsearch中也支持这样的查询，通过minimum_should_match来指定匹配度，比如70%。 1234567891011121314POST http://172.16.55.185:9200/itcast/person/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &quot;音乐 篮球&quot;, &quot;minimum_should_match&quot;:&quot;70%&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 4.权重：有些时候，我们可能需要对某些词增加权重来影响该条数据的得分示例： 1234567891011121314151617181920212223242526272829303132333435363738POST http://172.16.55.185:9200/itcast/person/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;游泳篮球&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;音乐&quot;, &quot;boost&quot;: 10 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;跑步&quot;, &quot;boost&quot;: 2 &#125; &#125; &#125; ] &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; boost字段表示权重，根据自己需求设置。 5.短语匹配1234567891011121314&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;羽毛球篮球&quot; &#125; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 短语匹配意味着不仅仅是词要匹配，并且词的顺序也要一致 如果觉得这样太过于苛刻，可以增加slop参数，允许跳过N个词进行匹配 123456789101112131415&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;羽毛球足球&quot;, &quot;slop&quot;: 3 &#125; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 八、Elasticsearch集群1.集群节点 master节点 配置文件中node.master属性为true(默认为true)，就有资格被选为master节点。 master节点用于控制整个集群的操作。比如创建或删除索引，管理其它非master节点等。 data节点 配置文件中node.data属性为true(默认为true)，就有资格被设置成data节点。 data节点主要用于执行数据相关的操作。比如文档的CRUD。 客户端节点 配置文件中node.master属性和node.data属性均为false。 该节点不能作为master节点，也不能作为data节点。 可以作为客户端节点，用于响应用户的请求，把请求转发到其他节点 部落节点 当一个节点配置tribe.*的时候，它是一个特殊的客户端，它可以连接多个集群，在所有连接的集群上执 行搜索和其他操作。 2.分片和副本实际上，索引只是一个用来指向一个或多个分片的逻辑命名空间。 一个分片是最小的工作单元，一个lucene实例，它本身就是一个完整的搜索引擎，但是应用程序不会和他直接通信；分片包括主分片和复制分片，复制分片只是主分片的一个副本为了防止故障数据丢失的，同时提供读请求；每个文档都对应一个单独的主分片，所以主分片的数量决定存储数据大小 3.故障问题假如有三个节点，他们的配置都是node.master为true,minimum_master_nodes的大小为1。es-node1,es-node2.es-node3 node2为主节点时，停止node1时，主分片会转移到2，3两个节点，恢复后再转移回去 停止node2时，node3变为主节点，node2的主分片转移到1，3中，但是恢复node2后，并没有转移回来，而是出现脑裂现象，node2与node3都为master节点，所以出现问题。解决问题的办法：minimum_master_nodes的大小官方推荐：(N/2)+1，N为集群中节点数 4.分布式文档1.存储方式当向一个集群保存文档时，文档存储到节点的方式既不是随机的也不是轮询的，而是固定的计算公式 1shard = hash(routing) % number_of_primary_shards 2.文档的写和搜索新建、索引和删除请求都是写操作，写操作只能写在主分片中，而读操作可以在主分片和复制分片中都可以，为了平衡负载 3.全文搜索：全文搜索分为两个阶段：搜索和取回；1搜索：客户端发送请求给某一个节点，这一节点转发请求到每一个分片中，每一分片返回相同的固定长度的结果到开始的节点中，该节点把所以结果排序后得出最终结果。2取回：根据最终结果的信息再向相关分片发送请求获取结果中的文档详细信息，然后返回个客户端。 九、Java客户端1.REST客户端导入依赖测试： 1234567891011121314151617181920public class TestESREST &#123; private static final ObjectMapper MAPPER = new ObjectMapper(); private RestClient restClient; @Before public void init() &#123; RestClientBuilder restClientBuilder = RestClient.builder( new HttpHost(&quot;172.16.55.185&quot;, 9200, &quot;http&quot;), new HttpHost(&quot;172.16.55.185&quot;, 9201, &quot;http&quot;), new HttpHost(&quot;172.16.55.185&quot;, 9202, &quot;http&quot;)); this.restClient = restClientBuilder.build(); &#125; @After public void after() throws IOException &#123; restClient.close(); &#125; //测试代码。。。查询ealsticsearch接口文档&#125; 2.高级REST客户端12345678910111213141516171819public class TestRestHighLevel &#123; private RestHighLevelClient restClient; @Before public void init() &#123; RestClientBuilder restClientBuilder = RestClient.builder( new HttpHost(&quot;172.16.55.185&quot;, 9200, &quot;http&quot;), new HttpHost(&quot;172.16.55.185&quot;, 9201, &quot;http&quot;), new HttpHost(&quot;172.16.55.185&quot;, 9202, &quot;http&quot;)); this.restClient = new RestHighLevelClient(restClientBuilder); &#125; @After public void after() throws IOException &#123; restClient.close(); &#125; //测试代码。。。查询ealsticsearch接口文档&#125; 十、Spring Data Elasticsearch与spring整合导入依赖，编写properties文件，编写启动类。。 编写示例对象： 1234567891011121314@Data @AllArgsConstructor @NoArgsConstructor@Document(indexName = &quot;itcast&quot;, type = &quot;user&quot;, shards = 6, replicas = 1) //shards出现bug，创建后只有5个分片，可以改成自己创建索引，然后createindex设为falsepublic class User&#123; @Id private Long id; @Field(store = true) private String name; @Field private Integer age; @Field private String hobby;&#125; 新增数据 1234567891011121314151617@RunWith(SpringRunner.class) @SpringBootTest public class TestSpringBootES &#123; @Autowired private ElasticsearchTemplate elasticsearchTemplate; //spring最会玩的模版 @Test public void testSave()&#123; User user = new User(); user.setId(1001L); user.setAge(20); user.setName(&quot;张三&quot;); user.setHobby(&quot;足球、篮球、听音乐&quot;); IndexQuery indexQuery = new IndexQueryBuilder().withObject(user).build(); String index = this.elasticsearchTemplate.index(indexQuery); System.out.println(index); &#125; &#125; 批量插入数据 12345678910111213141516@Test public void testBulk() &#123; List list = new ArrayList(); for (int i = 0; i &lt; 5000; i++) &#123; User user = new User(); user.setId(1001L + i); user.setAge(i % 50 + 10); user.setName(&quot;张三&quot; + i); user.setHobby(&quot;足球、篮球、听音乐&quot;); IndexQuery indexQuery = new IndexQueryBuilder().withObject(user).build(); list.add(indexQuery); &#125; Long start = System.currentTimeMillis(); this.elasticsearchTemplate.bulkIndex(list); System.out.println(&quot;用时：&quot; + (System.currentTimeMillis() - start));//用时：4114ms &#125; 更新数据 12345678910@Test public void testUpdate() &#123; IndexRequest indexRequest = new IndexRequest(); indexRequest.source(&quot;age&quot;, &quot;30&quot;); UpdateQuery updateQuery = new UpdateQueryBuilder() .withId(&quot;1001&quot;) .withClass(User.class) .withIndexRequest(indexRequest).build(); this.elasticsearchTemplate.update(updateQuery); &#125; 搜索数据 123456789101112@Test public void testSearch() &#123; PageRequest pageRequest = PageRequest.of(1, 10); //设置分页参数 SearchQuery searchQuery = new NativeSearchQueryBuilder() .withQuery(QueryBuilders.matchQuery(&quot;name&quot;, &quot;张三&quot;)) // match查询 .withPageable(pageRequest) .build(); AggregatedPage&lt;User&gt; users = this.elasticsearchTemplate.queryForPage(searchQuery, User.class); System.out.println(&quot;总页数：&quot; + users.getTotalPages()); //获取总页数 for (User user : users.getContent()) &#123; // 获取搜索到的数据 System.out.println(user); &#125; &#125;","categories":[],"tags":[]},{"title":"关于RocketMQ的一些内容和特性","slug":"关于RocketMQ的一些内容和特性","date":"2020-11-01T07:11:07.000Z","updated":"2020-11-02T02:54:26.602Z","comments":true,"path":"2020/11/01/关于RocketMQ的一些内容和特性/","link":"","permalink":"http://example.com/2020/11/01/%E5%85%B3%E4%BA%8ERocketMQ%E7%9A%84%E4%B8%80%E4%BA%9B%E5%86%85%E5%AE%B9%E5%92%8C%E7%89%B9%E6%80%A7/","excerpt":"","text":"首先是发送消息分同步和异步， Producer的顺序消息：在某些业务中，consumer在消费消息时，是需要按照生产者发送消息的顺序进行消费的。 分布式消息：分布式事务分类有这几种： 基于单个JVM，数据库分库分表了（跨多个数据库）。 基于多JVM，服务拆分了（不跨数据库）。 基于多JVM，服务拆分了 并且数据库分库分表了。 采用半消息方式，发送消息后处理完本地事务再确认 执行流程 发送方向 MQ 服务端发送消息。 MQ Server 将消息持久化成功之后，向发送方 ACK 确认消息已经发送成功，此时消息为半消息。 发送方开始执行本地事务逻辑。 发送方根据本地事务执行结果向 MQ Server 提交二次确认（Commit 或是 Rollback），MQ Server 收到 Commit 状态则将半消息标记为可投递，订阅方最终将收到该消息；MQ Server 收到 Rollback 状态则删除半 消息，订阅方将不会接受该消息。 在断网或者是应用重启的特殊情况下，上述步骤4提交的二次确认最终未到达 MQ Server，经过固定时间后 MQ Server 将对该消息发起消息回查。 发送方收到消息回查后，需要检查对应消息的本地事务执行的最终结果。 发送方根据检查得到的本地事务的最终状态再次提交二次确认，MQ Server 仍按照步骤4对半消息进行操作。 Consumer的Push和Pullpush模式：客户端与服务端建立连接后，当服务端有消息时，将消息推送到客户端。 pull模式：客户端不断的轮询请求服务端，来获取新的消息。 底层都是pull，只不过push被封装到里面，pull需要自己实现 长轮询：发送一个pull等待服务器有消息再拉取，避免了一直pull造成压力 消息模式：集群模式和广播模式 重复消息：肯定存在的，自己解决，要么设置key相同的不处理，要么处理结果相同保持为一条。 RocketMQ存储RocketMQ中的消息数据存储，采用了零拷贝技术（使用 mmap + write 方式），文件系统采用 Linux Ext4 文件系 统进行存储。 RocketMQ消息的存储是由ConsumeQueue和CommitLog配合完成的，CommitLog是真正存储数据的文件， ConsumeQueue是索引文件，存储数据指向到物理文件的配置。 同步刷盘和异步刷盘 同步刷盘 在返回写成功状态时，消息已经被写入磁盘 。 具体流程是：消息写入内存的 PAGECACHE 后，立刻通知刷盘线程刷盘，然后等待刷盘完成，刷盘线程 执行完成后唤醒等待的线程，返回消息写成功的状态 。 异步刷盘 在返回写成功状态时，消息可能只是被写入了内存的 PAGECACHE，写操作的返回快，吞吐量大 当内存里的消息量积累到一定程度时，统一触发写磁盘动作，快速写入。","categories":[],"tags":[]},{"title":"关于RocketMQ发送消息报错","slug":"关于RocketMQ发送消息报错","date":"2020-10-31T05:43:14.000Z","updated":"2020-10-31T05:51:01.809Z","comments":true,"path":"2020/10/31/关于RocketMQ发送消息报错/","link":"","permalink":"http://example.com/2020/10/31/%E5%85%B3%E4%BA%8ERocketMQ%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E6%8A%A5%E9%94%99/","excerpt":"","text":"报错信息：Exception in thread “main” org.apache.rocketmq.remoting.exception.RemotingTooMuchRequestException: sendDefaultImpl call timeout 在虚拟机中rockermq配置完broker启动成功后信息为： 1The broker[itcast, 172.17.0.1:10911] boot success. serializeType&#x3D;JSON and name server is 172.16.185.55:9876 发现broker的ip是172.17.0.1，外部没办法访问所以必须把IP地址改掉，在虚拟机创建一个配置文件../conf/broker.conf 123brokerIP1=172.16.55.185 namesrvAddr=172.16.55.185:9876 brokerName=broker_haoke_im 启动时指定配置文件 -c 1bin&#x2F;mqbroker -c &#x2F;haoke&#x2F;rmq&#x2F;rmqbroker&#x2F;conf&#x2F;broker.conf 会发现成功信息变成如下 1The broker[itcast, 172.16.55.185:10911] boot success. serializeType&#x3D;JSON and name server is 172.16.55.185:9876 通常情况下这样就可以访问了 但是我的还是不行，原因是最容易被忽略的超时问题，把生产者的超时时间设置大一点就可以我这里设置15s 1producer.setSendMsgTimeout(15000);","categories":[],"tags":[]},{"title":"GraphQL学习之开发某服务接口实现","slug":"GraphQL学习之开发某服务接口实现","date":"2020-10-30T00:34:30.000Z","updated":"2020-11-01T01:06:38.776Z","comments":true,"path":"2020/10/30/GraphQL学习之开发某服务接口实现/","link":"","permalink":"http://example.com/2020/10/30/GraphQL%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%BC%80%E5%8F%91%E6%9F%90%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"1.导入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.graphql-java&lt;/groupId&gt; &lt;artifactId&gt;graphql-java&lt;/artifactId&gt; &lt;version&gt;11.0&lt;/version&gt; &lt;/dependency&gt; 2.编写haoke.graphqls文件 12345678910111213141516171819202122232425262728schema &#123; query: HaokeQuery &#125;type HaokeQuery &#123; HouseResources(id:Long):HouseResources &#125;type HouseResources&#123; id:Long! title:String estateId:Long buildingNum:String buildingUnit:String buildingFloorNum:String rent:Int rentMethod:Int paymentMethod:Int houseType:String coveredArea:String useArea:String floor:String orientation:String decoration:Int facilities:String pic:String houseDesc:String contact:String mobile:String time:Int propertyCost:String &#125; 3.编写GraphQLController 1234567891011@RequestMapping(&quot;graphql&quot;) //前端访问地址@Controller public class GraphQLController &#123; @Autowired private GraphQL graphQL; @GetMapping @ResponseBody public Map&lt;String, Object&gt; graphql(@RequestParam(&quot;query&quot;) String query) throws IOException &#123; return this.graphQL.execute(query).toSpecification(); &#125; &#125; 4.编写GraphQLProvider需要与SpringBoot整合，将GraphQL对象加载到Spring容器中 12345678910111213141516171819202122232425262728@Component public class GraphQLProvider &#123; private GraphQL graphQL; @Autowired private HouseResourcesService houseResourcesService; @PostConstruct public void init() throws IOException &#123; File file = ResourceUtils.getFile(&quot;classpath:haoke.graphqls&quot;); GraphQLSchema graphQLSchema = buildSchema(file); this.graphQL = GraphQL.newGraphQL(graphQLSchema).build(); &#125; private GraphQLSchema buildSchema(File file) &#123; TypeDefinitionRegistry typeRegistry = new SchemaParser().parse(file); RuntimeWiring runtimeWiring = buildWiring(); SchemaGenerator schemaGenerator = new SchemaGenerator(); return schemaGenerator.makeExecutableSchema(typeRegistry, runtimeWiring); &#125; private RuntimeWiring buildWiring() &#123; return RuntimeWiring.newRuntimeWiring() .type(&quot;HaokeQuery&quot;, builder -&gt; builder.dataFetcher(&quot;HouseResources&quot;, environment -&gt; &#123; Long id = environment.getArgument(&quot;id&quot;); return this.houseResourcesService.queryById(id); &#125; )).build(); &#125; @Bean public GraphQL graphQL() &#123; return graphQL; &#125; &#125; 以后每当增加查询时，都需要修改该方法，如果查询方法很多的话，那么这个方法将变得非常难以维护，所以需要 进改进。 编写MyDataFetcher接口 1234public interface MyDataFetcher &#123; String fieldName();//查询名称 Object dataFetcher(DataFetchingEnvironment environment);//具体实现查询的逻辑&#125; 编写实现类HouseResourcesDataFetcher 1234567891011121314@Component //加入到Spring容器 public class HouseResourcesDataFetcher implements MyDataFetcher &#123; @Autowired private HouseResourcesService houseResourcesService; @Override public String fieldName() &#123; return &quot;HouseResources&quot;; &#125; @Override public Object dataFetcher(DataFetchingEnvironment environment) &#123; Long id = environment.getArgument(&quot;id&quot;); return this.houseResourcesService.queryById(id); //实现了逻辑 &#125; &#125; 修改GraphQLProvider逻辑 12345678910111213141516171819202122232425262728293031@Component public class GraphQLProvider &#123; private GraphQL graphQL; @Autowired private List&lt;MyDataFetcher&gt; myDataFetchers; //注入容器中所有的MyDataFetcher实现类 @PostConstruct public void init() throws IOException &#123; File file = ResourceUtils.getFile(&quot;classpath:haoke.graphqls&quot;); GraphQLSchema graphQLSchema = buildSchema(file); this.graphQL = GraphQL.newGraphQL(graphQLSchema).build(); &#125; private GraphQLSchema buildSchema(File file) &#123; TypeDefinitionRegistry typeRegistry = new SchemaParser().parse(file); RuntimeWiring runtimeWiring = buildWiring(); SchemaGenerator schemaGenerator = new SchemaGenerator(); return schemaGenerator.makeExecutableSchema(typeRegistry, runtimeWiring); &#125; private RuntimeWiring buildWiring() &#123; return RuntimeWiring.newRuntimeWiring() .type(&quot;HaokeQuery&quot;, builder -&gt; &#123; for (MyDataFetcher myDataFetcher : myDataFetchers) &#123; builder.dataFetcher(myDataFetcher.fieldName(), environment -&gt; myDataFetcher.dataFetcher(environment)); &#125; return builder; &#125;).build(); &#125; @Bean public GraphQL graphQL() &#123; return graphQL; &#125; &#125; 后面如果要加业务逻辑之类的直接实现MyDataFetcher接口，把实现逻辑方法写一下，在Provider中通过myDataFetchers注入容器中所有的实现类，然后在方法中循环实现","categories":[],"tags":[]},{"title":"GraphQL学习之java实现","slug":"GraphQL学习之java实现","date":"2020-10-30T00:33:21.000Z","updated":"2020-10-30T01:40:11.312Z","comments":true,"path":"2020/10/30/GraphQL学习之java实现/","link":"","permalink":"http://example.com/2020/10/30/GraphQL%E5%AD%A6%E4%B9%A0%E4%B9%8Bjava%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"1.导入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.graphql-java&lt;/groupId&gt; &lt;artifactId&gt;graphql-java&lt;/artifactId&gt; &lt;version&gt;11.0&lt;/version&gt; &lt;/dependency&gt; 说明：graphql-java包并没有发布到maven中央仓库，需要配置第三方仓库才能使用。 在setting.xml文件里进行配置 12345678910111213141516171819202122232425262728&lt;profile&gt; &lt;id&gt;bintray&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;bintray&lt;/id&gt; &lt;url&gt;http://dl.bintray.com/andimarek/graphql-java&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;bintray&lt;/id&gt; &lt;url&gt;http://dl.bintray.com/andimarek/graphql-java&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; ………………………………………… &lt;activeProfiles&gt; ……………… &lt;activeProfile&gt;bintray&lt;/activeProfile&gt; &lt;/activeProfiles&gt; 创建user对象： 12345678910111213141516171819202122232425262728293031package cn.itcast.graphql.vo;public class User &#123; private Long id; private String name; private Integer age; public User() &#123; &#125; public User(Long id, String name, Integer age) &#123; this.id = id; this.name = name; this.age = age; &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; &#125; 编写查询User对象实现： 123456789101112#对应的User定义如下 schema &#123; #定义查询 query: UserQuery &#125;type UserQuery &#123; #定义查询的类型 user : User #指定对象以及参数类型 &#125;type User &#123; #定义对象 id:Long! # !表示该属性是非空项 name:String age:Int &#125; Java实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package cn.itcast.graphql.demo; import cn.itcast.graphql.vo.User; import graphql.ExecutionResult; import graphql.GraphQL; import graphql.schema.GraphQLFieldDefinition; import graphql.schema.GraphQLObjectType; import graphql.schema.GraphQLSchema; import graphql.schema.StaticDataFetcher; import static graphql.Scalars.*; import static graphql.schema.GraphQLFieldDefinition.newFieldDefinition; import static graphql.schema.GraphQLObjectType.newObject;public class GraphQLDemo &#123; /** * 定义Schema * &lt;p&gt; * schema &#123; #定义查询 * query: UserQuery * &#125; * * @return */ public static GraphQLSchema createGraphqlSchema(GraphQLFieldDefinition userDefinition) &#123; GraphQLObjectType userQuery = newObject().name(&quot;userQuery&quot;) .field(userDefinition).build(); return GraphQLSchema.newSchema().query(userQuery).build(); &#125; /** * 定义查询的类型 * &lt;p&gt; * type UserQuery &#123; #定义查询的类型 * user : User #指定对象 * &#125; * * @return */ public static GraphQLFieldDefinition createUserDefinition(GraphQLObjectType userType) &#123; return newFieldDefinition() .name(&quot;User&quot;) .type(userType) //静态数据 （和下面设置参数二选一） .dataFetcher(new StaticDataFetcher(new User(1L, &quot;张三&quot;, 20))) // 设置参数 （和上面静态数据二选一） .argument(newArgument().name(&quot;id&quot;).type(GraphQLLong).build()) .dataFetcher(environment -&gt; &#123; // environment是从前端传来的数据 Long id = environment.getArgument(&quot;id&quot;); return new User(id, &quot;张三_&quot;+id, 20 + id.intValue()); &#125;) .build(); &#125; /** * 定义User对象类型 * * type User &#123; #定义对象 * id:Long! # !表示该属性是非空项 * name:String * age:Int * &#125; * * * @return */ public static GraphQLObjectType createUserObjectType()&#123; return newObject() .name(&quot;User&quot;) .field(newFieldDefinition().name(&quot;id&quot;).type(GraphQLLong)) .field(newFieldDefinition().name(&quot;name&quot;).type(GraphQLString)) .field(newFieldDefinition().name(&quot;age&quot;).type(GraphQLInt)) .build(); &#125; //主函数测试 public static void main(String[] args) &#123; //得到userType放入GraphQLFieldDefinition中 GraphQLObjectType userObjectType = createUserObjectType(); //得到userDefinition放入GraphQLSchema中 GraphQLFieldDefinition userDefinition = createUserDefinition(userObjectType); //新建GraphQL对象 GraphQL graphQL = GraphQL.newGraphQL(createGraphqlSchema(userDefinition)).build(); //查询条件 String query = &quot;&#123;User&#123;id,name&#125;&#125;&quot;; //&#123;User(id:1)&#123;id,name&#125;&#125; 设置查询参数的查询条件 //执行 ExecutionResult executionResult = graphQL.execute(query); //打印数据 System.out.println... &#125;&#125; 使用SDL构建schema 推荐使用：SDL方法 创建user.graphqls文件 在resources目录下创建user.graphqls文件: 1234567891011schema &#123; query: UserQuery&#125;type UserQuery &#123; user(id:Long) : User &#125;type User &#123; id:Long! name:String age:Int &#125; 安装graphql插件 实现：因为在配置文件中，所以一定要读取配置文件得到相应的值 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class GraphQLSDLDemo &#123; /** * 读取文件内容 * * @param fileName * @return */ public static String readFileToString(String fileName)&#123; try &#123; return IOUtils.toString(GraphQLSDLDemo.class.getClassLoader().getResourceAsStream(fileName ), &quot;UTF-8&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; //定义Schema public static GraphQLSchema createGraphqlSchema(TypeDefinitionRegistry typeRegistry, RuntimeWiring wiring) &#123; SchemaGenerator schemaGenerator = new SchemaGenerator(); return schemaGenerator.makeExecutableSchema(typeRegistry, wiring); &#125; // 定义类型的注册器 public static TypeDefinitionRegistry createTypeDefinitionRegistry(String fileContent)&#123; SchemaParser schemaParser = new SchemaParser(); return schemaParser.parse(fileContent); &#125; //解决数据查询问题 public static RuntimeWiring createRuntimeWiring() &#123; return RuntimeWiring.newRuntimeWiring() .type(&quot;UserQuery&quot;, typeWiring -&gt; typeWiring .dataFetcher(&quot;user&quot;, environment -&gt; &#123; Long id = environment.getArgument(&quot;id&quot;); return new User(id, &quot;张三_&quot;+id, 20 + id.intValue()); &#125;) ).build(); &#125; public static void main(String[] args) &#123; String fileName = &quot;user.graphqls&quot;; TypeDefinitionRegistry registry=createTypeDefinitionRegistry(readFileToString(fileName)); RuntimeWiring runtimeWiring = createRuntimeWiring(); //新建GraphQl对象--createGraphqlSchema实现方式与第一次不同，两个参数 GraphQL graphQL = GraphQL.newGraphQL(createGraphqlSchema(registry, runtimeWiring)).build(); String query = &quot;&#123;user(id:1)&#123;id,name,age&#125;&#125;&quot;; ExecutionResult executionResult = graphQL.execute(query); System.out.println... &#125;&#125; 两种方式的数据都是静态数据，以后要自己实现查询语句。 第一种用java一步一步的写graphql语句一样，先定义查询，再定义查询类型，再定义对象类型； 第二种写配置文件，读取之后放入定义类型的注册器中，createGraphqlSchema还需要一个参数就是runtimeWiring解决数据查询问题","categories":[],"tags":[]},{"title":"GraphQL入门","slug":"GraphQL入门","date":"2020-10-29T23:57:42.000Z","updated":"2020-10-30T00:29:09.497Z","comments":true,"path":"2020/10/30/GraphQL入门/","link":"","permalink":"http://example.com/2020/10/30/GraphQL%E5%85%A5%E9%97%A8/","excerpt":"","text":"一、GraphQL介绍GraphQL 是由 Facebook 创造的用于描述复杂数据模型的一种查询语言。这里查询语言所指的并不是常规意义上 的类似 sql 语句的查询语言，而是一种用于前后端数据查询方式的规范。 二、GraphQL比Restful的优点解决Restful接口的资源浪费问题，因为当我们想要查询某请求的id和name字段，但是他还有很多字段我们不需要，如果全部拿到而只响应id和name那就是一种资源浪费。 123456789#请求 GET http://127.0.0.1/user/1001#响应： &#123; id : 1001, name : &quot;张三&quot;, age : 20, //不需要 address : &quot;北京市&quot;, //不需要 …… //不需要&#125; Restful接口的请求与响应 还有一种问题是Retful的一次请求不能满足需求，需要有多次请求才能完成，而GraphQL可以一次请求满足 123456789101112131415161718#查询用户信息 GET http://127.0.0.1/user/1001 #响应： &#123; id : 1001, name : &quot;张三&quot;, age : 20, address : &quot;北京市&quot;, …… &#125;#查询用户的身份证信息 GET http://127.0.0.1/card/8888 #响应： &#123; id : 8888, name : &quot;张三&quot;, cardNumber : &quot;999999999999999&quot;, address : &quot;北京市&quot;, …… &#125; Restful接口的多次请求满足响应需求 三、进一步了解GraphQL1.按需索取，避免浪费 2.一次查询多个数据 3.API演进无需划分版本 四、查询规范1.字段：查询和其结果拥有几乎一样的结构 2.参数：语法格式：（参数名：参数值） 3.别名：一次查询多个相同对象，但是值不同，要起别名 别名1：对象(){} 别名2：对象(){} 4.片段：查询对的属相如果相同，可以采用片段的方式进行简化定义 12345678910别名1：对象()&#123; ...片段名&#125;别名2：对象()&#123; ...片段名&#125;fragment 片段名 on Character&#123; 属性 ...&#125; 五、GraphQL的Schema和类型规范Schema定义结构： 1234567891011schema &#123; #定义查询 query: UserQuery &#125;type UserQuery &#123; #定义查询的类型 user(id:ID) : User #指定对象以及参数类型 &#125;type User &#123; #定义对象 id:ID! # !表示该属性是非空项 name:String age:Int &#125; 标量类型： Int ：有符号 32 位整数。 Float ：有符号双精度浮点值。 String ：UTF‐8 字符序列。 Boolean ： true 或者 false 。 ID ：ID 标量类型表示一个唯一标识符，通常用以重新获取对象或者作为缓存中的键。 接口：跟许多类型系统一样，GraphQL 支持接口。一个接口是一个抽象类型，它包含某些字段，而对象类型必须包含这 些字段，才能算实现了这个接口。","categories":[],"tags":[]},{"title":"JVM基本参数命令","slug":"JVM基本参数命令","date":"2020-10-28T02:47:25.000Z","updated":"2020-10-28T03:07:50.904Z","comments":true,"path":"2020/10/28/JVM基本参数命令/","link":"","permalink":"http://example.com/2020/10/28/JVM%E5%9F%BA%E6%9C%AC%E5%8F%82%E6%95%B0%E5%91%BD%E4%BB%A4/","excerpt":"","text":"JVM参数类型分三种：1.标准参数、2.-X参数、3.-XX参数（使用率高） 1234567标准参数常用命令：java -version 查看版本java -showversion 查看版本并退出java -D&lt;名称&gt;&#x3D;&lt;值&gt; 设置系统属性java -help 输出帮助消息java -server 选择server VM 默认就是serverjava -client 选择client VM -server与-client区别：ServerVM初始堆空间大，默认使用并行垃圾回收器，启动慢运行快，ClientVM初始堆空间小，使用串行垃圾回收器，启动更快，运行更慢。还有一点就是现在64位机器只支持server不支持client了。 12345-X参数常用命令：主要是三种模式-Xint：解释模式-Xcomp：编译模式-Xmixed：混合模式 -Xint模式会强制JVM执行所有字节码，运行速度低，-Xcomp使用时，会把所有代码编译成本地代码，-Xmixed是混合模式，是默认的，由JVM自己决定。-Xint是编译比较快而运行是比较慢的-Xcomp是编译比较慢但是运行是比较快的。 1234567-XX参数常用命令：两种方式，boolean类型与非boolean类型boolean：-XX：[+-]&lt;名称&gt; 表示禁止或启动某命令非boolean：-XX：NewRatio&#x3D;1 表示新生代与老年代的比值-Xms：JVM初始堆内存，等价于-XX:InitialHeapSize-XmX：JVm最大堆内存，等价于-XX:MaxHeapSize，-XX:+PrintFlagsFinal 查看jvm运行参数 、运行-XX：+PrintFlagsFinal查看运行参数时，出现=与:=两种形式，其中=表示原始值，而:=表示修改过的值。 12345678910111213jps -l 查看进程idjinfo -flags &lt;进程id&gt; 查看正在运行的jvm所有参数jinfo -flags &lt;参数名&gt; &lt;进程id&gt; 查看某个特定参数jstat 查看堆内存使用情况jstat -class &lt;pid&gt; 查看class加载统计jstat -complier &lt;pid&gt; 查看编译统计jstat -gc &lt;pid&gt; 查看垃圾回收统计jmap 对堆内存进行统计分析jmap -heap &lt;pid&gt; 查看内存使用情况jmap -histo &lt;pid&gt; | more 查看内存中所有对象数量及大小jmap -histo:live &lt;pid&gt; | more 查看内存中活跃对象的数量及大小jmap -dump:format&#x3D;b,file&#x3D;&#x2F;temp&#x2F;dumpdat &lt;pid&gt; 将内存使用情况dump到文件中jhat -port &lt;port&gt; &lt;file&gt; 用jhat将二进制dump文件打开并开启一个端口供访问分析 两个可视化工具，MAT，VisualVM。","categories":[],"tags":[]},{"title":"JVM垃圾回收和垃圾收集器","slug":"JVM垃圾回收和垃圾收集器","date":"2020-10-28T02:47:10.000Z","updated":"2020-10-28T03:07:26.962Z","comments":true,"path":"2020/10/28/JVM垃圾回收和垃圾收集器/","link":"","permalink":"http://example.com/2020/10/28/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%92%8C%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"java与C语音不同，C语音需要手动垃圾回收，而java只需要关心内存申请，不需要关心垃圾回收。 算法：：垃圾回收算法：引用计数法、标记清除法、标记压缩法、复制算法、分代算法等。引用计数法：每个对象都有一个计数器，每当对象被引用时就+1，不被引用就-1，最后都不引用的时候，计数器为0，这时如果进行垃圾回收，此对象会被回收掉。优点：实时性好，不用等内存满了再清理，计数器为0可以直接回收掉；缺点：更新数据会有一点时间开销，浪费cpu资源，最大的问题是无法解决循环引用问题。 标记清除法：从根节点开始标记引用的对象，未被标记的就回收掉，即使两个对象循环引用但是从根节点标记引用的对象没有这两个，所以会被回收掉。解决了循环引用无法回收的问题，这是优点，缺点是效率低下，每次都要遍历所有对象，对其进行标记和清除，并且清理后的内存碎片化严重 标记压缩法：标记清除法的改进，标记相同，清除时，先将存活的对象压缩到内存一端，然后清理掉其他对象，优点是解决了碎片化问题，但是多了一步移动对象的操作，效率也会低下。 复制算法：复制算法的核心就是，将原有的内存空间一分为二，每次只用其中的一块，在垃圾回收时，将正在使用的对象复制到另一个内存空间中，然后将该内存空间清空，交换两个内存的角色，完成垃圾的回收。值得一提的是jvm中的年轻代两个survivor区就是用的这样的算法。优点是垃圾对象多时，效率较高，内存无碎片化。缺点是垃圾对象少时，不适用，比如老年代，再就是一次只能用一半的内存空间内存使用率低。 分代算法：根据回收对象的特点进行选择，在jvm中，年轻代适合使用复制算法，老年代适合使用标记清除或标记压缩算法。 收集器：包括：串行垃圾收集器、并行垃圾收集器、CMS（并发）垃圾收集器、G1垃圾收集器。串行垃圾收集器：是指使用单线程进行垃圾回收，工作时只有一个线程，其他程序都要暂停，这种现象称为STW（Stop-The-World），对于交互性较强的应用而言，这种垃圾收集器是不能够接受的。一般在Javaweb应用中是不会采用该收集器的。因为交互性的应用暂停服务会影响使用效果。 并行垃圾收集器：将单线程改为了多线程进行垃圾回收，这样可以缩短垃圾回收的时间。但是还是要出现STW现象 ①ParNew垃圾收集器：只是单纯的把单线程改为多线程。 ②ParallelGC垃圾收集器：新增了两个和系统吞吐量相关的参数，使得其使用起来更加的灵活和高效。 CMS垃圾收集器：CMS以获取最小停顿时间为目的，这样就可以用在交互性的应用上。因为要获取最小停顿时间，所以对执行过程进行细化，比较复杂，下面是运行过程： 初始化标记(CMS-initial-mark) ,标记root，会导致stw； 并发标记(CMS-concurrent-mark)，与用户线程同时运行； 预清理（CMS-concurrent-preclean），与用户线程同时运行； 重新标记(CMS-remark) ，会导致stw； 并发清除(CMS-concurrent-sweep)，与用户线程同时运行； 调整堆大小，设置CMS在清理之后进行内存压缩，目的是清理内存中的碎片； 并发重置状态等待下次CMS的触发(CMS-concurrent-reset)，与用户线程同时运行； G1垃圾收集器：G1的设计原则就是简化JVM性能调优，开发人员只需要简单的三步即可完成调优：第一步，开启G1垃圾收集器第二步，设置堆的最大内存第三步，设置最大的停顿时间。 G1中提供了三种模式垃圾回收模式，Young GC、Mixed GC 和 Full GC，在不同的条件下被触发。 G1垃圾收集器的设计思想：与其他收集器相比，G1的最大区别是它摒弃了年轻代、老年代的物理划分，采用将堆内存分为若干个区域，这些区域包含了逻辑上的老年区，年轻区，还有一种特殊区域Humongous区，它主要装的是巨型对象。 在G1划分的区域中，年轻代的垃圾收集依然采用暂停所有应用线程的方式，将存活对象拷贝到老年代或者Survivor空间，G1收集器通过将对象从一个区域复制到另外一个区域，完成了清理工作。这就意味着，在正常的处理过程中，G1完成了堆的压缩（至少是部分堆的压缩），这样也就不会有cms内存碎片问题的存在了。 Young GC：主要对Eden区进行GC，在Eden区快耗尽时触发，存活数据移动到Survivor区，如果Survivor不够，则有部分移到年老代。Eden区数据为空时，停止GC。 Remembered Set（已记忆集合）其作用是跟踪指向某个堆内的对象引用。为找到年轻代的根对象采用这种方法，不然将年老代遍历一遍会很浪费时间。 Mixed GC：回收整个年轻代和部分年老代。触发是由参数 -XX:InitiatingHeapOccupancyPercent=n 决定。默认：45%，该参数的意思是：当老年代大小占整个堆大小百分比达到该阀值时触发。 123456789101112‐XX:+PrintGC 输出GC日志‐XX:+PrintGCDetails 输出GC的详细日志‐XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式）‐XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2013‐05‐04T21:53:59.234+0800）‐XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息‐Xloggc:..&#x2F;logs&#x2F;gc.log 日志文件的输出路径测试：‐XX:+UseG1GC ‐XX:MaxGCPauseMillis&#x3D;100 ‐Xmx256m ‐XX:+PrintGCDetails ‐XX:+PrintGCTimeStamps ‐XX:+PrintGCDateStamps ‐XX:+PrintHeapAtGC ‐Xloggc:F:&#x2F;&#x2F;test&#x2F;&#x2F;gc.log 最后介绍一个GC日志查看的网址：GC Easy是一款在线的可视化工具，易用、功能强大，网站：http://gceasy.io/","categories":[],"tags":[]},{"title":"Tomcat8优化入门","slug":"Tomcat8优化入门","date":"2020-10-28T02:46:53.000Z","updated":"2020-10-28T03:11:02.324Z","comments":true,"path":"2020/10/28/Tomcat8优化入门/","link":"","permalink":"http://example.com/2020/10/28/Tomcat8%E4%BC%98%E5%8C%96%E5%85%A5%E9%97%A8/","excerpt":"","text":"1.禁用AJP服务，默认开启占用8009端口 2.设置线程池，maxThreads：最大并发数，默认设置 200，一般建议在 500 ~ 1000；minSpareThreads：Tomcat 初始化时创建的线程数，默认设置 25；prestartminSpareThreads： 在 Tomcat 初始化的时候就初始化 minSpareThreads 的参数值，如果不等于 true，minSpareThreads 的值就没啥效果了 3.nio2运行模式，protocol=”org.apache.coyote.http11.Http11Nio2Protocol” 4.设置jvm垃圾回收器 前三个是对tomcat8自身优化，后面是对jvm参数调整。","categories":[],"tags":[]},{"title":"关于向数据库添加信息时未设置id报错","slug":"关于向数据库添加信息时未设置id报错","date":"2020-10-28T02:46:00.000Z","updated":"2020-10-28T03:13:50.130Z","comments":true,"path":"2020/10/28/关于向数据库添加信息时未设置id报错/","link":"","permalink":"http://example.com/2020/10/28/%E5%85%B3%E4%BA%8E%E5%90%91%E6%95%B0%E6%8D%AE%E5%BA%93%E6%B7%BB%E5%8A%A0%E4%BF%A1%E6%81%AF%E6%97%B6%E6%9C%AA%E8%AE%BE%E7%BD%AEid%E6%8A%A5%E9%94%99/","excerpt":"","text":"错误信息：ids for this class must be manually assigned before calling save(): sample.db.Completedsample 原因分析：从字面上理解的意思是，在save之前，必须手动指定id，其中id的； 解决办法：将主键设为自增，原先需要主键表示的数据，重新设置一个键。此时主键id的 ；之前主键没有设为自增，将id的generator的class设为increment也是同样的效果。","categories":[],"tags":[]},{"title":"记录一下一个小bug 关于Map集合的","slug":"记录一下一个小bug-关于Map集合的","date":"2020-10-28T02:44:55.000Z","updated":"2020-10-28T03:12:51.456Z","comments":true,"path":"2020/10/28/记录一下一个小bug-关于Map集合的/","link":"","permalink":"http://example.com/2020/10/28/%E8%AE%B0%E5%BD%95%E4%B8%80%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%B0%8Fbug-%E5%85%B3%E4%BA%8EMap%E9%9B%86%E5%90%88%E7%9A%84/","excerpt":"","text":"1234567891011121314151617181920212223@data//lombokpublic class ProfileResult&#123;//成员变量//...private Map&lt;String, Object&gt; map;//构造方法...String menus = &quot;&quot;;this.map.put(&quot;menus&quot;,menus);&#125;//-----------------------------@data//lombokpublic class ProfileResult&#123;//成员变量//...private Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();//构造方法...String menus = &quot;&quot;;this.map.put(&quot;menus&quot;,menus);&#125; 这两种写法，第一种会报错；因为map没有指向Map对象，报空指针异常；而第二种就在定义变量时赋值再调用方法时不会报异常。","categories":[],"tags":[]},{"title":"关于拦截器方法抛异常问题","slug":"关于拦截器方法抛异常问题","date":"2020-10-28T02:44:22.000Z","updated":"2020-10-28T03:12:02.082Z","comments":true,"path":"2020/10/28/关于拦截器方法抛异常问题/","link":"","permalink":"http://example.com/2020/10/28/%E5%85%B3%E4%BA%8E%E6%8B%A6%E6%88%AA%E5%99%A8%E6%96%B9%E6%B3%95%E6%8A%9B%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98/","excerpt":"","text":"前些日子写了个SpringMVC的拦截器方法preHandle最后判断否的时候抛出自定义异常，但是前端没有接收到，并报错自定义异常为null CommonException：null 然后查了一下，应该写一个异常处理类加@ControllerAdvice注解并扫描拦截器的包，方法是这样的： 1234567891011121314@ExceptionHandler(value = Exception.class)//异常处理注解 @ResponseBody//响应对象转化为json public Result error(HttpServletRequest request, HttpServletResponse response,Exception e) &#123; e.printStackTrace(); if(e.getClass() == CommonException.class) &#123;//接收到异常并判断是否为CommonException //类型转型 CommonException ce = (CommonException) e; Result result = new Result(ce.getResultCode());//返回result return result; &#125;else&#123; Result result = new Result(ResultCode.SERVER_ERROR); return result; &#125; &#125;","categories":[],"tags":[]},{"title":"Shrio安全框架入门","slug":"Shrio安全框架入门","date":"2020-10-28T02:44:05.000Z","updated":"2020-10-28T03:10:32.924Z","comments":true,"path":"2020/10/28/Shrio安全框架入门/","link":"","permalink":"http://example.com/2020/10/28/Shrio%E5%AE%89%E5%85%A8%E6%A1%86%E6%9E%B6%E5%85%A5%E9%97%A8/","excerpt":"","text":"作为安全框架shiro更加便捷简单，相比与Spring Security来说。 shiro主要使用的几个内部结构： Subject:主体，可以看到主体可以是任何可以与应用交互的“用户”; SecurityManager:相当于SpringMVC中的DispatcherServlet或者Struts2中的FilterDispatcher;是Shiro的心 脏;所有具体的交互都通过SecurityManager进行控制;它管理着所有Subject、且负责进行认证和授权、及会 话、缓存的管理。 Authenticator:认证器，负责主体认证的。 Authrizer:授权器，或者访问控制器，用来决定主体是否有权限进行相应的操作。 Realm:可以有1个或多个Realm，可以认为是安全实体数据源，即用于获取安全实体的;可以是JDBC实现，也可 以是LDAP实现，或者内存实现等等;由用户提供;注意:Shiro不知道你的用户/权限存储在哪及以何种格式存储; 所以我们一般在应用中都需要实现自己的Realm; 应用程序使用shiro过程 应用代码通过Subject来进行认证和授权，Subject委托给SecurityManage ,Realm被注入到SecurityManage中，Realm相当于从数据库获取安全数据。 1基于ini运行模式 12345678910111213141516171819202122232425262728293031323334 #模拟从数据库查询的用户 #数据格式 用户名=密码 zhangsan=123456 lisi=654321 //配置文件//认证 @Testpublic void testLogin() throws Exception&#123; //1.加载ini配置文件创建SecurityManager Factory&lt;SecurityManager&gt; factory = new IniSecurityManagerFactory(&quot;classpath:shiro.ini&quot;); //2.获取securityManagerSecurityManager securityManager = factory.getInstance(); //3.将securityManager绑定到当前运行环境 SecurityUtils.setSecurityManager(securityManager); //4.创建主体(此时的主体还为经过认证)Subject subject = SecurityUtils.getSubject();/*** 模拟登录，和传统等不同的是需要使用主体进行登录*///5.构造主体登录的凭证(即用户名/密码)//第一个参数:登录用户名，第二个参数:登录密码UsernamePasswordToken upToken = new UsernamePasswordToken(&quot;zhangsan&quot;,&quot;123456&quot;); //6.主体登录subject.login(upToken);//7.验证是否登录成功System.out.println(&quot;用户登录成功=&quot;+subject.isAuthenticated());//8.登录成功获取数据//getPrincipal 获取登录成功的安全数据System.out.println(subject.getPrincipal());&#125;//授权//配置文件 [users] #数据格式 用户名=密码,角色1,角色2.. zhangsan=123456,role1,role2 [roles] #数据格式 角色名=权限1，权限2 role1=user:save,user:update//7.用户认证成功之后才可以完成授权工作boolean hasPerm = subject.isPermitted(&quot;user:save&quot;);System.out.println(&quot;用户是否具有save权限=&quot;+hasPerm); 自定义realm 需要继承AuthorizingRealm父类 重写父类中的两个方法， doGetAuthorizationInfo :授权 从principals参数变量中获取已认证用户的信息，然后查询根据得到的id或者用户查询权限 doGetAuthenticationInfo :认证 认证的主要目的，比较用户输入的用户名密码是否和数据库中的一致，首先要把传入的变量强转为UsernamePasswordToken以获取用户名和密码 认证流程： 123451.Subject的login方法登录2.所有的Subject都委托于Security Manager管理3.Security Manager委托给Authenticator认证 它才是真正的身份验证，shiro API的核心身份认证入口，可以自定义实现4.Authenticator委托给AuthenticationStrategy进行Realm身份验证5.把token传入Realm中，获取身份验证信息 授权流程： 12341. 首先调用Subject.isPermitted&#x2F;hasRole接口，其会委托给SecurityManager，而SecurityManager接着会委托 给Authorizer;2. Authorizer是真正的授权者，如果我们调用如isPermitted(“user:view”)，其首先会通过PermissionResolver 把字符串转换成相应的Permission实例;3. 在进行授权之前，其会调用相应的Realm获取Subject相应的角色&#x2F;权限用于匹配传入的角色&#x2F;权限;4. Authorizer会判断Realm的角色&#x2F;权限是否和传入的匹配，如果有多个Realm，会委托给ModularRealmAuthorizer进行循环判断，如果匹配如isPermitted&#x2F;hasRole会返回true，否则返回false表示 授权失败。","categories":[],"tags":[]},{"title":"Shiro与SpringBoot整合","slug":"Shiro与SpringBoot整合","date":"2020-10-28T02:43:54.000Z","updated":"2020-10-28T03:05:19.967Z","comments":true,"path":"2020/10/28/Shiro与SpringBoot整合/","link":"","permalink":"http://example.com/2020/10/28/Shiro%E4%B8%8ESpringBoot%E6%95%B4%E5%90%88/","excerpt":"","text":"导入依赖 修改登录方法 自定义Realm Shiro配置 Shiro过滤器 授权 1基于配置授权 2基于注解授权 导入依赖: 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt; 修改登录方法：shiro需要采集到用户登录数据使用subject的login方法进入realm完成认证工作。 1234567891011 @RequestMapping(value=&quot;/login&quot;)public String login(String username,String password) &#123; try&#123; Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken uptoken = new UsernamePasswordToken(username,password); subject.login(uptoken); return &quot;登录成功&quot;; &#125; catch (Exception e) &#123; return &quot;用户名或密码错误&quot;; &#125;&#125; 自定义Realm：Shiro从Realm获取安全数据(如用户、角色、权限) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class CustomRealm extends AuthorizingRealm &#123; @Override public void setName(String name) &#123; super.setName(&quot;customRealm&quot;); &#125; @Autowired private UserService userService;/*** 构造授权方法 */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) &#123; //1.获取认证的用户数据 User user = (User)principalCollection.getPrimaryPrincipal(); //2.构造认证数据 SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); Set&lt;Role&gt; roles = user.getRoles(); for (Role role : roles) &#123; //添加角色信息 info.addRole(role.getName()); for (Permission permission:role.getPermissions()) &#123; //添加权限信息 info.addStringPermission(permission.getCode()); &#125; &#125; return info; &#125;/*** 认证方法 */ protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException &#123; //1.获取登录的upToken UsernamePasswordToken upToken = (UsernamePasswordToken)authenticationToken; //2.获取输入的用户名密码 String username = upToken.getUsername(); String password = new String(upToken.getPassword()); //3.数据库查询用户 User user = userService.findByName(username); //4.用户存在并且密码匹配存储用户数据 if(user != null &amp;&amp; user.getPassword().equals(password)) &#123; return new SimpleAuthenticationInfo(user,user.getPassword(),this.getName()); &#125;else &#123; //返回null会抛出异常，表明用户不存在或密码不匹配 return null; &#125; &#125;&#125; Shiro的配置：SecurityManager 是 Shiro 架构的心脏，用于协调内部的多个组件完成全部认证授权的过程，使用基于springboot的配置方式完成SecurityManager，Realm的装配。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Configurationpublic class ShiroConfiguration &#123; //配置自定义的Realm @Bean public CustomRealm getRealm() &#123; return new CustomRealm(); &#125; //配置安全管理器 @Bean public SecurityManager securityManager(CustomRealm realm) &#123; //使用默认的安全管理器 DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(realm); //将自定义的realm交给安全管理器统一调度管理 securityManager.setRealm(realm); return securityManager; &#125; //Filter工厂，设置对应的过滤条件和跳转条件 @Bean public ShiroFilterFactoryBean shirFilter(SecurityManager securityManager) &#123; //1.创建shiro过滤器工厂 ShiroFilterFactoryBean filterFactory = new ShiroFilterFactoryBean(); //2.设置安全管理器 filterFactory.setSecurityManager(securityManager); //3.通用配置(配置登录页面，登录成功页面，验证未成功页面) filterFactory.setLoginUrl(&quot;/autherror?code=1&quot;); //设置登录页面 filterFactory.setUnauthorizedUrl(&quot;/autherror?code=2&quot;); //授权失败跳转页面 //4.配置过滤器集合/*** key :访问连接 支持通配符的形式* value:过滤器类型 * shiro常用过滤器* anno :匿名访问(表明此链接所有人可以访问)* authc :认证后访问(表明此链接需登录认证成功之后可以访问)*/ Map&lt;String,String&gt; filterMap = new LinkedHashMap&lt;String,String&gt;(); // 配置不会被拦截的链接 顺序判断 filterMap.put(&quot;/user/home&quot;, &quot;anon&quot;); filterMap.put(&quot;/user/**&quot;, &quot;authc&quot;); //5.设置过滤器 filterFactory.setFilterChainDefinitionMap(filterMap); return filterFactory; &#125; //配置shiro注解支持 @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(SecurityManager securityManager) &#123; AuthorizationAttributeSourceAdvisor advisor = new AuthorizationAttributeSourceAdvisor(); · advisor.setSecurityManager(securityManager); return advisor; &#125; &#125; shiro中的过滤器: Filter 解释 anon 无参，开放权限，可以理解为匿名用户或游客 authc 无参，需要认证 logout 无参，注销，执行后会直接跳转到 shiroFilterFactoryBean.setLoginUrl(); 设置的 url authcBasic 无参，表示 httpBasic 认证 user 无参，表示必须存在用户，当登入操作时不做检查 ssl 无参，表示安全的URL请求，协议为 https perms[user] 参数可写多个，表示需要某个或某些权限才能通过，多个参数时写 perms[“user, admin”]，当有多个参数时必须每个参数都通过才算通过 roles[admin] 参数可写多个，表示是某个或某些角色才能通过，多个参数时写 roles[“admin，user”]， 当有多个参数时必须每个参数都通过才算通过 rest[user] 根据请求的方法，相当于 perms[user:method]，其中 method 为 post，get，delete 等 port[8081] 当请求的URL端口不是8081时，跳转到当前访问主机HOST的8081端口 注意:anon, authc, authcBasic, user 是第一组认证过滤器，perms, port, rest, roles, ssl 是第二组授权过滤 器，要通过授权过滤器，就先要完成登陆认证操作(即先要完成认证才能前去寻找授权) 才能走第二组授权器 (例如访问需要 roles 权限的 url，如果还没有登陆的话，会直接跳转到 shiroFilterFactoryBean.setLoginUrl(); 设置的 url ) 基于配置的授权 123456789//配置请求连接过滤器配置//匿名访问(所有人员可以使用) filterMap.put(&quot;/user/home&quot;, &quot;anon&quot;); //具有指定权限访问filterMap.put(&quot;/user/find&quot;, &quot;perms[user-find]&quot;); //认证之后访问(登录之后可以访问) filterMap.put(&quot;/user/**&quot;, &quot;authc&quot;); //具有指定角色可以访问filterMap.put(&quot;/user/**&quot;, &quot;roles[系统管理员]&quot;); 基于注解的授权 123456789(1)RequiresPermissions //查询@RequiresPermissions(value = &quot;user-find&quot;) public String find() &#123;return &quot;查询用户成功&quot;; &#125;(2)RequiresRoles //查询@RequiresRoles(value = &quot;系统管理员&quot;) public String find() &#123;return &quot;查询用户成功&quot;; &#125;","categories":[],"tags":[]},{"title":"Shiro中的会话管理（怎样创建自己维护的会话）","slug":"Shiro中的会话管理（怎样创建自己维护的会话）","date":"2020-10-28T02:43:40.000Z","updated":"2021-01-07T06:37:40.219Z","comments":true,"path":"2020/10/28/Shiro中的会话管理（怎样创建自己维护的会话）/","link":"","permalink":"http://example.com/2020/10/28/Shiro%E4%B8%AD%E7%9A%84%E4%BC%9A%E8%AF%9D%E7%AE%A1%E7%90%86%EF%BC%88%E6%80%8E%E6%A0%B7%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%BB%B4%E6%8A%A4%E7%9A%84%E4%BC%9A%E8%AF%9D%EF%BC%89/","excerpt":"","text":"shiro提供了三个默认实现： DefaultSessionManager:用于JavaSE环境 ServletContainerSessionManager:用于Web环境，直接使用servlet容器的会话。 DefaultWebSessionManager:用于web环境，自己维护会话(自己维护着会话，直接废弃了Servlet容器的会话管理)。 在web程序中，通过shiro的Subject.login()方法登录成功后，用户的认证信息实际上是保存在HttpSession中的 Shiro结合redis的统一会话管理 构建环境 123456//导入依赖&lt;dependency&gt; &lt;groupId&gt;org.crazycake&lt;/groupId&gt; &lt;artifactId&gt;shiro-redis&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt;&lt;/dependency&gt; 1234//在springboot配置文件中添加redis配置 redis: host: 127.0.0.1 port: 6379 自定义shiro会话管理器 12345678910111213141516171819202122232425/*** 自定义的sessionManager 继承DefaultWebSessionManager*/public class CustomSessionManager extends DefaultWebSessionManager &#123;/*** 头信息中具有sessionid* 请求头:Authorization: sessionid ** 指定sessionId的获取方式*/ protected Serializable getSessionId(ServletRequest request, ServletResponse response) &#123; //获取请求头Authorization中的数据 String id = WebUtils.toHttp(request).getHeader(&quot;Authorization&quot;); if(StringUtils.isEmpty(id)) &#123; //如果没有携带，生成新的sessionId return super.getSessionId(request,response); &#125; else &#123; //返回sessionId; request.setAttribute(ShiroHttpServletRequest.REFERENCED_SESSION_ID_SOURCE,&quot;header&quot;); request.setAttribute(ShiroHttpServletRequest.REFERENCED_SESSION_ID, id); request.setAttribute(ShiroHttpServletRequest.REFERENCED_SESSION_ID_IS_VALID, Boolean.TRUE); &#125; &#125; return id; &#125; 配置Shiro基于redis的会话管理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950511. 配置shiro的RedisManager，通过shiro-redis包提供的RedisManager统一对redis操作 @Value(&quot;$&#123;spring.redis.host&#125;&quot;) private String host;@Value(&quot;$&#123;spring.redis.port&#125;&quot;) private int port;//配置shiro redisManagerpublic RedisManager redisManager() &#123; RedisManager redisManager = new RedisManager(); redisManager.setHost(host); redisManager.setPort(port);return redisManager;&#125;2. Shiro内部有自己的本地缓存机制，为了更加统一方便管理，全部替换redis实现 //配置Shiro的缓存管理器//使用redis实现public RedisCacheManager cacheManager() &#123; RedisCacheManager redisCacheManager = new RedisCacheManager(); redisCacheManager.setRedisManager(redisManager()); return redisCacheManager;&#125;3. 配置SessionDao，使用shiro-redis实现的基于redis的sessionDao /*** RedisSessionDAO shiro sessionDao层的实现 通过redis * 使用的是shiro-redis开源插件*/public RedisSessionDAO redisSessionDAO() &#123; RedisSessionDAO redisSessionDAO = new RedisSessionDAO(); redisSessionDAO.setRedisManager(redisManager()); return redisSessionDAO;&#125;4. 配置会话管理器，指定sessionDao的依赖关系 /*** 3.会话管理器 */public DefaultWebSessionManager sessionManager() &#123; CustomSessionManager sessionManager = new CustomSessionManager(); sessionManager.setSessionDAO(redisSessionDAO()); return sessionManager;&#125;5. 统一交给SecurityManager管理 //配置安全管理器@Beanpublic SecurityManager securityManager(CustomRealm realm) &#123; //使用默认的安全管理器 DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(realm); // 自定义session管理 使用redis securityManager.setSessionManager(sessionManager()); // 自定义缓存实现 使用redis securityManager.setCacheManager(cacheManager()); //将自定义的realm交给安全管理器统一调度管理 securityManager.setRealm(realm); return securityManager;&#125;","categories":[],"tags":[]},{"title":"导入依赖出现循环报错","slug":"导入依赖出现循环报错","date":"2020-10-28T02:43:22.000Z","updated":"2020-10-28T02:58:42.387Z","comments":true,"path":"2020/10/28/导入依赖出现循环报错/","link":"","permalink":"http://example.com/2020/10/28/%E5%AF%BC%E5%85%A5%E4%BE%9D%E8%B5%96%E5%87%BA%E7%8E%B0%E5%BE%AA%E7%8E%AF%E6%8A%A5%E9%94%99/","excerpt":"","text":"Error:java: Annotation processing is not supported for module cycles. Please ensure that all modules 分析：cycle [qrcode-common,qrcode-manager-pojo] ：从这里可以看出，qrcode-common,qrcode-manager-pojo这两个模块有问题，即互相依赖，类似死循环","categories":[],"tags":[]},{"title":"使用umi build出现的Path must be a string恶心解决方法","slug":"使用umi-build出现的Path-must-be-a-string恶心解决方法","date":"2020-10-28T02:42:09.000Z","updated":"2020-10-28T02:58:09.004Z","comments":true,"path":"2020/10/28/使用umi-build出现的Path-must-be-a-string恶心解决方法/","link":"","permalink":"http://example.com/2020/10/28/%E4%BD%BF%E7%94%A8umi-build%E5%87%BA%E7%8E%B0%E7%9A%84Path-must-be-a-string%E6%81%B6%E5%BF%83%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/","excerpt":"","text":"解决方法：按照官网升级umi-plugin-react的版本就完事，很多博客就写了个简单的文字叙述，具体怎么做也没做具体说明，出现这个错误的原因应该是umi插件版本与umi的版本有冲突，现在直接使用命令安装umi的话，系统都会默认给你安装最新版umi3，而umi3已经对原来的插件进行了更新，所以如果你的umi是最新版的，就要参考官方文档最新版的配置方式，不能一味地使用原来的配置，否则会一直报错。技术更新快，几个月前的新技术，在今天可能已经更新几个版本了，所以有问题，记得多看官方说明文档。 改完一定要重新导入依赖才能进行build！！！ 官网：https://umijs.org/docs/upgrade-to-umi-3#%E5%8D%87%E7%BA%A7-umi-plugin-react-%E4%B8%BA-umijspreset-react","categories":[],"tags":[]},{"title":"docker创建启动percona(mysql)闪退问题","slug":"docker创建启动percona-mysql-闪退问题","date":"2020-10-28T02:41:31.000Z","updated":"2020-10-28T02:57:23.057Z","comments":true,"path":"2020/10/28/docker创建启动percona-mysql-闪退问题/","link":"","permalink":"http://example.com/2020/10/28/docker%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8percona-mysql-%E9%97%AA%E9%80%80%E9%97%AE%E9%A2%98/","excerpt":"","text":"#创建容器docker create –name percona -v /data/mysql-data:/var/lib/mysql -p 3306:3306 -eMYSQL_ROOT_PASSWORD=root percona:5.7.23 docker start percona 直接闪退 因为/data目录没有访问权限 开启权限 chomd -R 777 data","categories":[],"tags":[]},{"title":"关于SpringBoot启动某Service实现类报错发现两个实例问题","slug":"关于SpringBoot启动某Service实现类报错发现两个实例问题","date":"2020-10-24T09:08:25.000Z","updated":"2020-10-28T02:50:26.087Z","comments":true,"path":"2020/10/24/关于SpringBoot启动某Service实现类报错发现两个实例问题/","link":"","permalink":"http://example.com/2020/10/24/%E5%85%B3%E4%BA%8ESpringBoot%E5%90%AF%E5%8A%A8%E6%9F%90Service%E5%AE%9E%E7%8E%B0%E7%B1%BB%E6%8A%A5%E9%94%99%E5%8F%91%E7%8E%B0%E4%B8%A4%E4%B8%AA%E5%AE%9E%E4%BE%8B%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题如下 1ServiceImpl required a single bean, but 2 were found; 原因是写了两个实现类继承某父类，而其中一个写了泛型，一个没写所以报错，部分代码如下： 123456789101112131415@Servicepublic class AdServiceImpl extends BaseServiceImpl implements AdService &#123; //分页查询广告 @Override public PageInfo&lt;Ad&gt; queryAdList(Ad ad, Integer page, Integer pageSize) &#123; //逻辑代码 //返回值 &#125;&#125;@Service@Transactionalpublic class HouseResourcesServiceImpl extends BaseServiceImpl&lt;HouseResources&gt; implements HouseResourcesService &#123; //方法代码&#125; 可以看出第一个实现类继承的BaseServiceImpl没有泛型，第二个存在，然后就报错了。改为对应的泛型就好了。","categories":[],"tags":[]},{"title":"关于SpringBoot启动报错DataSource问题","slug":"关于SpringBoot启动报错DataSource问题","date":"2020-10-24T09:07:05.000Z","updated":"2020-10-24T09:54:44.642Z","comments":true,"path":"2020/10/24/关于SpringBoot启动报错DataSource问题/","link":"","permalink":"http://example.com/2020/10/24/%E5%85%B3%E4%BA%8ESpringBoot%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99DataSource%E9%97%AE%E9%A2%98/","excerpt":"","text":"Springboot启动时报错 If you want an embedded database (H2, HSQL or Derby), please put it on the classpath.产生这个错误的原因是springboot的自动配置，如果你没有配置DataSource就会导致这个错误 版本一：解决方法 1@SpringBootApplication(exclude = DataSourceAutoConfiguration.class)//排除自动配置 关于exclude属性和DataSource问题 exclude属性是@SpringBootApplication注解中的@EnableAutoConfiguration属性 排除某个类的自动配置 在本例中，datasource没有配置所以启动时报错 版本二：上述问题又出现了一次这次按版本一没有成功，在网上找了很多答案, 其中的一篇:https://www.cnblogs.com/yourGod/p/9178515.html 按照上述的解决办法:) 版本一这样之后还会报错： 12345Description:Field userRepository in com.wcyq.demo.service.impl.UserServiceImpl required a bean of type &#39;com.wcyq.demo.domain.UserRepository&#39; that could not be found.Action:Consider defining a bean of type &#39;com.wcyq.demo.domain.UserRepository&#39; in your configuration.Process finished with exit code 1 这样dao就找不到了 最后解决办法是application.yml文件配置改变一下 123456789101112131415161718spring:application:name: ihrm-companydatasource:url: jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;ihrm?characterEncoding&#x3D;utf-8username: rootpassword: rootdriverClassName: com.mysql.jdbc.Driver改为下面的 加了druid连接spring:application:name: ihrm-companydatasource:druid:url: jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;ihrm?characterEncoding&#x3D;utf-8username: rootpassword: rootdriverClassName: com.mysql.jdbc.Driver 是yml文件出现了问题，在此记录一下 最后总结一下，导入了和数据库连接的依赖就要考虑你的datasource问题，如果需要就在配置文件中配置好，SpringBoot会自动导入；如果不需要，把exclude写上，或者依赖删掉。","categories":[],"tags":[]},{"title":"关于Spring组件未加人到容器报错问题","slug":"关于Spring组件未加人到容器报错问题","date":"2020-10-21T07:12:27.000Z","updated":"2020-10-21T07:13:28.583Z","comments":true,"path":"2020/10/21/关于Spring组件未加人到容器报错问题/","link":"","permalink":"http://example.com/2020/10/21/%E5%85%B3%E4%BA%8ESpring%E7%BB%84%E4%BB%B6%E6%9C%AA%E5%8A%A0%E4%BA%BA%E5%88%B0%E5%AE%B9%E5%99%A8%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98/","excerpt":"","text":"123问题描述：The injection point has the following annotations:- @org.springframework.beans.factory.annotation.Autowired(required&#x3D;true)Action: 原因是自己在写项目时不小心把Spring的Service组件忘记加注解了，导致没有导入到spring容器中，而另一个类引用了该组件，启动时找不到，所以报错。 以后出现这种错误可以猜测到是Spring的某个@Autowired注解下的组件未导入到容器中。。 在此记录一下","categories":[],"tags":[]},{"title":"第一篇hexo博客","slug":"第一篇hexo博客","date":"2020-10-21T07:06:37.000Z","updated":"2020-10-21T07:10:58.897Z","comments":true,"path":"2020/10/21/第一篇hexo博客/","link":"","permalink":"http://example.com/2020/10/21/%E7%AC%AC%E4%B8%80%E7%AF%87hexo%E5%8D%9A%E5%AE%A2/","excerpt":"","text":"第一篇hexo文章， 以后就在这里写博客。。 加油0.0","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-10-21T05:14:20.539Z","updated":"2020-10-21T05:23:31.288Z","comments":true,"path":"2020/10/21/hello-world/","link":"","permalink":"http://example.com/2020/10/21/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}